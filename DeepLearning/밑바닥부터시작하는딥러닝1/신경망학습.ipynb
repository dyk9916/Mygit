{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차제곱합\n",
    "\n",
    "def sum_of_squares(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 신경망의 출력\n",
    "t = [0,0,1,0,0,0,0,0,0,0] # 정답 레이블\n",
    "\n",
    "sum_of_squares(np.array(y), np.array(t)) # 0.09750000000000003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 엔트로피 오차\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "  delta = 1e-7\n",
    "  return -np.sum(t * np.log(y + delta)) # 아주 작은 delta를 더해줌으로써 np.log(0)이 -inf가 되는 것을 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 신경망의 출력\n",
    "t = [0,0,1,0,0,0,0,0,0,0] # 정답 레이블\n",
    "\n",
    "cross_entropy_error(np.array(y), np.array(t)) # 0.510825457099338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n"
     ]
    }
   ],
   "source": [
    "delta = 1e-7\n",
    "a = -(1*np.log(0.6+delta)) # 정답 레이블이 1일 때의 교차 엔트로피 오차\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('C:/Users/KimDongyoung/Desktop/Github/my_git/mygit/DEEPLEARNING/퍼셉트론/mnist.py')\n",
    "import pickle\n",
    "from mnist import load_mnist\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터셋 불러오기\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()  # x_train, t_train: 훈련 데이터, x_test, t_test: 테스트 데이터\n",
    "\n",
    "# 데이터 정규화\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# 데이터 형태 변환\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = x_test.shape[0]\n",
    "batch_mask = np.random.choice(test_size, batch_size)\n",
    "x_batch = x_test[batch_mask]\n",
    "t_batch = t_test[batch_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 784), (10,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.shape, t_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미니배치용 교차 엔트로피 오차 구현\n",
    "# 정답 레이블이 원-핫 인코딩이 아닌 숫자 레이블로 주어졌을 때\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size) # t는 실제 정답 레이블\n",
    "        y = y.reshape(1, y.size) # y는 신경망의 출력\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size # np.arange(batch_size)는 0부터 batch_size-1까지 배열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.118095650958317"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(x_batch), np.array(t_batch)) # 0.510825457099338"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수치미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "  h = 1e-4\n",
    "  return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "def function_1(x):\n",
    "  return 0.01**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0999999999995449"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5) # 0.1999999999990898"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 편미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "  return x[0]**2 + x[1]**2  # x_0^2 + x_1^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0 = 3, x1 = 4일 때, x0에 대한 편미분\n",
    "\n",
    "def function_tmp1(x0):\n",
    "  return x0*x0 + 4.0**2.0\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0) # 6.00000000000378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0 = 3, x1 = 4일 때, x1에 대한 편미분\n",
    "\n",
    "def function_tmp2(x1):\n",
    "  return 3.0**2.0 + x1*x1\n",
    "\n",
    "numerical_diff(function_tmp1, 4.0) # 7.999999999999119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 편미분을 동시에 계산\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "  h = 1e-4\n",
    "  grad = np.zeros_like(x)\n",
    "\n",
    "  for idx in range(x.size):\n",
    "    tmp_val = x[idx]\n",
    "\n",
    "    # f(x+h) 계산\n",
    "    x[idx] = tmp_val + h\n",
    "    fxh1 = f(x)\n",
    "\n",
    "    # f(x-h) 계산\n",
    "    x[idx] = tmp_val - h\n",
    "    fxh2 = f(x)\n",
    "\n",
    "    grad[idx] = (fxh1 - fxh2) / (2*h)   # f(x+h) - f(x-h) / 2h\n",
    "    x[idx] = tmp_val\n",
    "\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0])) # array([6., 8.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f,init_x, lr=0.01, step_num=100):\n",
    "  x = init_x\n",
    "\n",
    "  for i in range(step_num):\n",
    "    grad = numerical_gradient(f, x)\n",
    "    x -= lr * grad\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "  return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100) # (-3,4)로 시작하여 (0,0)으로 수렴, 실제로 최솟값은 (0,0)이므로 정확하게 수렴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망에서의 기울기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트맥스 함수 구현시 주의점: 오버플로 방지\n",
    "# 지수 함수는 쉽게 아주 큰 값을 내뱉어버릴 수 있기 때문에 이를 방지하기 위해 입력 신호 중 최댓값을 빼주는 방법을 사용\n",
    "# 가령 np.exp(10)은 22026.465794806718이지만, np.exp(1000)은 inf를 반환\n",
    "# np.max(x)는 입력 신호 중 최댓값을 반환\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - np.max(x, axis=1, keepdims=True) # 오버플로 대책\n",
    "        x = np.exp(x)\n",
    "        x /= np.sum(x, axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x) # 오버플로 대책\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return x\n",
    "\n",
    "# 평균 손실함수\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size) # t는 실제 정답 레이블\n",
    "        y = y.reshape(1, y.size) # y는 신경망의 출력\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size  # np.log(0)이 -inf가 되는 것을 방지하기 위해 아주 작은 delta(1e-7)를 더해줌\n",
    "\n",
    "# 수치 미분\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001, \n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    # np.nditer: 다차원 배열을 순차적으로 접근할 수 있게 해주는 이터레이터\n",
    "    while not it.finished: # 이터레이터의 끝까지 반복\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "  def __init__(self):\n",
    "    self.W = np.random.randn(2,3) # 가중치 랜덤으로 초기화, 2*3 행렬\n",
    "    \n",
    "  def predict(self, x):\n",
    "    return np.dot(x, self.W)\n",
    "  \n",
    "  def loss(self, x, t):\n",
    "    z = self.predict(x)\n",
    "    y = softmax(z)\n",
    "    loss = cross_entropy_error(y, t)  # 손실함수\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44386323  0.33367433  1.49407907]\n",
      " [-0.20515826  0.3130677  -0.85409574]]\n",
      "[0.0816755  0.48196553 0.12776128]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W) # 가중치 매개변수, 랜덤으로 초기화\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.217877225737111"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1]) # 정답 레이블\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16951916  0.25296623 -0.42248539]\n",
      " [ 0.25427874  0.37944935 -0.63372809]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2층 신경망 클래스 구현하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퍼셉트론과 달리 신경망은 활성화 함수로 시그모이드 함수를 사용한다.\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "  def __init__(self, input_size, hidden_size, output_size, weight_init_std =0.01): # weight_init_std: 가중치 초기화 시 정규분포의 스케일\n",
    "    self.params = {}\n",
    "    # 평균이 0이고 표준편차가 weight_init_std인 정규분포를 따르는 가중치를 생성하여 초기화\n",
    "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) # 1번째 층의 가중치, 예를 들어 input size(입력)가 784이고 hidden size(은닉층)가 100이면 784*100의 가중치 행렬이 생성\n",
    "    self.params['b1'] = np.zeros(hidden_size) # 1번째 층의 편향, 이 배열의 모든 원소는 0으로 초기화\n",
    "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) # 2번째 층의 가중치, output size은 출력층의 뉴런 수\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "  \n",
    "  def predict(self,x):\n",
    "    W1, W2 = self.params['W1'], self.params['W2']\n",
    "    b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "    # 입력층에서 1층으로의 신호 전달\n",
    "    a1 = np.dot(x, W1) + b1  \n",
    "    z1 = sigmoid(a1) # 활성화 함수로 시그모이드 함수 사용\n",
    "    \n",
    "    # 1층에서 출력층으로의 신호 전달\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    y = softmax(a2) # 출력층의 활성화 함수로 소프트맥스 함수 사용\n",
    "    \n",
    "    return y\n",
    "  \n",
    "  def loss(self, x, t): # x: 입력 데이터, t: 정답 레이블\n",
    "    y = self.predict(x) # 신경망의 출력(예측값)\n",
    "    \n",
    "    # 손실함수로 교차 엔트로피 오차 사용\n",
    "    return cross_entropy_error(y, t) # 미니배치 처리를 고려하여 평균 손실함수 반환\n",
    "  \n",
    "  def accuracy(self, x, t):\n",
    "    y = self.predict(x)\n",
    "    y = np.argmax(y, axis=1) # 각 데이터의 예측 출력값 중 가장 큰 값의 인덱스 반환\n",
    "    \n",
    "    if t.ndim != 1: # 정답 레이블이 원-핫 인코딩이 아닌 경우\n",
    "      t = np.argmax(t, axis=1)  # 인덱스 형태로 변환\n",
    "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "    return accuracy\n",
    "  \n",
    "  # 수치 미분을 사용한 기울기 계산\n",
    "  # x: 입력 데이터, t: 정답 레이블\n",
    "  def numerical_gradient(self, x, t):\n",
    "    loss_W = lambda W: self.loss(x, t) # 손실함수의 W에 대한 함수\n",
    "    \n",
    "    grads = {}\n",
    "    grads['W1'] = numerical_gradient(loss_W, self.params['W1']) # 수치 미분을 활용해 W1 기울기를 계산, 갱신\n",
    "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.32676196882358 0.12\n"
     ]
    }
   ],
   "source": [
    "# 신경망 생성\n",
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "net.params['W1'].shape, net.params['b1'].shape, net.params['W2'].shape, net.params['b2'].shape\n",
    "\n",
    "x = np.random.rand(100, 784) # 더미 입력 데이터(100장 분량)\n",
    "t = np.random.rand(100, 10) # 더미 정답 레이블(100장 분량)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# mnist 데이터셋 불러오기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000, 10), (10000, 784), (10000, 10))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, t_train.shape, x_test.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_num = 5  # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100  # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09736666666666667, 0.0982)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확도 계산\n",
    "train_acc = network.accuracy(x_train, t_train)\n",
    "test_acc = network.accuracy(x_test, t_test)\n",
    "train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11236666666666667, 0.1135)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확도 계산\n",
    "train_acc = network.accuracy(x_train, t_train)\n",
    "test_acc = network.accuracy(x_test, t_test)\n",
    "train_acc, test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
