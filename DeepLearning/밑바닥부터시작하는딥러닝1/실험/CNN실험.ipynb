{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"C:/Users/KimDongyoung/Desktop/Github/my_git/mygit/DEEPLEARNING/퍼셉트론\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "import mnist as mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 CNN vs. 복잡한 CNN   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1) # y가 예측 값이므로 argmax로 가장 큰 값의 인덱스를 가져온다.\n",
    "            acc += np.sum(y == tt)   # tt는 정답 레이블이므로 y와 비교하여 같으면 True, 다르면 False가 나온다. True는 1로 취급되므로 sum을 하면 맞춘 개수가 나온다.\n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_mnist(flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299463280389267\n",
      "=== epoch:1, train acc:0.128, test acc:0.126 ===\n",
      "train loss:2.2989614768409594\n",
      "train loss:2.2936797878154316\n",
      "train loss:2.286135772192152\n",
      "train loss:2.285675575674831\n",
      "train loss:2.269435246314802\n",
      "train loss:2.245600252637436\n",
      "train loss:2.2295067774308874\n",
      "train loss:2.2082421950589315\n",
      "train loss:2.197246650089114\n",
      "train loss:2.14894303553277\n",
      "train loss:2.144443170692122\n",
      "train loss:2.1563659311562784\n",
      "train loss:2.0469411599859115\n",
      "train loss:1.963214852002711\n",
      "train loss:1.991917895612214\n",
      "train loss:1.9162850761837538\n",
      "train loss:1.8424210842196813\n",
      "train loss:1.7631559038421207\n",
      "train loss:1.8013407672749127\n",
      "train loss:1.7164697395783874\n",
      "train loss:1.5693093947254297\n",
      "train loss:1.4829975153359825\n",
      "train loss:1.3191407416156387\n",
      "train loss:1.339843556273409\n",
      "train loss:1.2360654580387527\n",
      "train loss:1.3033741232222218\n",
      "train loss:1.083773908450147\n",
      "train loss:1.0397156232057556\n",
      "train loss:1.0911749360428629\n",
      "train loss:0.9803058639182726\n",
      "train loss:0.906700693998136\n",
      "train loss:0.9173587826459477\n",
      "train loss:1.0389578576692875\n",
      "train loss:0.813882364550547\n",
      "train loss:0.7620550763001797\n",
      "train loss:0.6805731554605089\n",
      "train loss:0.7062641300448886\n",
      "train loss:0.7106442077807928\n",
      "train loss:0.7123945247387496\n",
      "train loss:0.6311327454910344\n",
      "train loss:0.6407038163827522\n",
      "train loss:0.5942741447240065\n",
      "train loss:0.7049860202268887\n",
      "train loss:0.6423274273457502\n",
      "train loss:0.5856886951519001\n",
      "train loss:0.5511962753148479\n",
      "train loss:0.6921543959639322\n",
      "train loss:0.6981241286782973\n",
      "train loss:0.602243700356203\n",
      "train loss:0.49686711425928126\n",
      "train loss:0.5270893498233341\n",
      "train loss:0.5015589121018329\n",
      "train loss:0.502816541492273\n",
      "train loss:0.4456040808563509\n",
      "train loss:0.6118771918653207\n",
      "train loss:0.5634287710172338\n",
      "train loss:0.5694478548005734\n",
      "train loss:0.4852707070467918\n",
      "train loss:0.4452603774726782\n",
      "train loss:0.5084568135623296\n",
      "train loss:0.46067709143993285\n",
      "train loss:0.6738744473732161\n",
      "train loss:0.5053927020936989\n",
      "train loss:0.4684718065230406\n",
      "train loss:0.6369804746944682\n",
      "train loss:0.5839517241520711\n",
      "train loss:0.3289368299045362\n",
      "train loss:0.5056909830555274\n",
      "train loss:0.3471351834594547\n",
      "train loss:0.5762320175618815\n",
      "train loss:0.5597305661968015\n",
      "train loss:0.5043418264606285\n",
      "train loss:0.5028175064958735\n",
      "train loss:0.5394053252238149\n",
      "train loss:0.4074474293938092\n",
      "train loss:0.37085266148293394\n",
      "train loss:0.5527467014917948\n",
      "train loss:0.3260565274895637\n",
      "train loss:0.28891169610006306\n",
      "train loss:0.335178408516515\n",
      "train loss:0.3630160049966725\n",
      "train loss:0.3703527089308828\n",
      "train loss:0.4761008614678363\n",
      "train loss:0.40683807702773095\n",
      "train loss:0.4346281297370016\n",
      "train loss:0.33484051312133395\n",
      "train loss:0.34618914249432187\n",
      "train loss:0.5703109099886516\n",
      "train loss:0.4856593618693156\n",
      "train loss:0.35025323215717563\n",
      "train loss:0.34290230987520715\n",
      "train loss:0.3906302040490211\n",
      "train loss:0.3147926517032306\n",
      "train loss:0.32640229608384064\n",
      "train loss:0.5051364193053232\n",
      "train loss:0.2582755470475323\n",
      "train loss:0.33948320219255196\n",
      "train loss:0.4809997226430917\n",
      "train loss:0.37149325443755593\n",
      "train loss:0.4231316353714207\n",
      "train loss:0.44658928765400424\n",
      "train loss:0.3351175162475547\n",
      "train loss:0.2839805586674836\n",
      "train loss:0.3267647293334941\n",
      "train loss:0.30546539708489917\n",
      "train loss:0.28953997941882037\n",
      "train loss:0.32466039176518047\n",
      "train loss:0.35896332338804704\n",
      "train loss:0.40088412092188264\n",
      "train loss:0.4024711589612614\n",
      "train loss:0.27823851577550357\n",
      "train loss:0.3490192676490443\n",
      "train loss:0.48085822841763515\n",
      "train loss:0.3527437834387869\n",
      "train loss:0.3799010956483362\n",
      "train loss:0.5988649736983634\n",
      "train loss:0.31051486958246327\n",
      "train loss:0.27838477640293424\n",
      "train loss:0.30871899720986323\n",
      "train loss:0.2514317800360266\n",
      "train loss:0.420628933346186\n",
      "train loss:0.2720296848264065\n",
      "train loss:0.29827735365645375\n",
      "train loss:0.2897684536318183\n",
      "train loss:0.34215362117018844\n",
      "train loss:0.36573118376957126\n",
      "train loss:0.22004568968238036\n",
      "train loss:0.2493001697766754\n",
      "train loss:0.21907884967010788\n",
      "train loss:0.3139650843825235\n",
      "train loss:0.47320555792260577\n",
      "train loss:0.5604842547393275\n",
      "train loss:0.32694661777494055\n",
      "train loss:0.5046873401639386\n",
      "train loss:0.33703347586401267\n",
      "train loss:0.42936621382258033\n",
      "train loss:0.29602399452471295\n",
      "train loss:0.29170708681498814\n",
      "train loss:0.40384415814930863\n",
      "train loss:0.5019243656152144\n",
      "train loss:0.37154130904156973\n",
      "train loss:0.2504554301215587\n",
      "train loss:0.1507417239514493\n",
      "train loss:0.3974712519997529\n",
      "train loss:0.28758525201810753\n",
      "train loss:0.38648366716285687\n",
      "train loss:0.22723142535570925\n",
      "train loss:0.3523744015269895\n",
      "train loss:0.3877263386579741\n",
      "train loss:0.3346008585650641\n",
      "train loss:0.2826351443237034\n",
      "train loss:0.5217349115292226\n",
      "train loss:0.3397632855428057\n",
      "train loss:0.29756478472047043\n",
      "train loss:0.2692471907654547\n",
      "train loss:0.24031312946681496\n",
      "train loss:0.33428130265743805\n",
      "train loss:0.22476191301237086\n",
      "train loss:0.33118460106977815\n",
      "train loss:0.3757603187302122\n",
      "train loss:0.4687885320674803\n",
      "train loss:0.25257451898258315\n",
      "train loss:0.22173075755574811\n",
      "train loss:0.22837612829502874\n",
      "train loss:0.22936373075453528\n",
      "train loss:0.19510921316583205\n",
      "train loss:0.24411852609789939\n",
      "train loss:0.21121581908771767\n",
      "train loss:0.3385468841698868\n",
      "train loss:0.22761289147344801\n",
      "train loss:0.4620545693207236\n",
      "train loss:0.2300464048310788\n",
      "train loss:0.35972520246872103\n",
      "train loss:0.21030764308091826\n",
      "train loss:0.2324471574579306\n",
      "train loss:0.21463827335804372\n",
      "train loss:0.33459087812200694\n",
      "train loss:0.2695771278153913\n",
      "train loss:0.4966941151236449\n",
      "train loss:0.42501822143092677\n",
      "train loss:0.3304274825144241\n",
      "train loss:0.21715894063559818\n",
      "train loss:0.3997199098384423\n",
      "train loss:0.3078878128400482\n",
      "train loss:0.4493722803967604\n",
      "train loss:0.503861290732514\n",
      "train loss:0.27668990054815346\n",
      "train loss:0.3484319918397077\n",
      "train loss:0.1766360024515355\n",
      "train loss:0.3314260978204098\n",
      "train loss:0.344236712606067\n",
      "train loss:0.34583426996181565\n",
      "train loss:0.220933188350296\n",
      "train loss:0.38678182020058566\n",
      "train loss:0.35097416035339113\n",
      "train loss:0.3989867538564125\n",
      "train loss:0.4062226843341816\n",
      "train loss:0.2471172493265662\n",
      "train loss:0.2929713959517152\n",
      "train loss:0.2612865057918081\n",
      "train loss:0.3042718512300547\n",
      "train loss:0.2861261409851674\n",
      "train loss:0.5274819585224639\n",
      "train loss:0.3316471920554005\n",
      "train loss:0.25716896092257185\n",
      "train loss:0.16663548422684396\n",
      "train loss:0.2233411182480631\n",
      "train loss:0.4378794799918014\n",
      "train loss:0.23275361336373768\n",
      "train loss:0.35699394890851777\n",
      "train loss:0.22764857379691378\n",
      "train loss:0.4574983610878445\n",
      "train loss:0.2750480670415435\n",
      "train loss:0.2628037855600567\n",
      "train loss:0.22390296174624522\n",
      "train loss:0.2708383961474049\n",
      "train loss:0.3734730073174019\n",
      "train loss:0.3023605206662032\n",
      "train loss:0.37008593697948916\n",
      "train loss:0.23093032340938197\n",
      "train loss:0.22236178057012562\n",
      "train loss:0.22710343648766634\n",
      "train loss:0.18900139854568754\n",
      "train loss:0.24330273791915968\n",
      "train loss:0.37122902840065414\n",
      "train loss:0.47172219541773386\n",
      "train loss:0.2587606432327133\n",
      "train loss:0.3499047745513254\n",
      "train loss:0.17562073176137016\n",
      "train loss:0.27508958571174746\n",
      "train loss:0.22329920974613843\n",
      "train loss:0.2703874664954189\n",
      "train loss:0.21208734920046673\n",
      "train loss:0.1974725082436933\n",
      "train loss:0.3717975509073874\n",
      "train loss:0.20169881754353894\n",
      "train loss:0.24528612738073988\n",
      "train loss:0.21456787241276673\n",
      "train loss:0.2079059370961889\n",
      "train loss:0.3334533849219293\n",
      "train loss:0.3022638826119697\n",
      "train loss:0.21287670109841716\n",
      "train loss:0.2321450649091893\n",
      "train loss:0.3002273791050414\n",
      "train loss:0.4801714460938542\n",
      "train loss:0.2139709914448295\n",
      "train loss:0.2717519292479904\n",
      "train loss:0.3452048557545767\n",
      "train loss:0.3485592545031009\n",
      "train loss:0.30233952742822495\n",
      "train loss:0.26255379941332807\n",
      "train loss:0.3189134933766232\n",
      "train loss:0.20708037861043088\n",
      "train loss:0.251559984720395\n",
      "train loss:0.3258132567250559\n",
      "train loss:0.24410456960373902\n",
      "train loss:0.13527377677638566\n",
      "train loss:0.25568445953221997\n",
      "train loss:0.1909802462365355\n",
      "train loss:0.275134230426443\n",
      "train loss:0.3549917714735063\n",
      "train loss:0.4206271371794535\n",
      "train loss:0.2817867077720973\n",
      "train loss:0.18175773691598393\n",
      "train loss:0.31083854589981\n",
      "train loss:0.19241449213359865\n",
      "train loss:0.23804108898488463\n",
      "train loss:0.24247497921504327\n",
      "train loss:0.3146312621602202\n",
      "train loss:0.26256778230011746\n",
      "train loss:0.18805128025898643\n",
      "train loss:0.4227301830596018\n",
      "train loss:0.18225228008041391\n",
      "train loss:0.16280818684120965\n",
      "train loss:0.21136077950009802\n",
      "train loss:0.157929784023052\n",
      "train loss:0.18748012531127517\n",
      "train loss:0.27252248402004914\n",
      "train loss:0.28308090431694194\n",
      "train loss:0.31625690407435797\n",
      "train loss:0.22762250201286602\n",
      "train loss:0.18946157247584636\n",
      "train loss:0.16430935109560113\n",
      "train loss:0.2228083838467732\n",
      "train loss:0.10913048056430913\n",
      "train loss:0.24043184650830568\n",
      "train loss:0.21342564731054345\n",
      "train loss:0.15841618937133073\n",
      "train loss:0.18616229114319463\n",
      "train loss:0.20275282281756646\n",
      "train loss:0.12915606368241733\n",
      "train loss:0.2893309803390286\n",
      "train loss:0.14772719928598244\n",
      "train loss:0.12340501582759975\n",
      "train loss:0.23391336096938736\n",
      "train loss:0.22323822436818958\n",
      "train loss:0.2206472000572049\n",
      "train loss:0.2555064859793734\n",
      "train loss:0.2694950206691462\n",
      "train loss:0.32623668416009854\n",
      "train loss:0.2348788570807283\n",
      "train loss:0.19954574958152552\n",
      "train loss:0.20659857255344555\n",
      "train loss:0.30061523942449164\n",
      "train loss:0.2400510900782171\n",
      "train loss:0.2254492845716901\n",
      "train loss:0.16398488225607405\n",
      "train loss:0.23141158670322112\n",
      "train loss:0.15505150967147613\n",
      "train loss:0.19034388616483924\n",
      "train loss:0.37375548077737714\n",
      "train loss:0.2874826157601266\n",
      "train loss:0.1976342773564985\n",
      "train loss:0.12238828665953036\n",
      "train loss:0.4214885128887949\n",
      "train loss:0.29705680757034136\n",
      "train loss:0.18499829114228472\n",
      "train loss:0.2565346633773771\n",
      "train loss:0.2422323649443882\n",
      "train loss:0.14422125135259534\n",
      "train loss:0.25179741787324117\n",
      "train loss:0.22388811444386955\n",
      "train loss:0.17427312794190622\n",
      "train loss:0.24147510199205705\n",
      "train loss:0.23261712874569246\n",
      "train loss:0.2159185360504848\n",
      "train loss:0.1551814329275549\n",
      "train loss:0.15259280900889388\n",
      "train loss:0.12199135898054861\n",
      "train loss:0.18365811402845325\n",
      "train loss:0.14906366665933446\n",
      "train loss:0.28726605863949545\n",
      "train loss:0.14582204364359236\n",
      "train loss:0.21113923268324958\n",
      "train loss:0.16259197903262085\n",
      "train loss:0.2571030563328565\n",
      "train loss:0.22897399085186393\n",
      "train loss:0.11252622089828718\n",
      "train loss:0.21132550157516153\n",
      "train loss:0.25202673218017746\n",
      "train loss:0.2833805090239104\n",
      "train loss:0.21495675078092702\n",
      "train loss:0.26824575055404287\n",
      "train loss:0.2466887347755994\n",
      "train loss:0.1711070611010829\n",
      "train loss:0.220127623946549\n",
      "train loss:0.2982720178477214\n",
      "train loss:0.1494542243779663\n",
      "train loss:0.22571957718812505\n",
      "train loss:0.14784179566446873\n",
      "train loss:0.20762983800473997\n",
      "train loss:0.13768932154792038\n",
      "train loss:0.24257417502568643\n",
      "train loss:0.1868974424881418\n",
      "train loss:0.2598716699126311\n",
      "train loss:0.1614374530503786\n",
      "train loss:0.23472515312920808\n",
      "train loss:0.1528251797308213\n",
      "train loss:0.17704530119521839\n",
      "train loss:0.2746293062500793\n",
      "train loss:0.21689820781529875\n",
      "train loss:0.12742710558166045\n",
      "train loss:0.35208547512079624\n",
      "train loss:0.37755359522163057\n",
      "train loss:0.1318025105244506\n",
      "train loss:0.13555454087013674\n",
      "train loss:0.13598881064941462\n",
      "train loss:0.1516218168762361\n",
      "train loss:0.06421030898292117\n",
      "train loss:0.26814235382420876\n",
      "train loss:0.19159535387011675\n",
      "train loss:0.225196405264251\n",
      "train loss:0.14831301542704187\n",
      "train loss:0.1553738542473978\n",
      "train loss:0.29287959532208124\n",
      "train loss:0.2554767620629546\n",
      "train loss:0.17714546038269469\n",
      "train loss:0.13296539636284002\n",
      "train loss:0.13634373010316786\n",
      "train loss:0.1343391022712083\n",
      "train loss:0.21442470636445726\n",
      "train loss:0.15764694454639222\n",
      "train loss:0.19535448621206933\n",
      "train loss:0.1741010451442309\n",
      "train loss:0.21848771727226615\n",
      "train loss:0.16195202632099012\n",
      "train loss:0.17485610102366722\n",
      "train loss:0.2580649173840467\n",
      "train loss:0.11059955672354718\n",
      "train loss:0.13669555874900088\n",
      "train loss:0.4321223906615989\n",
      "train loss:0.13162007382762753\n",
      "train loss:0.13633160983793058\n",
      "train loss:0.1620655311157089\n",
      "train loss:0.16335444389735973\n",
      "train loss:0.1074608180852464\n",
      "train loss:0.29081311725950576\n",
      "train loss:0.11648681765633592\n",
      "train loss:0.1753237186286369\n",
      "train loss:0.13952621774675947\n",
      "train loss:0.1417941971154714\n",
      "train loss:0.1192950147021043\n",
      "train loss:0.11850160975708425\n",
      "train loss:0.1776012576787531\n",
      "train loss:0.11588949884806293\n",
      "train loss:0.0916217093113489\n",
      "train loss:0.09656950633369553\n",
      "train loss:0.1878332607648823\n",
      "train loss:0.23321933916099993\n",
      "train loss:0.15054943783548239\n",
      "train loss:0.17740432816517895\n",
      "train loss:0.23088462874087123\n",
      "train loss:0.31638396967890026\n",
      "train loss:0.08791849646092845\n",
      "train loss:0.13518129166720372\n",
      "train loss:0.2299981909676524\n",
      "train loss:0.1357173566746292\n",
      "train loss:0.11725303321116519\n",
      "train loss:0.1482841097706554\n",
      "train loss:0.19414646746085917\n",
      "train loss:0.14004277238134388\n",
      "train loss:0.18547341410093196\n",
      "train loss:0.16091775156627017\n",
      "train loss:0.12803190649577462\n",
      "train loss:0.16078616209924612\n",
      "train loss:0.18666084168890612\n",
      "train loss:0.22634313639298356\n",
      "train loss:0.17945651930169665\n",
      "train loss:0.15809246916109734\n",
      "train loss:0.16538605630190098\n",
      "train loss:0.17802981193631676\n",
      "train loss:0.1216244965148604\n",
      "train loss:0.1104272117541573\n",
      "train loss:0.19476207841901147\n",
      "train loss:0.15394136934746938\n",
      "train loss:0.10227442031866811\n",
      "train loss:0.1389745188408287\n",
      "train loss:0.15049138294113582\n",
      "train loss:0.10312811908188951\n",
      "train loss:0.1460655567474426\n",
      "train loss:0.3111928508006353\n",
      "train loss:0.07471060488953489\n",
      "train loss:0.10005920137423\n",
      "train loss:0.17044415511528827\n",
      "train loss:0.2201624585362758\n",
      "train loss:0.11661921747239377\n",
      "train loss:0.15093904677194453\n",
      "train loss:0.1387085672256363\n",
      "train loss:0.1531600811266439\n",
      "train loss:0.10448510750893426\n",
      "train loss:0.15240633367771253\n",
      "train loss:0.18722692486132028\n",
      "train loss:0.11462284710186568\n",
      "train loss:0.06786609662958878\n",
      "train loss:0.13453133149765664\n",
      "train loss:0.1717813993561057\n",
      "train loss:0.1269284776509803\n",
      "train loss:0.2769872329695425\n",
      "train loss:0.15540091885348967\n",
      "train loss:0.1486755431982507\n",
      "train loss:0.1548054257355784\n",
      "train loss:0.09154187204482657\n",
      "train loss:0.16033773490279365\n",
      "train loss:0.12296174190673285\n",
      "train loss:0.19397646570101718\n",
      "train loss:0.10114935711524184\n",
      "train loss:0.1841281456796892\n",
      "train loss:0.29836967080071586\n",
      "train loss:0.06698599619025351\n",
      "train loss:0.2927247414155765\n",
      "train loss:0.26895049716448094\n",
      "train loss:0.17289080252696287\n",
      "train loss:0.0850401996021886\n",
      "train loss:0.1748466526180652\n",
      "train loss:0.09969898728572274\n",
      "train loss:0.11794232193248494\n",
      "train loss:0.15575079736851255\n",
      "train loss:0.16732756588862385\n",
      "train loss:0.14894567193959754\n",
      "train loss:0.12536025108533067\n",
      "train loss:0.1678116731916753\n",
      "train loss:0.15991311725452603\n",
      "train loss:0.10822805910143192\n",
      "train loss:0.06828822304024569\n",
      "train loss:0.19181183024719325\n",
      "train loss:0.11626012757083287\n",
      "train loss:0.23430834839596693\n",
      "train loss:0.22186104080878744\n",
      "train loss:0.17942758930468966\n",
      "train loss:0.19196584258807556\n",
      "train loss:0.17336808519728564\n",
      "train loss:0.08523683531852562\n",
      "train loss:0.1339228583373382\n",
      "train loss:0.08126198130372553\n",
      "train loss:0.18656829505115\n",
      "train loss:0.2392946541370823\n",
      "train loss:0.11443924767656415\n",
      "train loss:0.18941334398895088\n",
      "train loss:0.09645933896568738\n",
      "train loss:0.18850405898944123\n",
      "train loss:0.12949536671976095\n",
      "train loss:0.0780990224811168\n",
      "train loss:0.1461305388075763\n",
      "train loss:0.08416754966216831\n",
      "train loss:0.2007609886857434\n",
      "train loss:0.11590969348242419\n",
      "train loss:0.26821362151648986\n",
      "train loss:0.13119069172401276\n",
      "train loss:0.11441943935656435\n",
      "train loss:0.10123641240342121\n",
      "train loss:0.13475364083458177\n",
      "train loss:0.11967403552881103\n",
      "train loss:0.09564229933368422\n",
      "train loss:0.2579120475568831\n",
      "train loss:0.13263243539704772\n",
      "train loss:0.21411412923068696\n",
      "train loss:0.08648418236948525\n",
      "train loss:0.16850720975507244\n",
      "train loss:0.09078345098842085\n",
      "train loss:0.20812373103749754\n",
      "train loss:0.1267966499358405\n",
      "train loss:0.11810215925109346\n",
      "train loss:0.17273836937399578\n",
      "train loss:0.14531061856565242\n",
      "train loss:0.08947832770581683\n",
      "train loss:0.17227108426626508\n",
      "train loss:0.15209630758469694\n",
      "train loss:0.15160472115991194\n",
      "train loss:0.20437308509926655\n",
      "train loss:0.1036057579836192\n",
      "train loss:0.143095016283427\n",
      "train loss:0.07099761606041417\n",
      "train loss:0.08207616351866673\n",
      "train loss:0.17742291879636413\n",
      "train loss:0.07891246586166789\n",
      "train loss:0.17155507225818975\n",
      "train loss:0.17262274918564674\n",
      "train loss:0.08679673250021434\n",
      "train loss:0.12721021694259957\n",
      "train loss:0.243702542146139\n",
      "train loss:0.12228474796510513\n",
      "train loss:0.10387064794140927\n",
      "train loss:0.11402725007929035\n",
      "train loss:0.14155549442722803\n",
      "train loss:0.07277219476690278\n",
      "train loss:0.1173531087307811\n",
      "train loss:0.07635994719063689\n",
      "train loss:0.1344227407207286\n",
      "train loss:0.24873007562163094\n",
      "train loss:0.09228991863947458\n",
      "train loss:0.08853221036614102\n",
      "train loss:0.06907513550194562\n",
      "train loss:0.09841520297104302\n",
      "train loss:0.05231340059038034\n",
      "train loss:0.18747277700038997\n",
      "train loss:0.15310280036443982\n",
      "train loss:0.0799284229686236\n",
      "train loss:0.07115609826566384\n",
      "train loss:0.17641467464605184\n",
      "train loss:0.11711450412348032\n",
      "train loss:0.12589161380059322\n",
      "train loss:0.10574733864746035\n",
      "train loss:0.16426920182140012\n",
      "train loss:0.13186552194370071\n",
      "train loss:0.1293256773461828\n",
      "train loss:0.052790845199513034\n",
      "train loss:0.16570206611676944\n",
      "train loss:0.14266251433108346\n",
      "train loss:0.11527303166159651\n",
      "train loss:0.18383800315604887\n",
      "train loss:0.03586819891554129\n",
      "train loss:0.14289281807469276\n",
      "train loss:0.09816058272974232\n",
      "train loss:0.11301600636633534\n",
      "train loss:0.1692589096013785\n",
      "train loss:0.11164698434555521\n",
      "train loss:0.07126113819468405\n",
      "train loss:0.05602411643647829\n",
      "train loss:0.07136005343830662\n",
      "train loss:0.14060304583177394\n",
      "train loss:0.048764156217288475\n",
      "train loss:0.2149685899003312\n",
      "train loss:0.09637304006224795\n",
      "train loss:0.08436031950196402\n",
      "train loss:0.1358325008535959\n",
      "train loss:0.09402059719544326\n",
      "train loss:0.10248634540339685\n",
      "train loss:0.09991063035843585\n",
      "train loss:0.12163481052873909\n",
      "train loss:0.061964095051974234\n",
      "train loss:0.19512341166390434\n",
      "train loss:0.0751568778229083\n",
      "train loss:0.11285627318527519\n",
      "train loss:0.12482704616151272\n",
      "train loss:0.1379548547331524\n",
      "train loss:0.06933148342534796\n",
      "train loss:0.0637888602759748\n",
      "train loss:0.07679784264135649\n",
      "train loss:0.14841812690377806\n",
      "train loss:0.1403721216988687\n",
      "=== epoch:2, train acc:0.965, test acc:0.955 ===\n",
      "train loss:0.20239583861362764\n",
      "train loss:0.1251819550448545\n",
      "train loss:0.04137226486270574\n",
      "train loss:0.1089717524391265\n",
      "train loss:0.07662750734936749\n",
      "train loss:0.08536352525700475\n",
      "train loss:0.11671319688271088\n",
      "train loss:0.09768046131580882\n",
      "train loss:0.26866737646288497\n",
      "train loss:0.20379618690447943\n",
      "train loss:0.07700295475007544\n",
      "train loss:0.130039045507713\n",
      "train loss:0.18866721208445408\n",
      "train loss:0.1597276047661038\n",
      "train loss:0.2310022365915072\n",
      "train loss:0.11348647622838236\n",
      "train loss:0.14024662243082567\n",
      "train loss:0.06857914459040557\n",
      "train loss:0.14167925622968544\n",
      "train loss:0.13213681322890575\n",
      "train loss:0.07077611867138617\n",
      "train loss:0.18523150059523016\n",
      "train loss:0.06530029631040121\n",
      "train loss:0.05665600654478412\n",
      "train loss:0.14881528697659674\n",
      "train loss:0.1028253891377175\n",
      "train loss:0.07001143217537475\n",
      "train loss:0.10372095422585946\n",
      "train loss:0.11753905798391603\n",
      "train loss:0.14114590880989095\n",
      "train loss:0.15719145628010212\n",
      "train loss:0.07070558963500177\n",
      "train loss:0.07593359480275517\n",
      "train loss:0.0863943391124767\n",
      "train loss:0.1636134771405959\n",
      "train loss:0.10525267492931153\n",
      "train loss:0.07527660132516557\n",
      "train loss:0.1582045069220836\n",
      "train loss:0.16318477796591793\n",
      "train loss:0.0731270878618723\n",
      "train loss:0.06155086003699394\n",
      "train loss:0.11028893356499622\n",
      "train loss:0.10376683608820483\n",
      "train loss:0.12489606573311207\n",
      "train loss:0.07749850216317584\n",
      "train loss:0.11075212471868776\n",
      "train loss:0.12666698932013237\n",
      "train loss:0.16998865381203715\n",
      "train loss:0.11388726296725285\n",
      "train loss:0.088398161656366\n",
      "train loss:0.13771988998016677\n",
      "train loss:0.10265622431030075\n",
      "train loss:0.1635073095619263\n",
      "train loss:0.13633736781339195\n",
      "train loss:0.12901436972983643\n",
      "train loss:0.12330000039325563\n",
      "train loss:0.06638444783162278\n",
      "train loss:0.09049534906683408\n",
      "train loss:0.0848503807474234\n",
      "train loss:0.15435391963543021\n",
      "train loss:0.07245286156418375\n",
      "train loss:0.23799150353442367\n",
      "train loss:0.16034150313837034\n",
      "train loss:0.14031795437532749\n",
      "train loss:0.08979038564501148\n",
      "train loss:0.07608827128664292\n",
      "train loss:0.11948249344921931\n",
      "train loss:0.12834892849404902\n",
      "train loss:0.08078413345143565\n",
      "train loss:0.06941965301219144\n",
      "train loss:0.07808796719251072\n",
      "train loss:0.1076331841442373\n",
      "train loss:0.13047574977680104\n",
      "train loss:0.06507830839557861\n",
      "train loss:0.045937306734912496\n",
      "train loss:0.18183624839070206\n",
      "train loss:0.05896029335530589\n",
      "train loss:0.13209210779624866\n",
      "train loss:0.18146413521955576\n",
      "train loss:0.12922246182999364\n",
      "train loss:0.0382465066172893\n",
      "train loss:0.07260400589489147\n",
      "train loss:0.04961551913571574\n",
      "train loss:0.14979157364562168\n",
      "train loss:0.12923130272364583\n",
      "train loss:0.10054646355458226\n",
      "train loss:0.12370151683649269\n",
      "train loss:0.0767568246606267\n",
      "train loss:0.22494712780760978\n",
      "train loss:0.11103971452109365\n",
      "train loss:0.16914571739015208\n",
      "train loss:0.06326576082903322\n",
      "train loss:0.15817648871370676\n",
      "train loss:0.13525946436821645\n",
      "train loss:0.09715707660699544\n",
      "train loss:0.20959786579022155\n",
      "train loss:0.0976501488052825\n",
      "train loss:0.12351550224431124\n",
      "train loss:0.1096379166269216\n",
      "train loss:0.18192266665356674\n",
      "train loss:0.11183209036125995\n",
      "train loss:0.08529690884954158\n",
      "train loss:0.043739881524481355\n",
      "train loss:0.03878579133531444\n",
      "train loss:0.10720978311964943\n",
      "train loss:0.03200030007853507\n",
      "train loss:0.07863462379531755\n",
      "train loss:0.06218971658313437\n",
      "train loss:0.23154025065421885\n",
      "train loss:0.1242601015087936\n",
      "train loss:0.1319318067064544\n",
      "train loss:0.14349437066170892\n",
      "train loss:0.18895441905366617\n",
      "train loss:0.07281892372188616\n",
      "train loss:0.062404274671025536\n",
      "train loss:0.06589547523924051\n",
      "train loss:0.09426997131879053\n",
      "train loss:0.0901336252869736\n",
      "train loss:0.09398825878024258\n",
      "train loss:0.053669453309707356\n",
      "train loss:0.0822300614931536\n",
      "train loss:0.13013533415634448\n",
      "train loss:0.1141595771566189\n",
      "train loss:0.1910675477397574\n",
      "train loss:0.07262429642232786\n",
      "train loss:0.10377886239066218\n",
      "train loss:0.09160633692750528\n",
      "train loss:0.13560946939022783\n",
      "train loss:0.061223654137019315\n",
      "train loss:0.10327514007044622\n",
      "train loss:0.0749846535710388\n",
      "train loss:0.0655442614864533\n",
      "train loss:0.16393417445362182\n",
      "train loss:0.10087091953430839\n",
      "train loss:0.07917624564657609\n",
      "train loss:0.02676060718048445\n",
      "train loss:0.20512178178811363\n",
      "train loss:0.10800406884073267\n",
      "train loss:0.07409113453752139\n",
      "train loss:0.15147504957985772\n",
      "train loss:0.17504475674349337\n",
      "train loss:0.10693314435502592\n",
      "train loss:0.1251767689543817\n",
      "train loss:0.07887565757890162\n",
      "train loss:0.07520197090278406\n",
      "train loss:0.1720768682994625\n",
      "train loss:0.07604551533809514\n",
      "train loss:0.09432861361604913\n",
      "train loss:0.07714388679422159\n",
      "train loss:0.15933842617135188\n",
      "train loss:0.16039851774964384\n",
      "train loss:0.06838934349421578\n",
      "train loss:0.09819945394736017\n",
      "train loss:0.05501006681147635\n",
      "train loss:0.12700600799925638\n",
      "train loss:0.08533830105414757\n",
      "train loss:0.143214989227178\n",
      "train loss:0.1446286256314285\n",
      "train loss:0.127901548396966\n",
      "train loss:0.08225418652256834\n",
      "train loss:0.08407126240082462\n",
      "train loss:0.21583589993519944\n",
      "train loss:0.1460795341665456\n",
      "train loss:0.07166243202388883\n",
      "train loss:0.08028030590837272\n",
      "train loss:0.04422555893626789\n",
      "train loss:0.06987153416148859\n",
      "train loss:0.12523027212005178\n",
      "train loss:0.06019960852709966\n",
      "train loss:0.0505302230928573\n",
      "train loss:0.1321330538509674\n",
      "train loss:0.09235677081564538\n",
      "train loss:0.26230488552158354\n",
      "train loss:0.12702693397957232\n",
      "train loss:0.1639268104109027\n",
      "train loss:0.07164294976763648\n",
      "train loss:0.1798602329383498\n",
      "train loss:0.08882490158018844\n",
      "train loss:0.12231361838686966\n",
      "train loss:0.09268251665196413\n",
      "train loss:0.07311205727173235\n",
      "train loss:0.05008953476058884\n",
      "train loss:0.11079959256093617\n",
      "train loss:0.09668253009095651\n",
      "train loss:0.10638009653502564\n",
      "train loss:0.09213519820632156\n",
      "train loss:0.08042124557407504\n",
      "train loss:0.07520071211220601\n",
      "train loss:0.07992293887154434\n",
      "train loss:0.1476313414563768\n",
      "train loss:0.10540417423445651\n",
      "train loss:0.04349832695900867\n",
      "train loss:0.16370699994237964\n",
      "train loss:0.02386460569047912\n",
      "train loss:0.06477080996445402\n",
      "train loss:0.19308533568885025\n",
      "train loss:0.10704204821137192\n",
      "train loss:0.14511997262177537\n",
      "train loss:0.07423755144039548\n",
      "train loss:0.0823854875930611\n",
      "train loss:0.06848163549574036\n",
      "train loss:0.042352012415576346\n",
      "train loss:0.028227817589911018\n",
      "train loss:0.10793932829518478\n",
      "train loss:0.10103523481674692\n",
      "train loss:0.11000661571999562\n",
      "train loss:0.19067721036716748\n",
      "train loss:0.1093472433784836\n",
      "train loss:0.1323650267887198\n",
      "train loss:0.06933841498207391\n",
      "train loss:0.08420876276205858\n",
      "train loss:0.054082435332264256\n",
      "train loss:0.062089714032897356\n",
      "train loss:0.07241032130565273\n",
      "train loss:0.03222207385028315\n",
      "train loss:0.07661162667446689\n",
      "train loss:0.10975722164795623\n",
      "train loss:0.12014027854981711\n",
      "train loss:0.06954990738435772\n",
      "train loss:0.1655267910415002\n",
      "train loss:0.024601583026532873\n",
      "train loss:0.06930588751634581\n",
      "train loss:0.06076401632426695\n",
      "train loss:0.06175888635982921\n",
      "train loss:0.12566046950698362\n",
      "train loss:0.029096470253910937\n",
      "train loss:0.1586013646544565\n",
      "train loss:0.039389022522621754\n",
      "train loss:0.06634115113184855\n",
      "train loss:0.16273686488262362\n",
      "train loss:0.1259040182132282\n",
      "train loss:0.10992251279910317\n",
      "train loss:0.12655144623580883\n",
      "train loss:0.1261206105454393\n",
      "train loss:0.07563933942003326\n",
      "train loss:0.07881603944668468\n",
      "train loss:0.07454721895805892\n",
      "train loss:0.07477490947618787\n",
      "train loss:0.09043442249256246\n",
      "train loss:0.07481964834769776\n",
      "train loss:0.06301702522115586\n",
      "train loss:0.06726974797061135\n",
      "train loss:0.14645569251970206\n",
      "train loss:0.052957909529865885\n",
      "train loss:0.05934579999561278\n",
      "train loss:0.06375226289245725\n",
      "train loss:0.15284553727052197\n",
      "train loss:0.1520645117075371\n",
      "train loss:0.07802731500676294\n",
      "train loss:0.09046626298490563\n",
      "train loss:0.05864072772640659\n",
      "train loss:0.11470885377877162\n",
      "train loss:0.1494249169167384\n",
      "train loss:0.09788713860957614\n",
      "train loss:0.06241061492715344\n",
      "train loss:0.1492920056513613\n",
      "train loss:0.06991732296843674\n",
      "train loss:0.169882947498853\n",
      "train loss:0.1496010491342092\n",
      "train loss:0.09063597125076163\n",
      "train loss:0.14603071328505446\n",
      "train loss:0.21714094493696046\n",
      "train loss:0.09000850433229501\n",
      "train loss:0.10777006053687682\n",
      "train loss:0.04753145189247076\n",
      "train loss:0.11823240950327683\n",
      "train loss:0.04696206604825979\n",
      "train loss:0.10467967914338563\n",
      "train loss:0.1032299968408997\n",
      "train loss:0.18154150952612674\n",
      "train loss:0.0593740529257785\n",
      "train loss:0.12339245333158662\n",
      "train loss:0.05691804136988223\n",
      "train loss:0.03447772077108033\n",
      "train loss:0.06393176152582976\n",
      "train loss:0.0864128081599864\n",
      "train loss:0.10757908699764274\n",
      "train loss:0.04916919155339005\n",
      "train loss:0.08924160055616641\n",
      "train loss:0.10484544924437036\n",
      "train loss:0.08611463385872357\n",
      "train loss:0.0943421172184552\n",
      "train loss:0.0686536067663693\n",
      "train loss:0.08858013630146294\n",
      "train loss:0.18234750022107757\n",
      "train loss:0.0449622549977456\n",
      "train loss:0.07124494998333869\n",
      "train loss:0.12963419946855403\n",
      "train loss:0.1187126514726794\n",
      "train loss:0.17697380885495156\n",
      "train loss:0.04894482963517058\n",
      "train loss:0.03608327248759556\n",
      "train loss:0.05829777458957056\n",
      "train loss:0.08586044143330808\n",
      "train loss:0.07874412527978837\n",
      "train loss:0.11981545858406838\n",
      "train loss:0.21573181222705326\n",
      "train loss:0.08964839809306305\n",
      "train loss:0.060766068401905\n",
      "train loss:0.1841000736855732\n",
      "train loss:0.08178441123090624\n",
      "train loss:0.09646135946744544\n",
      "train loss:0.09146713358788709\n",
      "train loss:0.03316661667671731\n",
      "train loss:0.06894450865647202\n",
      "train loss:0.18126240348793826\n",
      "train loss:0.12007191762946118\n",
      "train loss:0.08982418452607466\n",
      "train loss:0.11408749457150857\n",
      "train loss:0.10728980369287472\n",
      "train loss:0.08487184947663388\n",
      "train loss:0.04975425212037503\n",
      "train loss:0.0732807310312642\n",
      "train loss:0.05591690992780341\n",
      "train loss:0.06137636974903822\n",
      "train loss:0.08237928237726753\n",
      "train loss:0.06198734731556806\n",
      "train loss:0.12495533378271045\n",
      "train loss:0.09198686972998595\n",
      "train loss:0.05299886329273674\n",
      "train loss:0.06066064744284295\n",
      "train loss:0.08078304812446348\n",
      "train loss:0.10243067258366187\n",
      "train loss:0.16171492678968183\n",
      "train loss:0.09304838962533538\n",
      "train loss:0.05600477727433994\n",
      "train loss:0.09762676485205757\n",
      "train loss:0.05519544347610882\n",
      "train loss:0.1083285335362816\n",
      "train loss:0.07946025677447423\n",
      "train loss:0.12630932469915418\n",
      "train loss:0.06101994002873475\n",
      "train loss:0.0350993186645025\n",
      "train loss:0.06590732269090474\n",
      "train loss:0.03890368934761541\n",
      "train loss:0.08196453162487004\n",
      "train loss:0.030067132901644464\n",
      "train loss:0.04120171233469322\n",
      "train loss:0.08855374388532235\n",
      "train loss:0.09181049665985505\n",
      "train loss:0.06217875012106225\n",
      "train loss:0.0716131685413293\n",
      "train loss:0.053094631104840515\n",
      "train loss:0.06269727329991275\n",
      "train loss:0.0630231648064332\n",
      "train loss:0.08493987025455578\n",
      "train loss:0.02212067821087467\n",
      "train loss:0.0360798414829369\n",
      "train loss:0.019664304778715454\n",
      "train loss:0.13229444503108317\n",
      "train loss:0.06083553039139734\n",
      "train loss:0.11713013837667718\n",
      "train loss:0.02104694006654016\n",
      "train loss:0.04312715473932688\n",
      "train loss:0.07196669276289573\n",
      "train loss:0.092057334198547\n",
      "train loss:0.15816817825992716\n",
      "train loss:0.026548290188932927\n",
      "train loss:0.04122616941630718\n",
      "train loss:0.0880192645378612\n",
      "train loss:0.040215408615703725\n",
      "train loss:0.16721429082454783\n",
      "train loss:0.194199398527065\n",
      "train loss:0.13758835077357462\n",
      "train loss:0.0662749229157629\n",
      "train loss:0.1225160710024358\n",
      "train loss:0.031689743972848655\n",
      "train loss:0.05304716799495813\n",
      "train loss:0.2301493735378947\n",
      "train loss:0.11143405141593238\n",
      "train loss:0.11926233582824074\n",
      "train loss:0.09925524839550975\n",
      "train loss:0.12540018103349937\n",
      "train loss:0.11940803253207422\n",
      "train loss:0.07093935276946824\n",
      "train loss:0.08387618874260186\n",
      "train loss:0.12728215877837606\n",
      "train loss:0.09344497768632146\n",
      "train loss:0.055162095293878555\n",
      "train loss:0.10548031100293967\n",
      "train loss:0.052245249973093426\n",
      "train loss:0.03352457324444248\n",
      "train loss:0.049672725549326655\n",
      "train loss:0.23885941288930831\n",
      "train loss:0.04975699673447771\n",
      "train loss:0.19975515978398775\n",
      "train loss:0.07541396494590885\n",
      "train loss:0.10346289840404822\n",
      "train loss:0.1256967009816465\n",
      "train loss:0.10477861157535182\n",
      "train loss:0.13247899218858628\n",
      "train loss:0.049868678324040294\n",
      "train loss:0.05797369686442316\n",
      "train loss:0.1382711343888042\n",
      "train loss:0.042141828754238275\n",
      "train loss:0.07789721227713187\n",
      "train loss:0.11787897333750787\n",
      "train loss:0.06628789526661066\n",
      "train loss:0.05684648681663948\n",
      "train loss:0.08466433061096494\n",
      "train loss:0.09758326602856827\n",
      "train loss:0.0423544027017104\n",
      "train loss:0.041668032503853765\n",
      "train loss:0.09129326493751894\n",
      "train loss:0.062331625617313674\n",
      "train loss:0.043416726937635215\n",
      "train loss:0.032977380740290246\n",
      "train loss:0.10224963453007409\n",
      "train loss:0.042173817023219505\n",
      "train loss:0.029990084518790056\n",
      "train loss:0.05267514755036046\n",
      "train loss:0.03021067721612704\n",
      "train loss:0.06448559526738054\n",
      "train loss:0.07923452880153749\n",
      "train loss:0.0383528173187376\n",
      "train loss:0.09803806601237179\n",
      "train loss:0.06297828691506896\n",
      "train loss:0.032552601443819155\n",
      "train loss:0.10993963098102952\n",
      "train loss:0.053324232248585374\n",
      "train loss:0.1093136681714768\n",
      "train loss:0.028305048683204728\n",
      "train loss:0.05787738707935887\n",
      "train loss:0.027204271849803886\n",
      "train loss:0.10227313255626479\n",
      "train loss:0.05917482337792469\n",
      "train loss:0.05243999608063387\n",
      "train loss:0.049685734584810036\n",
      "train loss:0.015182370942135613\n",
      "train loss:0.15135521853213477\n",
      "train loss:0.07338504265743251\n",
      "train loss:0.08351155455956649\n",
      "train loss:0.0366858214962537\n",
      "train loss:0.11572063734188691\n",
      "train loss:0.050152417389187715\n",
      "train loss:0.10003918739622579\n",
      "train loss:0.02472208441611637\n",
      "train loss:0.11020293272456742\n",
      "train loss:0.07429339039983542\n",
      "train loss:0.09040419113812001\n",
      "train loss:0.13400241756904469\n",
      "train loss:0.04507809712968922\n",
      "train loss:0.0849993643485639\n",
      "train loss:0.05173679510376754\n",
      "train loss:0.03825980566199798\n",
      "train loss:0.0899474896596916\n",
      "train loss:0.06602956810064584\n",
      "train loss:0.11978301477791103\n",
      "train loss:0.02131335854515369\n",
      "train loss:0.10253834941587885\n",
      "train loss:0.05033449060569832\n",
      "train loss:0.10117932579679895\n",
      "train loss:0.09727243020402987\n",
      "train loss:0.05927641797714343\n",
      "train loss:0.2587032239981725\n",
      "train loss:0.08247575504569729\n",
      "train loss:0.061901562273591365\n",
      "train loss:0.033806941456951095\n",
      "train loss:0.05783463965794072\n",
      "train loss:0.15314703300511254\n",
      "train loss:0.09725441224113301\n",
      "train loss:0.06544045588466446\n",
      "train loss:0.05293280785025625\n",
      "train loss:0.04127284787650759\n",
      "train loss:0.09284420500741357\n",
      "train loss:0.05213049526928871\n",
      "train loss:0.06680588380708245\n",
      "train loss:0.07219284518037215\n",
      "train loss:0.05761782257640395\n",
      "train loss:0.09303279524656177\n",
      "train loss:0.06648169975331664\n",
      "train loss:0.0712677405302881\n",
      "train loss:0.08748088576253814\n",
      "train loss:0.04621166606661517\n",
      "train loss:0.01792741790410688\n",
      "train loss:0.09201217846960097\n",
      "train loss:0.028994659645758175\n",
      "train loss:0.0641071437913111\n",
      "train loss:0.03689309270891981\n",
      "train loss:0.05180376324203974\n",
      "train loss:0.040302608749837834\n",
      "train loss:0.09360783699853663\n",
      "train loss:0.040168909108221955\n",
      "train loss:0.057493881506867656\n",
      "train loss:0.14869931860884011\n",
      "train loss:0.14711707596231868\n",
      "train loss:0.09827667049571578\n",
      "train loss:0.1540144705606776\n",
      "train loss:0.11297206092333122\n",
      "train loss:0.057482929627931195\n",
      "train loss:0.03777182435194179\n",
      "train loss:0.12073199755640247\n",
      "train loss:0.07581709652403182\n",
      "train loss:0.02586146825019214\n",
      "train loss:0.1064814581348317\n",
      "train loss:0.048140418538059136\n",
      "train loss:0.07646941266076662\n",
      "train loss:0.04793024117307225\n",
      "train loss:0.026239328901160434\n",
      "train loss:0.03466666492781782\n",
      "train loss:0.07881145281207401\n",
      "train loss:0.126201433205506\n",
      "train loss:0.025926013388045274\n",
      "train loss:0.04666835559219225\n",
      "train loss:0.12877816127652075\n",
      "train loss:0.04744784495005195\n",
      "train loss:0.035799243702480235\n",
      "train loss:0.0762363973031903\n",
      "train loss:0.023706507577317282\n",
      "train loss:0.12440661806997869\n",
      "train loss:0.05850224565452069\n",
      "train loss:0.12750803461128196\n",
      "train loss:0.02411514860684526\n",
      "train loss:0.07429311930912026\n",
      "train loss:0.03724481764527144\n",
      "train loss:0.1930338383537281\n",
      "train loss:0.0538817642121731\n",
      "train loss:0.03999145333989336\n",
      "train loss:0.06546970359171306\n",
      "train loss:0.055608936110391104\n",
      "train loss:0.07588118875854111\n",
      "train loss:0.07220762615391146\n",
      "train loss:0.06832927748728544\n",
      "train loss:0.03833478347014699\n",
      "train loss:0.02224277493539932\n",
      "train loss:0.03488690979704184\n",
      "train loss:0.06605179821148029\n",
      "train loss:0.08938434285523954\n",
      "train loss:0.02118896997394564\n",
      "train loss:0.07709802812499772\n",
      "train loss:0.03716786775275294\n",
      "train loss:0.028308019345969028\n",
      "train loss:0.07805876390676232\n",
      "train loss:0.06347297421446992\n",
      "train loss:0.05280739269949878\n",
      "train loss:0.05074427987259223\n",
      "train loss:0.07946229470698485\n",
      "train loss:0.03421710124668767\n",
      "train loss:0.0781466835920507\n",
      "train loss:0.042557704833135146\n",
      "train loss:0.09111339118902678\n",
      "train loss:0.055101260579414495\n",
      "train loss:0.04504939698383758\n",
      "train loss:0.07696167303512473\n",
      "train loss:0.04872496816780281\n",
      "train loss:0.02309942656107741\n",
      "train loss:0.0404289368581004\n",
      "train loss:0.042318089696671335\n",
      "train loss:0.03566030509043947\n",
      "train loss:0.08885686320354864\n",
      "train loss:0.05564614682567275\n",
      "train loss:0.1112484805842725\n",
      "train loss:0.04511900001940036\n",
      "train loss:0.040557816130558366\n",
      "train loss:0.05685133497272988\n",
      "train loss:0.029936445317866798\n",
      "train loss:0.04631338579793756\n",
      "train loss:0.09213133817910026\n",
      "train loss:0.03416669228918202\n",
      "train loss:0.10000959846604017\n",
      "train loss:0.06570999501159693\n",
      "train loss:0.054835223519439244\n",
      "train loss:0.08057173124758178\n",
      "train loss:0.07651688285278105\n",
      "train loss:0.06378866577438125\n",
      "train loss:0.15335502756475844\n",
      "train loss:0.032945210257872004\n",
      "train loss:0.06602843288148705\n",
      "train loss:0.07759981980712012\n",
      "train loss:0.07339025808210071\n",
      "train loss:0.09909556516666118\n",
      "train loss:0.10831777934567213\n",
      "train loss:0.015003962056459874\n",
      "train loss:0.0455920821787631\n",
      "train loss:0.047964205688859236\n",
      "train loss:0.05613768227433176\n",
      "train loss:0.061931335918076424\n",
      "train loss:0.05693150729415634\n",
      "train loss:0.07104650895882891\n",
      "train loss:0.12828709811846356\n",
      "train loss:0.054563486754692116\n",
      "train loss:0.09014332000424377\n",
      "train loss:0.030493518787013472\n",
      "train loss:0.04339090733125551\n",
      "train loss:0.06823157016778722\n",
      "train loss:0.0483738359229141\n",
      "train loss:0.05492457148669935\n",
      "train loss:0.09783846082277323\n",
      "train loss:0.10016327716162837\n",
      "train loss:0.04314762459680147\n",
      "train loss:0.0691489490741121\n",
      "train loss:0.015278314001589563\n",
      "train loss:0.07322945317409393\n",
      "train loss:0.08772153957138133\n",
      "train loss:0.04151114888036666\n",
      "train loss:0.0136089883388908\n",
      "train loss:0.031841718683322515\n",
      "train loss:0.07504392929866714\n",
      "train loss:0.023491290136543067\n",
      "train loss:0.057595762785900895\n",
      "=== epoch:3, train acc:0.977, test acc:0.976 ===\n",
      "train loss:0.06370967909661805\n",
      "train loss:0.040378760709722794\n",
      "train loss:0.058682959325749386\n",
      "train loss:0.06004711216005421\n",
      "train loss:0.05070276952224158\n",
      "train loss:0.035973276444976714\n",
      "train loss:0.05800708345455949\n",
      "train loss:0.04565039763872183\n",
      "train loss:0.08019043566257354\n",
      "train loss:0.034408548238060076\n",
      "train loss:0.07779958931863154\n",
      "train loss:0.08469227521393191\n",
      "train loss:0.05183888987072588\n",
      "train loss:0.19259522440954113\n",
      "train loss:0.10104804994494143\n",
      "train loss:0.12749081234081\n",
      "train loss:0.04278724409345374\n",
      "train loss:0.0438962954104685\n",
      "train loss:0.04480263535114989\n",
      "train loss:0.020381761289044776\n",
      "train loss:0.041544243620850914\n",
      "train loss:0.04266527370513293\n",
      "train loss:0.033422782174040135\n",
      "train loss:0.07593406936259149\n",
      "train loss:0.04687961819255247\n",
      "train loss:0.023921195037583123\n",
      "train loss:0.034091769585251355\n",
      "train loss:0.030455032216761352\n",
      "train loss:0.0891515503405726\n",
      "train loss:0.04926363752504017\n",
      "train loss:0.06685577330386042\n",
      "train loss:0.040013195057351056\n",
      "train loss:0.0521175763222359\n",
      "train loss:0.013113118362456704\n",
      "train loss:0.029354768989650865\n",
      "train loss:0.032936088456176536\n",
      "train loss:0.020710614276470377\n",
      "train loss:0.07266627934319418\n",
      "train loss:0.09101580830225978\n",
      "train loss:0.06561599321730306\n",
      "train loss:0.04833630474838068\n",
      "train loss:0.08251752060205908\n",
      "train loss:0.03954269317441363\n",
      "train loss:0.12658480914356474\n",
      "train loss:0.10869202051815158\n",
      "train loss:0.03838042283868314\n",
      "train loss:0.049718352680330556\n",
      "train loss:0.13216498211274438\n",
      "train loss:0.033954626600179914\n",
      "train loss:0.045440241047354266\n",
      "train loss:0.0632274326272058\n",
      "train loss:0.14658795707249386\n",
      "train loss:0.0984359087394359\n",
      "train loss:0.0862087059481243\n",
      "train loss:0.17524716314860778\n",
      "train loss:0.12296722588876995\n",
      "train loss:0.029251405468740277\n",
      "train loss:0.032630356257962914\n",
      "train loss:0.04404872102773745\n",
      "train loss:0.04031867472778235\n",
      "train loss:0.03679568685475416\n",
      "train loss:0.0496616016017081\n",
      "train loss:0.053099082412578893\n",
      "train loss:0.125188946237956\n",
      "train loss:0.0660695621123463\n",
      "train loss:0.06476823101415649\n",
      "train loss:0.039737802382709506\n",
      "train loss:0.02349114990031647\n",
      "train loss:0.06271436173707969\n",
      "train loss:0.017747410426537026\n",
      "train loss:0.05104543610609964\n",
      "train loss:0.050448951681257785\n",
      "train loss:0.08509865000132574\n",
      "train loss:0.07454536845473644\n",
      "train loss:0.04052766297414146\n",
      "train loss:0.10453905038628025\n",
      "train loss:0.0739357281269252\n",
      "train loss:0.09965932631297603\n",
      "train loss:0.05148290735931567\n",
      "train loss:0.04856521825423142\n",
      "train loss:0.08237688897728944\n",
      "train loss:0.05773099253236036\n",
      "train loss:0.054850295101815306\n",
      "train loss:0.04139375696006037\n",
      "train loss:0.060945975600624334\n",
      "train loss:0.07358542192914483\n",
      "train loss:0.02091457391233528\n",
      "train loss:0.06347176458952564\n",
      "train loss:0.09985027199034016\n",
      "train loss:0.09170565713288248\n",
      "train loss:0.03006609415342003\n",
      "train loss:0.08880792598998821\n",
      "train loss:0.01999496293181477\n",
      "train loss:0.06547674777238852\n",
      "train loss:0.02983948809594532\n",
      "train loss:0.057844996976655905\n",
      "train loss:0.02133688648934732\n",
      "train loss:0.06595607450813053\n",
      "train loss:0.036009150873356204\n",
      "train loss:0.07166586315364552\n",
      "train loss:0.04262090304291781\n",
      "train loss:0.03596044574992968\n",
      "train loss:0.03338078138712074\n",
      "train loss:0.029711371007371486\n",
      "train loss:0.06558363791656623\n",
      "train loss:0.05152806778359595\n",
      "train loss:0.013063343092242012\n",
      "train loss:0.034069136853356975\n",
      "train loss:0.03834712747422785\n",
      "train loss:0.08756327877954312\n",
      "train loss:0.08537153945632696\n",
      "train loss:0.09449725596609516\n",
      "train loss:0.07350447456646511\n",
      "train loss:0.043586833067261736\n",
      "train loss:0.050903733274693067\n",
      "train loss:0.05919273919460433\n",
      "train loss:0.10608444644708005\n",
      "train loss:0.031059105622777014\n",
      "train loss:0.0582697366667764\n",
      "train loss:0.037825458801099554\n",
      "train loss:0.03863800086185787\n",
      "train loss:0.034300350108768474\n",
      "train loss:0.061416691867272916\n",
      "train loss:0.034333597552685585\n",
      "train loss:0.08803746268634229\n",
      "train loss:0.04632855217037648\n",
      "train loss:0.11845488542243174\n",
      "train loss:0.04615889502691738\n",
      "train loss:0.077656271106067\n",
      "train loss:0.03723528263379736\n",
      "train loss:0.029914329986379253\n",
      "train loss:0.15801347336952948\n",
      "train loss:0.045499699649610766\n",
      "train loss:0.07035601446371208\n",
      "train loss:0.043286196878365636\n",
      "train loss:0.08086371940351068\n",
      "train loss:0.05484961445933165\n",
      "train loss:0.03512848559892964\n",
      "train loss:0.06111417074449414\n",
      "train loss:0.027174973080240097\n",
      "train loss:0.06702334874030964\n",
      "train loss:0.0870047833413635\n",
      "train loss:0.03868607242129995\n",
      "train loss:0.08977204365444441\n",
      "train loss:0.05950414831873978\n",
      "train loss:0.05434084981435991\n",
      "train loss:0.08269191184097471\n",
      "train loss:0.06474028344153158\n",
      "train loss:0.06495887973279424\n",
      "train loss:0.03779649310686331\n",
      "train loss:0.039679221175453944\n",
      "train loss:0.06401865854122399\n",
      "train loss:0.08002048628115904\n",
      "train loss:0.027139335150218616\n",
      "train loss:0.0695276491366232\n",
      "train loss:0.09415684123049506\n",
      "train loss:0.03040905216826291\n",
      "train loss:0.05394015276192569\n",
      "train loss:0.03445684932102458\n",
      "train loss:0.10776084547989669\n",
      "train loss:0.08257500140821293\n",
      "train loss:0.11608949023196467\n",
      "train loss:0.014529495721657761\n",
      "train loss:0.12470437079996201\n",
      "train loss:0.07310760361636388\n",
      "train loss:0.0871826321296134\n",
      "train loss:0.0642223034973151\n",
      "train loss:0.062394973110386845\n",
      "train loss:0.09294243928189888\n",
      "train loss:0.05269325552331068\n",
      "train loss:0.025438328559047853\n",
      "train loss:0.027118457494734908\n",
      "train loss:0.03201750367630751\n",
      "train loss:0.02095752369188096\n",
      "train loss:0.0990884489209941\n",
      "train loss:0.06491103429170286\n",
      "train loss:0.18403328358630905\n",
      "train loss:0.04013262939696082\n",
      "train loss:0.08814043413142236\n",
      "train loss:0.09005560593758082\n",
      "train loss:0.05210926583468458\n",
      "train loss:0.05458366824073235\n",
      "train loss:0.029560902088478933\n",
      "train loss:0.05830950186288307\n",
      "train loss:0.04637530856414293\n",
      "train loss:0.039885062663581786\n",
      "train loss:0.05266921077670515\n",
      "train loss:0.07267716613870333\n",
      "train loss:0.07300326648790448\n",
      "train loss:0.04458086102463861\n",
      "train loss:0.016019566759291615\n",
      "train loss:0.04696613749313114\n",
      "train loss:0.0735034297240482\n",
      "train loss:0.01999214388977683\n",
      "train loss:0.03701731035540378\n",
      "train loss:0.04110856522032653\n",
      "train loss:0.09573717979291771\n",
      "train loss:0.018709736243015793\n",
      "train loss:0.03825226733533017\n",
      "train loss:0.03865338040225506\n",
      "train loss:0.0642912398379323\n",
      "train loss:0.043126827304568745\n",
      "train loss:0.09145637495326091\n",
      "train loss:0.018170723460167258\n",
      "train loss:0.016343833178632992\n",
      "train loss:0.0927940199620508\n",
      "train loss:0.09719463726127447\n",
      "train loss:0.030629451487588957\n",
      "train loss:0.024287346518560585\n",
      "train loss:0.040970716919095436\n",
      "train loss:0.08702311550189645\n",
      "train loss:0.02313254778339943\n",
      "train loss:0.04939811036284978\n",
      "train loss:0.01975770370728974\n",
      "train loss:0.03521257367838533\n",
      "train loss:0.024542815527829234\n",
      "train loss:0.08994902112687686\n",
      "train loss:0.08928501751336261\n",
      "train loss:0.020018021788869893\n",
      "train loss:0.07379579981666551\n",
      "train loss:0.0801989953047722\n",
      "train loss:0.024261660525401893\n",
      "train loss:0.03187288993822077\n",
      "train loss:0.01712286049006593\n",
      "train loss:0.03899260303052412\n",
      "train loss:0.07133831331368042\n",
      "train loss:0.0322243304134898\n",
      "train loss:0.06265902374417039\n",
      "train loss:0.06032361713345155\n",
      "train loss:0.06818047471989329\n",
      "train loss:0.02329088346953847\n",
      "train loss:0.02374899593884607\n",
      "train loss:0.03543129889535315\n",
      "train loss:0.044168047591028575\n",
      "train loss:0.06382288415328831\n",
      "train loss:0.010808603493786215\n",
      "train loss:0.059252158631606655\n",
      "train loss:0.0230728242967578\n",
      "train loss:0.11348492703591634\n",
      "train loss:0.04748346849056048\n",
      "train loss:0.02079953451051215\n",
      "train loss:0.0729511737541037\n",
      "train loss:0.07599571885064813\n",
      "train loss:0.037902122487680795\n",
      "train loss:0.027228605056770647\n",
      "train loss:0.0859165459244559\n",
      "train loss:0.03462741111021296\n",
      "train loss:0.046141895454686645\n",
      "train loss:0.0655648133865093\n",
      "train loss:0.03387388219476814\n",
      "train loss:0.019678866811908027\n",
      "train loss:0.023105466575151005\n",
      "train loss:0.03883202161353317\n",
      "train loss:0.06866648079580823\n",
      "train loss:0.011690703064489254\n",
      "train loss:0.10959023647882725\n",
      "train loss:0.019091855437116972\n",
      "train loss:0.09143183133865757\n",
      "train loss:0.05466438204668439\n",
      "train loss:0.029591578922565507\n",
      "train loss:0.038754695383618594\n",
      "train loss:0.01477745186174596\n",
      "train loss:0.1456661306646075\n",
      "train loss:0.07317969216034323\n",
      "train loss:0.05094992061011523\n",
      "train loss:0.05800397822649092\n",
      "train loss:0.02893200197061974\n",
      "train loss:0.046367662775248196\n",
      "train loss:0.020092765411503188\n",
      "train loss:0.0821216664322095\n",
      "train loss:0.016827369768212794\n",
      "train loss:0.06525159853663512\n",
      "train loss:0.023044864451984993\n",
      "train loss:0.06227962618283429\n",
      "train loss:0.0795884547919937\n",
      "train loss:0.01853717867350528\n",
      "train loss:0.06323962809023677\n",
      "train loss:0.028790733888759128\n",
      "train loss:0.04916182611582173\n",
      "train loss:0.05868330974823688\n",
      "train loss:0.04665773921739508\n",
      "train loss:0.04491586557276021\n",
      "train loss:0.05466229919034471\n",
      "train loss:0.07822453214436097\n",
      "train loss:0.03231389041637664\n",
      "train loss:0.07028274068019842\n",
      "train loss:0.018180864578392474\n",
      "train loss:0.06308881351506615\n",
      "train loss:0.005664749392165249\n",
      "train loss:0.027419893353251252\n",
      "train loss:0.05840472522038641\n",
      "train loss:0.06607764780985587\n",
      "train loss:0.0713301496243524\n",
      "train loss:0.08515496505128145\n",
      "train loss:0.08262469820361693\n",
      "train loss:0.09308488615856791\n",
      "train loss:0.04598147531773983\n",
      "train loss:0.15739182103787555\n",
      "train loss:0.031131163073299016\n",
      "train loss:0.017301667894091707\n",
      "train loss:0.12731817649872437\n",
      "train loss:0.06482771731898142\n",
      "train loss:0.0962507429634797\n",
      "train loss:0.01395168558227524\n",
      "train loss:0.07565502375367371\n",
      "train loss:0.10827884532213186\n",
      "train loss:0.1917350340616537\n",
      "train loss:0.026531271463619305\n",
      "train loss:0.04038723170135771\n",
      "train loss:0.045684244774509504\n",
      "train loss:0.05002556605729505\n",
      "train loss:0.07560161576033125\n",
      "train loss:0.071988531598519\n",
      "train loss:0.017042209173269228\n",
      "train loss:0.03780004556480111\n",
      "train loss:0.03765091109413518\n",
      "train loss:0.05073207930772778\n",
      "train loss:0.16335780764346808\n",
      "train loss:0.0698343518623488\n",
      "train loss:0.0701641113555042\n",
      "train loss:0.05416630382644523\n",
      "train loss:0.020506443123140174\n",
      "train loss:0.04156500535443408\n",
      "train loss:0.08638997054983247\n",
      "train loss:0.032922837456220835\n",
      "train loss:0.05012497313838228\n",
      "train loss:0.03016782780644532\n",
      "train loss:0.10443861985439673\n",
      "train loss:0.023265026037845193\n",
      "train loss:0.023798450662254323\n",
      "train loss:0.03854515854194668\n",
      "train loss:0.08126945330077696\n",
      "train loss:0.016390296052048507\n",
      "train loss:0.017172027231088918\n",
      "train loss:0.07961969796048556\n",
      "train loss:0.13849575007627313\n",
      "train loss:0.0731821601465745\n",
      "train loss:0.04253635209133841\n",
      "train loss:0.017147935331416463\n",
      "train loss:0.014216095162141573\n",
      "train loss:0.09610102820209511\n",
      "train loss:0.05558637057887741\n",
      "train loss:0.0387154510580987\n",
      "train loss:0.02717647380215422\n",
      "train loss:0.02398368106089412\n",
      "train loss:0.039863564997193485\n",
      "train loss:0.024915980445717058\n",
      "train loss:0.028951207770488145\n",
      "train loss:0.054247548121977435\n",
      "train loss:0.024332164995706337\n",
      "train loss:0.014241634897182666\n",
      "train loss:0.08204489616918599\n",
      "train loss:0.03629775611869894\n",
      "train loss:0.030558074111032365\n",
      "train loss:0.057183546193147014\n",
      "train loss:0.04117749490137789\n",
      "train loss:0.024604621268356074\n",
      "train loss:0.0661278687200158\n",
      "train loss:0.022097457288039076\n",
      "train loss:0.015929120410039983\n",
      "train loss:0.03313669115228973\n",
      "train loss:0.01920723905563134\n",
      "train loss:0.0573350415364684\n",
      "train loss:0.016972771313260696\n",
      "train loss:0.04410639950701884\n",
      "train loss:0.10232584333994389\n",
      "train loss:0.09615090624397564\n",
      "train loss:0.05877721951472558\n",
      "train loss:0.008277916022029243\n",
      "train loss:0.05994990833489685\n",
      "train loss:0.0819047584238174\n",
      "train loss:0.09958176562419388\n",
      "train loss:0.08113045942757498\n",
      "train loss:0.014631324044681078\n",
      "train loss:0.04682809523113983\n",
      "train loss:0.05116344664857784\n",
      "train loss:0.16006936130904598\n",
      "train loss:0.14093810136728677\n",
      "train loss:0.05357151880789207\n",
      "train loss:0.1085139046903735\n",
      "train loss:0.026336559539055764\n",
      "train loss:0.04955858154354961\n",
      "train loss:0.09201215380031703\n",
      "train loss:0.05967373830416797\n",
      "train loss:0.027286391535608617\n",
      "train loss:0.050568049911258885\n",
      "train loss:0.10151111719269461\n",
      "train loss:0.06141615551319845\n",
      "train loss:0.034230548806352915\n",
      "train loss:0.029944891362395393\n",
      "train loss:0.03233533907527725\n",
      "train loss:0.07902238214861063\n",
      "train loss:0.06106128306472189\n",
      "train loss:0.0548492348933495\n",
      "train loss:0.08296479773043844\n",
      "train loss:0.01766424381015788\n",
      "train loss:0.032899712978983885\n",
      "train loss:0.027248505733005926\n",
      "train loss:0.06933386292785937\n",
      "train loss:0.05771735594080475\n",
      "train loss:0.04209087317199276\n",
      "train loss:0.027795882696932978\n",
      "train loss:0.0470809480691835\n",
      "train loss:0.01466050208747154\n",
      "train loss:0.04738719748164835\n",
      "train loss:0.07532155116390565\n",
      "train loss:0.06189696230041161\n",
      "train loss:0.04290828011694863\n",
      "train loss:0.013173493973874384\n",
      "train loss:0.06918211506593193\n",
      "train loss:0.05237602845603782\n",
      "train loss:0.06655816922831748\n",
      "train loss:0.043525717183026245\n",
      "train loss:0.025462902335494815\n",
      "train loss:0.025615137706084166\n",
      "train loss:0.04998642373598519\n",
      "train loss:0.019889501288603655\n",
      "train loss:0.05603612825256536\n",
      "train loss:0.029926308684880693\n",
      "train loss:0.07630494607527882\n",
      "train loss:0.02339942233028219\n",
      "train loss:0.07420138522624213\n",
      "train loss:0.02155484302479487\n",
      "train loss:0.04648961831114036\n",
      "train loss:0.015545797771639211\n",
      "train loss:0.04338939034761073\n",
      "train loss:0.034594545278788254\n",
      "train loss:0.014650803782710031\n",
      "train loss:0.033687939035361304\n",
      "train loss:0.07211725014179594\n",
      "train loss:0.05393598412428019\n",
      "train loss:0.013998819074532714\n",
      "train loss:0.043715122276382426\n",
      "train loss:0.10209842580747625\n",
      "train loss:0.018180161487799527\n",
      "train loss:0.017687852346996318\n",
      "train loss:0.050630673993764634\n",
      "train loss:0.025015029901811137\n",
      "train loss:0.036710522771656234\n",
      "train loss:0.02015153221682287\n",
      "train loss:0.041029484959038326\n",
      "train loss:0.03474879605926537\n",
      "train loss:0.01956422222861201\n",
      "train loss:0.033857198986586935\n",
      "train loss:0.053191455881484065\n",
      "train loss:0.06689069655772303\n",
      "train loss:0.10182447906832809\n",
      "train loss:0.07958769050357119\n",
      "train loss:0.04283127120011988\n",
      "train loss:0.01997607853630387\n",
      "train loss:0.06019563321060179\n",
      "train loss:0.06457403015065244\n",
      "train loss:0.06529641891065163\n",
      "train loss:0.11423181017438382\n",
      "train loss:0.04636759161640424\n",
      "train loss:0.07745347759376155\n",
      "train loss:0.026836890966314014\n",
      "train loss:0.07547454852336427\n",
      "train loss:0.03320569382229679\n",
      "train loss:0.09934687946348164\n",
      "train loss:0.12189536035720008\n",
      "train loss:0.1133966800458615\n",
      "train loss:0.05699732523467944\n",
      "train loss:0.0283054133059962\n",
      "train loss:0.020374615069135495\n",
      "train loss:0.03850257947820927\n",
      "train loss:0.12847037024262278\n",
      "train loss:0.053120772312049765\n",
      "train loss:0.042897214288631635\n",
      "train loss:0.14696156476474534\n",
      "train loss:0.08728528184020061\n",
      "train loss:0.1079158824777281\n",
      "train loss:0.03415056493726791\n",
      "train loss:0.028944059431782874\n",
      "train loss:0.020848315450997384\n",
      "train loss:0.0679729165574697\n",
      "train loss:0.05089280865834315\n",
      "train loss:0.08372100220877263\n",
      "train loss:0.03257051131333891\n",
      "train loss:0.05330506159692728\n",
      "train loss:0.036215402639653324\n",
      "train loss:0.010942686855131296\n",
      "train loss:0.011861394382025627\n",
      "train loss:0.042530877496106\n",
      "train loss:0.09231679323511308\n",
      "train loss:0.07310899292369061\n",
      "train loss:0.03416248648556501\n",
      "train loss:0.040966352049707756\n",
      "train loss:0.05906898862709322\n",
      "train loss:0.09857852211626084\n",
      "train loss:0.02321788403299586\n",
      "train loss:0.033254193050675646\n",
      "train loss:0.08542408407436498\n",
      "train loss:0.05348142022629353\n",
      "train loss:0.023043935600136987\n",
      "train loss:0.022827689407092726\n",
      "train loss:0.07494855902777149\n",
      "train loss:0.032234659383438814\n",
      "train loss:0.047210896927335974\n",
      "train loss:0.048374321896653694\n",
      "train loss:0.05873249069539825\n",
      "train loss:0.05798024960194688\n",
      "train loss:0.0723453774948712\n",
      "train loss:0.04707832197282061\n",
      "train loss:0.013666791131429226\n",
      "train loss:0.02167242819916104\n",
      "train loss:0.019652159008372617\n",
      "train loss:0.051514198038534215\n",
      "train loss:0.020543392445131328\n",
      "train loss:0.023949089994111068\n",
      "train loss:0.0445712711655333\n",
      "train loss:0.043921209386597465\n",
      "train loss:0.05137355142711288\n",
      "train loss:0.018571538861673672\n",
      "train loss:0.03380814206059155\n",
      "train loss:0.04833835098636244\n",
      "train loss:0.010947476124390471\n",
      "train loss:0.01305871575398829\n",
      "train loss:0.01083641493457135\n",
      "train loss:0.040765904654493736\n",
      "train loss:0.04474126053133306\n",
      "train loss:0.04083269104874108\n",
      "train loss:0.08367982912227856\n",
      "train loss:0.05216762684671813\n",
      "train loss:0.04065794671853433\n",
      "train loss:0.05318432777902531\n",
      "train loss:0.008440102776450572\n",
      "train loss:0.03266810661805085\n",
      "train loss:0.029163909037880072\n",
      "train loss:0.06918433880742252\n",
      "train loss:0.03260130113454666\n",
      "train loss:0.06454052280209631\n",
      "train loss:0.08790788367881515\n",
      "train loss:0.04418672456577755\n",
      "train loss:0.04875784018528111\n",
      "train loss:0.01576515122883476\n",
      "train loss:0.04172683820776627\n",
      "train loss:0.029201963945975608\n",
      "train loss:0.05742021434550258\n",
      "train loss:0.060729903538604066\n",
      "train loss:0.028049046174759625\n",
      "train loss:0.12015097488702611\n",
      "train loss:0.05675203146020496\n",
      "train loss:0.03362070162015338\n",
      "train loss:0.05869174785689728\n",
      "train loss:0.029878187788412055\n",
      "train loss:0.05625347928640912\n",
      "train loss:0.04632586642571036\n",
      "train loss:0.025716611821733983\n",
      "train loss:0.025178702250912614\n",
      "train loss:0.017404123269885753\n",
      "train loss:0.07630169533433277\n",
      "train loss:0.02875668603331644\n",
      "train loss:0.012782549177068161\n",
      "train loss:0.04390312772776636\n",
      "train loss:0.0425173225030808\n",
      "train loss:0.023178923068144112\n",
      "train loss:0.07565215923717436\n",
      "train loss:0.073250084718116\n",
      "train loss:0.02746978642746393\n",
      "train loss:0.03088489543687706\n",
      "train loss:0.040604205035351545\n",
      "train loss:0.015021029486233071\n",
      "train loss:0.133660055496468\n",
      "train loss:0.03209917016237351\n",
      "train loss:0.04877859943118926\n",
      "train loss:0.08588229054515521\n",
      "train loss:0.021230045213212215\n",
      "train loss:0.032616329001789394\n",
      "train loss:0.03403237530601111\n",
      "train loss:0.06521239664483598\n",
      "train loss:0.02889683921801709\n",
      "train loss:0.008591252393141109\n",
      "train loss:0.051741208060480395\n",
      "train loss:0.02540160243786122\n",
      "train loss:0.053053920701276985\n",
      "train loss:0.039065001526197324\n",
      "train loss:0.07119238929535318\n",
      "train loss:0.024817796702298608\n",
      "train loss:0.036491073783003224\n",
      "train loss:0.011064725514932454\n",
      "train loss:0.01008403934876711\n",
      "train loss:0.03797929061028708\n",
      "train loss:0.019674756442205326\n",
      "train loss:0.02878910622847735\n",
      "train loss:0.03866251766773904\n",
      "train loss:0.060497396088956226\n",
      "train loss:0.02664106149926247\n",
      "train loss:0.023422272825661814\n",
      "train loss:0.07212531534129996\n",
      "train loss:0.015366797442889393\n",
      "train loss:0.047228207072773774\n",
      "train loss:0.06982406280704455\n",
      "train loss:0.03911472966641519\n",
      "train loss:0.028255406626569432\n",
      "train loss:0.01007044701530937\n",
      "train loss:0.056909962395177205\n",
      "train loss:0.1469731887174796\n",
      "train loss:0.021485084306667367\n",
      "train loss:0.0488220137411772\n",
      "=== epoch:4, train acc:0.983, test acc:0.985 ===\n",
      "train loss:0.019355212751855703\n",
      "train loss:0.024463277764668098\n",
      "train loss:0.0042865081558220485\n",
      "train loss:0.06550170969667912\n",
      "train loss:0.014897787710122755\n",
      "train loss:0.020210919205476272\n",
      "train loss:0.08224056795589413\n",
      "train loss:0.06375252322425574\n",
      "train loss:0.048637146155674105\n",
      "train loss:0.007388679392281814\n",
      "train loss:0.02105611127017892\n",
      "train loss:0.05060218414839209\n",
      "train loss:0.009687175928791057\n",
      "train loss:0.011089540522439576\n",
      "train loss:0.06059176887002456\n",
      "train loss:0.05542961869525695\n",
      "train loss:0.147873235861276\n",
      "train loss:0.07107663296138869\n",
      "train loss:0.03928853519115233\n",
      "train loss:0.014360207153275194\n",
      "train loss:0.06401277593899238\n",
      "train loss:0.15569615514850338\n",
      "train loss:0.02162649930325643\n",
      "train loss:0.06151750716152927\n",
      "train loss:0.060859501892341614\n",
      "train loss:0.022806872286127523\n",
      "train loss:0.049218874654829874\n",
      "train loss:0.013362886657326965\n",
      "train loss:0.02714213330010205\n",
      "train loss:0.014379984525678714\n",
      "train loss:0.08020812261046256\n",
      "train loss:0.1319527552638441\n",
      "train loss:0.02963561774132646\n",
      "train loss:0.03722234804020865\n",
      "train loss:0.01164689199658843\n",
      "train loss:0.03681362043413042\n",
      "train loss:0.16618277413337604\n",
      "train loss:0.15153021686794257\n",
      "train loss:0.09356806997616189\n",
      "train loss:0.015892093089114875\n",
      "train loss:0.09820756897322128\n",
      "train loss:0.03848342316555274\n",
      "train loss:0.013550139411046082\n",
      "train loss:0.010547215483730742\n",
      "train loss:0.015209305305110839\n",
      "train loss:0.09725005617458114\n",
      "train loss:0.021787354979815342\n",
      "train loss:0.010481519764928691\n",
      "train loss:0.08236813742514291\n",
      "train loss:0.016333687952144082\n",
      "train loss:0.013136667523068454\n",
      "train loss:0.0999876346408125\n",
      "train loss:0.09933922474563811\n",
      "train loss:0.03869635875292506\n",
      "train loss:0.021956250198577852\n",
      "train loss:0.055633045231099645\n",
      "train loss:0.023433116194014864\n",
      "train loss:0.04179347250474945\n",
      "train loss:0.030953827132065785\n",
      "train loss:0.016083856997080145\n",
      "train loss:0.09548358998927309\n",
      "train loss:0.03184357995834736\n",
      "train loss:0.07378616805223007\n",
      "train loss:0.03951960336197366\n",
      "train loss:0.036791426989782924\n",
      "train loss:0.034267742407262866\n",
      "train loss:0.04357119083492831\n",
      "train loss:0.021679904411212987\n",
      "train loss:0.01077165823812893\n",
      "train loss:0.0340000443116551\n",
      "train loss:0.017594082452828338\n",
      "train loss:0.015361641374771497\n",
      "train loss:0.02124679253533826\n",
      "train loss:0.024943408245239704\n",
      "train loss:0.09597562429166792\n",
      "train loss:0.023586645329249853\n",
      "train loss:0.011317036022787733\n",
      "train loss:0.04697189864548117\n",
      "train loss:0.023874922111699018\n",
      "train loss:0.027131064348619532\n",
      "train loss:0.04177394171383584\n",
      "train loss:0.02701001145411935\n",
      "train loss:0.03580658372248533\n",
      "train loss:0.01485850598296597\n",
      "train loss:0.022275213355108833\n",
      "train loss:0.018012197953466157\n",
      "train loss:0.06160028816577148\n",
      "train loss:0.010962467037440082\n",
      "train loss:0.039091819618617796\n",
      "train loss:0.031524662929224845\n",
      "train loss:0.027599112896545163\n",
      "train loss:0.004708975826562285\n",
      "train loss:0.0037484283317918148\n",
      "train loss:0.034294543586633364\n",
      "train loss:0.005670615198902912\n",
      "train loss:0.037969108879432494\n",
      "train loss:0.01747046261356758\n",
      "train loss:0.021866732376452678\n",
      "train loss:0.018990228122454367\n",
      "train loss:0.017539548555300787\n",
      "train loss:0.08218242463716811\n",
      "train loss:0.02807477570062209\n",
      "train loss:0.02368981768947956\n",
      "train loss:0.07812332739502463\n",
      "train loss:0.02455163011759156\n",
      "train loss:0.023284048702568477\n",
      "train loss:0.021328042513249067\n",
      "train loss:0.04281844809911703\n",
      "train loss:0.10727439958865982\n",
      "train loss:0.03638515574022769\n",
      "train loss:0.00798953527846962\n",
      "train loss:0.01887280012457969\n",
      "train loss:0.01666103123665098\n",
      "train loss:0.012265459258726758\n",
      "train loss:0.021086218350526674\n",
      "train loss:0.01984449318377987\n",
      "train loss:0.04696029530123349\n",
      "train loss:0.014276775255027917\n",
      "train loss:0.05090062255949718\n",
      "train loss:0.04728232514508145\n",
      "train loss:0.007569096715117114\n",
      "train loss:0.02787782692845342\n",
      "train loss:0.03929477130581536\n",
      "train loss:0.037765177592124964\n",
      "train loss:0.1932610305451665\n",
      "train loss:0.12179903619449144\n",
      "train loss:0.044247150915736534\n",
      "train loss:0.010051340676412043\n",
      "train loss:0.01669752588026877\n",
      "train loss:0.04683690469374084\n",
      "train loss:0.013669268193556178\n",
      "train loss:0.01873870290737431\n",
      "train loss:0.07116536851806111\n",
      "train loss:0.016339775886206137\n",
      "train loss:0.03948231434217416\n",
      "train loss:0.03395633581315274\n",
      "train loss:0.04130141906989107\n",
      "train loss:0.07029417305300902\n",
      "train loss:0.06350309826373823\n",
      "train loss:0.022929129682652673\n",
      "train loss:0.08602738860942971\n",
      "train loss:0.04282308482635685\n",
      "train loss:0.10657878742887636\n",
      "train loss:0.03147668764232786\n",
      "train loss:0.054983081596807634\n",
      "train loss:0.015201138591844503\n",
      "train loss:0.03238059621619714\n",
      "train loss:0.02858488667567532\n",
      "train loss:0.040110439331767694\n",
      "train loss:0.04582694608815982\n",
      "train loss:0.09258972138462035\n",
      "train loss:0.041926973332974286\n",
      "train loss:0.038886577047542066\n",
      "train loss:0.036485021992104835\n",
      "train loss:0.009563720105486513\n",
      "train loss:0.04155057822296374\n",
      "train loss:0.031739050144835565\n",
      "train loss:0.023477134426164805\n",
      "train loss:0.013343946917753645\n",
      "train loss:0.048423368866634654\n",
      "train loss:0.014326089805879662\n",
      "train loss:0.036079413942554074\n",
      "train loss:0.04509144378894892\n",
      "train loss:0.0667297761207882\n",
      "train loss:0.014639145291644805\n",
      "train loss:0.05554800433121005\n",
      "train loss:0.046978705310730035\n",
      "train loss:0.039672766839396\n",
      "train loss:0.02880719072122674\n",
      "train loss:0.02042043936830469\n",
      "train loss:0.015916924894929897\n",
      "train loss:0.03230855353028656\n",
      "train loss:0.05702482269259916\n",
      "train loss:0.018717120099826286\n",
      "train loss:0.04875124154923826\n",
      "train loss:0.018217708743675235\n",
      "train loss:0.011520647615300236\n",
      "train loss:0.06646318516071696\n",
      "train loss:0.06591895264001345\n",
      "train loss:0.023843607921921747\n",
      "train loss:0.023840056550594443\n",
      "train loss:0.02804767602476123\n",
      "train loss:0.019312708801502942\n",
      "train loss:0.03016149208101608\n",
      "train loss:0.03212322904550352\n",
      "train loss:0.032319158841523275\n",
      "train loss:0.03179087117242668\n",
      "train loss:0.006588373203115819\n",
      "train loss:0.031172613670426973\n",
      "train loss:0.02673284469465315\n",
      "train loss:0.027260991600886644\n",
      "train loss:0.15354042899056103\n",
      "train loss:0.06619693037550374\n",
      "train loss:0.035603347267537326\n",
      "train loss:0.011222708292883622\n",
      "train loss:0.030435588789677816\n",
      "train loss:0.015433295244733603\n",
      "train loss:0.012409966800164518\n",
      "train loss:0.03333449193824664\n",
      "train loss:0.02582893814149031\n",
      "train loss:0.024876842295775692\n",
      "train loss:0.009718863812228761\n",
      "train loss:0.10074329644644375\n",
      "train loss:0.025493510136693102\n",
      "train loss:0.02317528106320825\n",
      "train loss:0.02238491384523562\n",
      "train loss:0.03796092135020806\n",
      "train loss:0.042243757317880395\n",
      "train loss:0.0624272425502668\n",
      "train loss:0.00964132058351684\n",
      "train loss:0.04993680013479557\n",
      "train loss:0.0036075523271294134\n",
      "train loss:0.02677375628939099\n",
      "train loss:0.028368449898268974\n",
      "train loss:0.013015481709581316\n",
      "train loss:0.025125801061195056\n",
      "train loss:0.11641214848698084\n",
      "train loss:0.019314515335746664\n",
      "train loss:0.027643850729852884\n",
      "train loss:0.12872596906263223\n",
      "train loss:0.01127618686281192\n",
      "train loss:0.13200522278366442\n",
      "train loss:0.0477434596202993\n",
      "train loss:0.015350271525218548\n",
      "train loss:0.04215378338338922\n",
      "train loss:0.015162404788523445\n",
      "train loss:0.06297194535320565\n",
      "train loss:0.01101862761550792\n",
      "train loss:0.03751856766277434\n",
      "train loss:0.029910260576362372\n",
      "train loss:0.06531505448933897\n",
      "train loss:0.048522472146497304\n",
      "train loss:0.057053603167579764\n",
      "train loss:0.0878452849983614\n",
      "train loss:0.04494832265762413\n",
      "train loss:0.0111994214191326\n",
      "train loss:0.04708893470488171\n",
      "train loss:0.030817279490690967\n",
      "train loss:0.011250559117512311\n",
      "train loss:0.043284892524500664\n",
      "train loss:0.0193044113789965\n",
      "train loss:0.04388290379706013\n",
      "train loss:0.047271869485617464\n",
      "train loss:0.02907271954686603\n",
      "train loss:0.014527239070274773\n",
      "train loss:0.011509838519223792\n",
      "train loss:0.014861810193496574\n",
      "train loss:0.00667077648197911\n",
      "train loss:0.015558249762434702\n",
      "train loss:0.02459190345043205\n",
      "train loss:0.02259551700177579\n",
      "train loss:0.06964023462138128\n",
      "train loss:0.039527001344292885\n",
      "train loss:0.055577894487612696\n",
      "train loss:0.016371652301520215\n",
      "train loss:0.017620138156645947\n",
      "train loss:0.03662362965836781\n",
      "train loss:0.03403257940037407\n",
      "train loss:0.0368204269738944\n",
      "train loss:0.01378381171634483\n",
      "train loss:0.028859415088951294\n",
      "train loss:0.009992127543941383\n",
      "train loss:0.010603315295381954\n",
      "train loss:0.03434848729159985\n",
      "train loss:0.004563316850424932\n",
      "train loss:0.04035829080521649\n",
      "train loss:0.03345743187376038\n",
      "train loss:0.0823142192555137\n",
      "train loss:0.009783615061848813\n",
      "train loss:0.01256778307513879\n",
      "train loss:0.011990027399435119\n",
      "train loss:0.06368634081434495\n",
      "train loss:0.034407702964038986\n",
      "train loss:0.04550141743406583\n",
      "train loss:0.011621673271159762\n",
      "train loss:0.022713128614886727\n",
      "train loss:0.01983461243648008\n",
      "train loss:0.03603098245782887\n",
      "train loss:0.024755967234320365\n",
      "train loss:0.04735680261257987\n",
      "train loss:0.02596322569027992\n",
      "train loss:0.04517244307254746\n",
      "train loss:0.013508355461975239\n",
      "train loss:0.04539620861315712\n",
      "train loss:0.021080196182171586\n",
      "train loss:0.026693164312617246\n",
      "train loss:0.013575909074921398\n",
      "train loss:0.06656293002655796\n",
      "train loss:0.021423986580749246\n",
      "train loss:0.009989888688562789\n",
      "train loss:0.019275028013946222\n",
      "train loss:0.07520086847792677\n",
      "train loss:0.009629542768282179\n",
      "train loss:0.02256353439041343\n",
      "train loss:0.03670578226511447\n",
      "train loss:0.02391676764122592\n",
      "train loss:0.026228605947619728\n",
      "train loss:0.026280453598856102\n",
      "train loss:0.012873143785657033\n",
      "train loss:0.00963565424767588\n",
      "train loss:0.10573511351017037\n",
      "train loss:0.05113174775458838\n",
      "train loss:0.07194281430875701\n",
      "train loss:0.08692099515578568\n",
      "train loss:0.037326702881171195\n",
      "train loss:0.02752170384740503\n",
      "train loss:0.02661178753302532\n",
      "train loss:0.00700573899605751\n",
      "train loss:0.008635281290721138\n",
      "train loss:0.025508618041485537\n",
      "train loss:0.11679772489346534\n",
      "train loss:0.037196385311581724\n",
      "train loss:0.01489223154611561\n",
      "train loss:0.1899964884104492\n",
      "train loss:0.022013023287975927\n",
      "train loss:0.03617464161929401\n",
      "train loss:0.09209903422423045\n",
      "train loss:0.055405875204994466\n",
      "train loss:0.025253181644103687\n",
      "train loss:0.03717599974919546\n",
      "train loss:0.019013117345818432\n",
      "train loss:0.04018409489759391\n",
      "train loss:0.023394374740282796\n",
      "train loss:0.06027662219506422\n",
      "train loss:0.04044341813032346\n",
      "train loss:0.06549318098781562\n",
      "train loss:0.03141605760642713\n",
      "train loss:0.029398383859488772\n",
      "train loss:0.026567268459334943\n",
      "train loss:0.033249437234818414\n",
      "train loss:0.05443474550447944\n",
      "train loss:0.05152105492623565\n",
      "train loss:0.09920903285133098\n",
      "train loss:0.020344274959231617\n",
      "train loss:0.03483684860845889\n",
      "train loss:0.06497275894823433\n",
      "train loss:0.0460744063593237\n",
      "train loss:0.04848284908727536\n",
      "train loss:0.06801737639665237\n",
      "train loss:0.07550208404274557\n",
      "train loss:0.059353674042825856\n",
      "train loss:0.015347849394616425\n",
      "train loss:0.02016587454200269\n",
      "train loss:0.02771896470756986\n",
      "train loss:0.0500731940932737\n",
      "train loss:0.04460260467196463\n",
      "train loss:0.020304963966583815\n",
      "train loss:0.06296679949097563\n",
      "train loss:0.029278867816305846\n",
      "train loss:0.013429363871188421\n",
      "train loss:0.05608875000524253\n",
      "train loss:0.08865512302285071\n",
      "train loss:0.05295247612061565\n",
      "train loss:0.00857373668550603\n",
      "train loss:0.06951457684982354\n",
      "train loss:0.04317212821408388\n",
      "train loss:0.02770270169150779\n",
      "train loss:0.02289676584062721\n",
      "train loss:0.04881986250983493\n",
      "train loss:0.023769645125320822\n",
      "train loss:0.04492266499508738\n",
      "train loss:0.014829851990959717\n",
      "train loss:0.01986869292440263\n",
      "train loss:0.01379278523923333\n",
      "train loss:0.03332489693610808\n",
      "train loss:0.052634815364325044\n",
      "train loss:0.006852919996093986\n",
      "train loss:0.008452236386617202\n",
      "train loss:0.043259739681125335\n",
      "train loss:0.03235386630160675\n",
      "train loss:0.01505141890960987\n",
      "train loss:0.0857433140354281\n",
      "train loss:0.021461106745087985\n",
      "train loss:0.04379990202380431\n",
      "train loss:0.0541932432547264\n",
      "train loss:0.06693984009288438\n",
      "train loss:0.05051926342042866\n",
      "train loss:0.00954277299183708\n",
      "train loss:0.052407110490396695\n",
      "train loss:0.03948810280160744\n",
      "train loss:0.019576391743552617\n",
      "train loss:0.04601475783318354\n",
      "train loss:0.04050533927330416\n",
      "train loss:0.048913517714083386\n",
      "train loss:0.023055203399732202\n",
      "train loss:0.035205354916264586\n",
      "train loss:0.05998510682050693\n",
      "train loss:0.0344574159215172\n",
      "train loss:0.03756217096616536\n",
      "train loss:0.0685728033716655\n",
      "train loss:0.010444137695166533\n",
      "train loss:0.024436445081278693\n",
      "train loss:0.046746381647554444\n",
      "train loss:0.05862679377598012\n",
      "train loss:0.023975526819272223\n",
      "train loss:0.03268409028477787\n",
      "train loss:0.02733567903366272\n",
      "train loss:0.03756531050045974\n",
      "train loss:0.05078633807441896\n",
      "train loss:0.016620077642967263\n",
      "train loss:0.01799935618060076\n",
      "train loss:0.04672091948543947\n",
      "train loss:0.025864661299861473\n",
      "train loss:0.03457855030937523\n",
      "train loss:0.034484280823258454\n",
      "train loss:0.02150205439530043\n",
      "train loss:0.023847260416629292\n",
      "train loss:0.004226869612416543\n",
      "train loss:0.02135555088270136\n",
      "train loss:0.09503819701502253\n",
      "train loss:0.019854226214543625\n",
      "train loss:0.07619692588777022\n",
      "train loss:0.013837695351397146\n",
      "train loss:0.015395243730631294\n",
      "train loss:0.01941250932972432\n",
      "train loss:0.03631645313935037\n",
      "train loss:0.026301578276246188\n",
      "train loss:0.057731102834361614\n",
      "train loss:0.024919814596184343\n",
      "train loss:0.008794063373419399\n",
      "train loss:0.025466623402007848\n",
      "train loss:0.02615752866505243\n",
      "train loss:0.1178897611004282\n",
      "train loss:0.08644829254118953\n",
      "train loss:0.010089481022411877\n",
      "train loss:0.065641821142706\n",
      "train loss:0.022148686463178833\n",
      "train loss:0.07503622510926357\n",
      "train loss:0.05574850157014148\n",
      "train loss:0.05262515198050491\n",
      "train loss:0.0026432142484632258\n",
      "train loss:0.015462202508214862\n",
      "train loss:0.035006111891603124\n",
      "train loss:0.014706875134682692\n",
      "train loss:0.013947421905443857\n",
      "train loss:0.024562240749201013\n",
      "train loss:0.033410633106224996\n",
      "train loss:0.0698389883207559\n",
      "train loss:0.022718540191439228\n",
      "train loss:0.028160765621982192\n",
      "train loss:0.0732396446467737\n",
      "train loss:0.02589425363021058\n",
      "train loss:0.00944435653454629\n",
      "train loss:0.03563172133856349\n",
      "train loss:0.02116802715321317\n",
      "train loss:0.020489216091466186\n",
      "train loss:0.052785079616974345\n",
      "train loss:0.005217789899550571\n",
      "train loss:0.05608980725682879\n",
      "train loss:0.02230933882981455\n",
      "train loss:0.1104790144290486\n",
      "train loss:0.07749837141635368\n",
      "train loss:0.03875756909815095\n",
      "train loss:0.07812710011130526\n",
      "train loss:0.022904397810690962\n",
      "train loss:0.03933429504196583\n",
      "train loss:0.14747051465936192\n",
      "train loss:0.05427922509041558\n",
      "train loss:0.038401378210222555\n",
      "train loss:0.010001063432627178\n",
      "train loss:0.0033235865711304016\n",
      "train loss:0.017320332942726595\n",
      "train loss:0.031663022761618524\n",
      "train loss:0.02278761947989266\n",
      "train loss:0.04530729876402699\n",
      "train loss:0.05851284075237796\n",
      "train loss:0.014070281928564698\n",
      "train loss:0.04755510474548856\n",
      "train loss:0.030517078036937775\n",
      "train loss:0.01714119739755499\n",
      "train loss:0.022292988795295045\n",
      "train loss:0.0394106149877264\n",
      "train loss:0.01578980766677712\n",
      "train loss:0.025539843810003026\n",
      "train loss:0.02902376112367116\n",
      "train loss:0.05080562772685981\n",
      "train loss:0.0404982914432505\n",
      "train loss:0.0437267591405167\n",
      "train loss:0.1022980709768275\n",
      "train loss:0.03779521109987686\n",
      "train loss:0.008592091058478331\n",
      "train loss:0.025433310747296812\n",
      "train loss:0.05479583436677243\n",
      "train loss:0.0313484764310214\n",
      "train loss:0.028145567138482007\n",
      "train loss:0.003720543406669567\n",
      "train loss:0.01130233261849093\n",
      "train loss:0.026478970807757143\n",
      "train loss:0.02745897899879481\n",
      "train loss:0.08782096139139334\n",
      "train loss:0.10707701562148342\n",
      "train loss:0.030811164268517186\n",
      "train loss:0.012090021205111134\n",
      "train loss:0.01492511034120854\n",
      "train loss:0.009269603135996044\n",
      "train loss:0.018749222021646642\n",
      "train loss:0.019865366091942293\n",
      "train loss:0.014073167795323775\n",
      "train loss:0.029627612713576147\n",
      "train loss:0.020348757559848045\n",
      "train loss:0.010339895628284645\n",
      "train loss:0.05246024151244188\n",
      "train loss:0.047380055865336536\n",
      "train loss:0.01263561599977718\n",
      "train loss:0.005340049061566935\n",
      "train loss:0.021564866956423706\n",
      "train loss:0.02141704047583356\n",
      "train loss:0.0337206057798202\n",
      "train loss:0.06962003995287891\n",
      "train loss:0.05735678478042798\n",
      "train loss:0.05494807223434638\n",
      "train loss:0.023273063025305935\n",
      "train loss:0.09317430636170722\n",
      "train loss:0.09282719652574445\n",
      "train loss:0.02050122436876567\n",
      "train loss:0.015899194268267386\n",
      "train loss:0.008285940381857565\n",
      "train loss:0.041080161411041524\n",
      "train loss:0.05261444756221761\n",
      "train loss:0.04632113301188765\n",
      "train loss:0.018300224699708875\n",
      "train loss:0.09149360843630658\n",
      "train loss:0.010922378975613989\n",
      "train loss:0.06220492790047466\n",
      "train loss:0.09937003884439377\n",
      "train loss:0.019880124695887225\n",
      "train loss:0.06678427382100974\n",
      "train loss:0.01774769256580054\n",
      "train loss:0.036467362750007344\n",
      "train loss:0.027710638946170353\n",
      "train loss:0.009070888087077532\n",
      "train loss:0.016321808233331056\n",
      "train loss:0.03175182364926361\n",
      "train loss:0.014300950351929615\n",
      "train loss:0.07398407086734676\n",
      "train loss:0.01591968584698768\n",
      "train loss:0.017479114377109698\n",
      "train loss:0.03672074858409366\n",
      "train loss:0.02686553783175195\n",
      "train loss:0.013838273232165006\n",
      "train loss:0.022605636743280522\n",
      "train loss:0.03875054606088555\n",
      "train loss:0.01813387939326642\n",
      "train loss:0.009529464795288255\n",
      "train loss:0.03795502826353221\n",
      "train loss:0.07307167840186143\n",
      "train loss:0.03536602222596591\n",
      "train loss:0.06054566442815507\n",
      "train loss:0.05295393427560964\n",
      "train loss:0.0519793706661027\n",
      "train loss:0.06645779529059381\n",
      "train loss:0.004319164016193877\n",
      "train loss:0.020118229555857788\n",
      "train loss:0.050975939734415165\n",
      "train loss:0.043191121235178945\n",
      "train loss:0.03642286687697725\n",
      "train loss:0.03395225194069481\n",
      "train loss:0.019140759963715146\n",
      "train loss:0.04459299917413219\n",
      "train loss:0.05090799664628938\n",
      "train loss:0.04433932221861851\n",
      "train loss:0.023935601125550307\n",
      "train loss:0.02863501905283417\n",
      "train loss:0.019061129391482367\n",
      "train loss:0.011737666818778369\n",
      "train loss:0.017853158326872773\n",
      "train loss:0.023743078468238723\n",
      "train loss:0.010329485524506788\n",
      "train loss:0.014264670918630817\n",
      "train loss:0.013703455183918307\n",
      "train loss:0.05296086362880011\n",
      "train loss:0.017394406661160954\n",
      "train loss:0.01332376923205058\n",
      "train loss:0.017883043616795866\n",
      "train loss:0.024173717331835168\n",
      "train loss:0.022744570183144363\n",
      "train loss:0.005614160531387132\n",
      "train loss:0.08005171334593346\n",
      "train loss:0.03751510337674815\n",
      "train loss:0.02466980305759715\n",
      "train loss:0.02611973084340489\n",
      "train loss:0.033828251392830855\n",
      "train loss:0.0167454238579232\n",
      "train loss:0.022961504940724985\n",
      "train loss:0.018647192728264286\n",
      "train loss:0.02368263257573887\n",
      "train loss:0.004030462274459074\n",
      "train loss:0.029012961313947636\n",
      "train loss:0.019310265839962412\n",
      "train loss:0.022892720499736875\n",
      "train loss:0.06983313216535093\n",
      "train loss:0.009408890860654901\n",
      "train loss:0.04934781871474568\n",
      "train loss:0.02955523480215718\n",
      "train loss:0.025733506090278035\n",
      "train loss:0.011984284201791985\n",
      "train loss:0.036605316840792525\n",
      "train loss:0.025195841174025947\n",
      "train loss:0.015138191188564314\n",
      "train loss:0.08518405035373626\n",
      "=== epoch:5, train acc:0.987, test acc:0.98 ===\n",
      "train loss:0.012438340182372666\n",
      "train loss:0.04297633862763286\n",
      "train loss:0.01396369024361913\n",
      "train loss:0.01181463983563469\n",
      "train loss:0.03638995338239698\n",
      "train loss:0.02139788522567882\n",
      "train loss:0.0343163427034791\n",
      "train loss:0.005971265418937281\n",
      "train loss:0.009009593223768658\n",
      "train loss:0.040282748532306185\n",
      "train loss:0.04111116638872762\n",
      "train loss:0.02484896817368368\n",
      "train loss:0.03154072122938055\n",
      "train loss:0.06513899589809587\n",
      "train loss:0.026268891514105545\n",
      "train loss:0.014500275972518475\n",
      "train loss:0.02379581132265221\n",
      "train loss:0.018209135304933902\n",
      "train loss:0.013081682185298111\n",
      "train loss:0.037994090282497575\n",
      "train loss:0.014253007337231081\n",
      "train loss:0.008176144614219883\n",
      "train loss:0.041066682799285485\n",
      "train loss:0.0698543382437068\n",
      "train loss:0.044703085605660886\n",
      "train loss:0.017968623074072786\n",
      "train loss:0.04059493891255033\n",
      "train loss:0.01909635837473171\n",
      "train loss:0.009567453638977154\n",
      "train loss:0.017551060628814275\n",
      "train loss:0.06034603503446167\n",
      "train loss:0.0335755270613555\n",
      "train loss:0.10098464076749612\n",
      "train loss:0.025077367949043538\n",
      "train loss:0.0062268293647744265\n",
      "train loss:0.0033520957852170097\n",
      "train loss:0.026144652464200016\n",
      "train loss:0.03674120588490076\n",
      "train loss:0.047125652236537405\n",
      "train loss:0.011014664013034131\n",
      "train loss:0.026329266170203716\n",
      "train loss:0.04689199109394332\n",
      "train loss:0.008058302557901863\n",
      "train loss:0.0037027696754119792\n",
      "train loss:0.027694270159940446\n",
      "train loss:0.04760456638992517\n",
      "train loss:0.07113802601869755\n",
      "train loss:0.022234017310564867\n",
      "train loss:0.05431686413936581\n",
      "train loss:0.005662874686903656\n",
      "train loss:0.014006946084260459\n",
      "train loss:0.013765495934666144\n",
      "train loss:0.04100484299153288\n",
      "train loss:0.021993146608814546\n",
      "train loss:0.03915406138882256\n",
      "train loss:0.020877073401909335\n",
      "train loss:0.08238104747075488\n",
      "train loss:0.00596934460110641\n",
      "train loss:0.032438798710406995\n",
      "train loss:0.006208508260296846\n",
      "train loss:0.006376645944826525\n",
      "train loss:0.027853137481077823\n",
      "train loss:0.045884298136667064\n",
      "train loss:0.008687670733998884\n",
      "train loss:0.024719194635718547\n",
      "train loss:0.0115996264792115\n",
      "train loss:0.013137183367614056\n",
      "train loss:0.07522637452863175\n",
      "train loss:0.040505013686843186\n",
      "train loss:0.016265381515603965\n",
      "train loss:0.01279467445037402\n",
      "train loss:0.049830590199982046\n",
      "train loss:0.01838032290006148\n",
      "train loss:0.010288455116774588\n",
      "train loss:0.04770847551050657\n",
      "train loss:0.01748637648717158\n",
      "train loss:0.013939509287547154\n",
      "train loss:0.023702775761513806\n",
      "train loss:0.004357528262729326\n",
      "train loss:0.03401652152912451\n",
      "train loss:0.05347074508759699\n",
      "train loss:0.009187313849784417\n",
      "train loss:0.017506163085522566\n",
      "train loss:0.037678073465415636\n",
      "train loss:0.016376274313617586\n",
      "train loss:0.014541088419905385\n",
      "train loss:0.007798910450219652\n",
      "train loss:0.015327106045440858\n",
      "train loss:0.019984602368153324\n",
      "train loss:0.04337314505539806\n",
      "train loss:0.06089750317898665\n",
      "train loss:0.04057494355877597\n",
      "train loss:0.05871085513080887\n",
      "train loss:0.015946875528511494\n",
      "train loss:0.010330789239866594\n",
      "train loss:0.06875780926115872\n",
      "train loss:0.009738966068872664\n",
      "train loss:0.014488415766814327\n",
      "train loss:0.0062023654789681235\n",
      "train loss:0.03856134199327054\n",
      "train loss:0.0037348038904196483\n",
      "train loss:0.07152881378953452\n",
      "train loss:0.06886298979571812\n",
      "train loss:0.021918244609072296\n",
      "train loss:0.03533043333246692\n",
      "train loss:0.009093512585024537\n",
      "train loss:0.0142916806284293\n",
      "train loss:0.02518618007888713\n",
      "train loss:0.024764369844497418\n",
      "train loss:0.011439387767688922\n",
      "train loss:0.021388113116114896\n",
      "train loss:0.033406212819034956\n",
      "train loss:0.010967472921231987\n",
      "train loss:0.02888788546738627\n",
      "train loss:0.02396694491629454\n",
      "train loss:0.01705985717346127\n",
      "train loss:0.022900482722571888\n",
      "train loss:0.05629288303995779\n",
      "train loss:0.0684703436585683\n",
      "train loss:0.029404430711593887\n",
      "train loss:0.025325913738021896\n",
      "train loss:0.005148596096936091\n",
      "train loss:0.04080779218237828\n",
      "train loss:0.07688932019704034\n",
      "train loss:0.014088450530423879\n",
      "train loss:0.008906261766881996\n",
      "train loss:0.016473834804606992\n",
      "train loss:0.0417001759856592\n",
      "train loss:0.05415544546429746\n",
      "train loss:0.039622582030225166\n",
      "train loss:0.025863932254412116\n",
      "train loss:0.13814792735889803\n",
      "train loss:0.04735676647559749\n",
      "train loss:0.018841475739828074\n",
      "train loss:0.025160206442386496\n",
      "train loss:0.002479419230983708\n",
      "train loss:0.056746485338622366\n",
      "train loss:0.017343794249523026\n",
      "train loss:0.03164045699792158\n",
      "train loss:0.039842150543780146\n",
      "train loss:0.01833064676685571\n",
      "train loss:0.01479842559377329\n",
      "train loss:0.03930366590268172\n",
      "train loss:0.017045670757884653\n",
      "train loss:0.0437367525184769\n",
      "train loss:0.0379816005649674\n",
      "train loss:0.05137392371553994\n",
      "train loss:0.022607564687814708\n",
      "train loss:0.026511983867776766\n",
      "train loss:0.012924655990077744\n",
      "train loss:0.1085223734792845\n",
      "train loss:0.05457650325854019\n",
      "train loss:0.023134460076097093\n",
      "train loss:0.040284825820147974\n",
      "train loss:0.09492978111330799\n",
      "train loss:0.006730819546039669\n",
      "train loss:0.07697701677229989\n",
      "train loss:0.056701726712456324\n",
      "train loss:0.06003968148861737\n",
      "train loss:0.015520611367123288\n",
      "train loss:0.06057146446102715\n",
      "train loss:0.1329592978172505\n",
      "train loss:0.010358888279273525\n",
      "train loss:0.022588165519694045\n",
      "train loss:0.006988116704597324\n",
      "train loss:0.055858036902381863\n",
      "train loss:0.11220184775717128\n",
      "train loss:0.010738210795293864\n",
      "train loss:0.04158834636841902\n",
      "train loss:0.008979931939918235\n",
      "train loss:0.06172886383527685\n",
      "train loss:0.028803714142851065\n",
      "train loss:0.036366440782775365\n",
      "train loss:0.07218836182893203\n",
      "train loss:0.07515589237643464\n",
      "train loss:0.07620842926315659\n",
      "train loss:0.010366533171234604\n",
      "train loss:0.02505295826357838\n",
      "train loss:0.011690929484881788\n",
      "train loss:0.06404447820002174\n",
      "train loss:0.05129367607483618\n",
      "train loss:0.009871587039358228\n",
      "train loss:0.030810784860974147\n",
      "train loss:0.013127476609682592\n",
      "train loss:0.016502705928575653\n",
      "train loss:0.016457194471297256\n",
      "train loss:0.04126609551683149\n",
      "train loss:0.02083359809418661\n",
      "train loss:0.05165048747552742\n",
      "train loss:0.02439787429735601\n",
      "train loss:0.03864936135744183\n",
      "train loss:0.05505427440144211\n",
      "train loss:0.01904641444507342\n",
      "train loss:0.010582197260160879\n",
      "train loss:0.012641749910872762\n",
      "train loss:0.011815841481182579\n",
      "train loss:0.07287897956003526\n",
      "train loss:0.015072411287758491\n",
      "train loss:0.02790990026456346\n",
      "train loss:0.0729228920531698\n",
      "train loss:0.006341358557984388\n",
      "train loss:0.01986182431537677\n",
      "train loss:0.029278404684955982\n",
      "train loss:0.040806195708683025\n",
      "train loss:0.020396167343282107\n",
      "train loss:0.003964929295262446\n",
      "train loss:0.03466821817916655\n",
      "train loss:0.04191096702590154\n",
      "train loss:0.021023623575964684\n",
      "train loss:0.0062912441848379\n",
      "train loss:0.08048941141324852\n",
      "train loss:0.012502658980250608\n",
      "train loss:0.01874922300600883\n",
      "train loss:0.10033172496760474\n",
      "train loss:0.012216129713612585\n",
      "train loss:0.020007382324037287\n",
      "train loss:0.0333291656423709\n",
      "train loss:0.018990680897460265\n",
      "train loss:0.09896336662198825\n",
      "train loss:0.03908160609706286\n",
      "train loss:0.03192334820101496\n",
      "train loss:0.010177090714887047\n",
      "train loss:0.031175745197911806\n",
      "train loss:0.01600229889401697\n",
      "train loss:0.04081497665277915\n",
      "train loss:0.01442899919727026\n",
      "train loss:0.008014053836199197\n",
      "train loss:0.044443015791699\n",
      "train loss:0.03561679337109008\n",
      "train loss:0.027402558219125253\n",
      "train loss:0.056403039563682526\n",
      "train loss:0.014800640051805541\n",
      "train loss:0.038064640295895194\n",
      "train loss:0.012413935749962624\n",
      "train loss:0.042567673848974884\n",
      "train loss:0.01761498301897584\n",
      "train loss:0.00740774414629959\n",
      "train loss:0.007348692759718297\n",
      "train loss:0.01932343423548016\n",
      "train loss:0.02676974993846868\n",
      "train loss:0.01987311475016876\n",
      "train loss:0.005701576995891434\n",
      "train loss:0.023770936843404193\n",
      "train loss:0.01692946938377042\n",
      "train loss:0.0073349714642753795\n",
      "train loss:0.008885560223514301\n",
      "train loss:0.03198811868586211\n",
      "train loss:0.03979454149762379\n",
      "train loss:0.015727260634013557\n",
      "train loss:0.025925251986247632\n",
      "train loss:0.02161593035954666\n",
      "train loss:0.07437322361864546\n",
      "train loss:0.01098449466693324\n",
      "train loss:0.004628188168697139\n",
      "train loss:0.024156455641331384\n",
      "train loss:0.009938316481832667\n",
      "train loss:0.012995336039397754\n",
      "train loss:0.0627773835969121\n",
      "train loss:0.01676358801177791\n",
      "train loss:0.02379359839380776\n",
      "train loss:0.025137729719494125\n",
      "train loss:0.014259193803248854\n",
      "train loss:0.06523539595489278\n",
      "train loss:0.01972248891067507\n",
      "train loss:0.03639804900525412\n",
      "train loss:0.054307373701618956\n",
      "train loss:0.07441861457663458\n",
      "train loss:0.16431256778328088\n",
      "train loss:0.011142007970437408\n",
      "train loss:0.09618474368906242\n",
      "train loss:0.052712808645086796\n",
      "train loss:0.01837216625713975\n",
      "train loss:0.02419164693695647\n",
      "train loss:0.00942533783472796\n",
      "train loss:0.01845431395472174\n",
      "train loss:0.029524691767680685\n",
      "train loss:0.010402480123772782\n",
      "train loss:0.039441295376607524\n",
      "train loss:0.02286820868873732\n",
      "train loss:0.052017292382117546\n",
      "train loss:0.02794452587769818\n",
      "train loss:0.03836730088593926\n",
      "train loss:0.012019625401045739\n",
      "train loss:0.008681244108863943\n",
      "train loss:0.05138616159113787\n",
      "train loss:0.026038307590421205\n",
      "train loss:0.02617360237919167\n",
      "train loss:0.008238335525480995\n",
      "train loss:0.033480037368332737\n",
      "train loss:0.02454072288369616\n",
      "train loss:0.0277648711345259\n",
      "train loss:0.01294826411851758\n",
      "train loss:0.019705668049482362\n",
      "train loss:0.011583780328585995\n",
      "train loss:0.007397766978386755\n",
      "train loss:0.006303968691367338\n",
      "train loss:0.01660333705256419\n",
      "train loss:0.023555080733201575\n",
      "train loss:0.008004645348465766\n",
      "train loss:0.029396123851872333\n",
      "train loss:0.026664995126539467\n",
      "train loss:0.03005947846918189\n",
      "train loss:0.014648030569292356\n",
      "train loss:0.06216374454539795\n",
      "train loss:0.0552228349644514\n",
      "train loss:0.004672273712788505\n",
      "train loss:0.03560992651414657\n",
      "train loss:0.009656026960797023\n",
      "train loss:0.024007288599179274\n",
      "train loss:0.05793888344898214\n",
      "train loss:0.027152909353938392\n",
      "train loss:0.016196213916561605\n",
      "train loss:0.014093451378244939\n",
      "train loss:0.029455420290299197\n",
      "train loss:0.04652338794533688\n",
      "train loss:0.08487441828622468\n",
      "train loss:0.038114303797273576\n",
      "train loss:0.024825961234698393\n",
      "train loss:0.014461710337796085\n",
      "train loss:0.02398507501447053\n",
      "train loss:0.02878119150469814\n",
      "train loss:0.031588666768026304\n",
      "train loss:0.015521411902669488\n",
      "train loss:0.12019600150761268\n",
      "train loss:0.008004394869483596\n",
      "train loss:0.030654050778096745\n",
      "train loss:0.06538952388282207\n",
      "train loss:0.005883373997003831\n",
      "train loss:0.03682097208162772\n",
      "train loss:0.035800413247532054\n",
      "train loss:0.0729683842690017\n",
      "train loss:0.023486428149575266\n",
      "train loss:0.0189348154713843\n",
      "train loss:0.00862701628657141\n",
      "train loss:0.07464507345716646\n",
      "train loss:0.006646016722304268\n",
      "train loss:0.03856598936068756\n",
      "train loss:0.01594854027151628\n",
      "train loss:0.027789511492015233\n",
      "train loss:0.020839415532307594\n",
      "train loss:0.024186248872092513\n",
      "train loss:0.026528967392388005\n",
      "train loss:0.03679042491897876\n",
      "train loss:0.04420358876010951\n",
      "train loss:0.017800278897164522\n",
      "train loss:0.03838215776102179\n",
      "train loss:0.016172630724695877\n",
      "train loss:0.0302563993167568\n",
      "train loss:0.01807483157507473\n",
      "train loss:0.0753877224885861\n",
      "train loss:0.041229769021077466\n",
      "train loss:0.01869698697717131\n",
      "train loss:0.04135451894817368\n",
      "train loss:0.06427147671554947\n",
      "train loss:0.004724273299673481\n",
      "train loss:0.06460339708012619\n",
      "train loss:0.007604209663105899\n",
      "train loss:0.006123111608733447\n",
      "train loss:0.029860423405627622\n",
      "train loss:0.004705369704935959\n",
      "train loss:0.04626333447758717\n",
      "train loss:0.013661124836664023\n",
      "train loss:0.030394103609372627\n",
      "train loss:0.03442628466477013\n",
      "train loss:0.0553849336317467\n",
      "train loss:0.029479342209876268\n",
      "train loss:0.02605003907194259\n",
      "train loss:0.02821801932212208\n",
      "train loss:0.020220275505579796\n",
      "train loss:0.023071821760696963\n",
      "train loss:0.0028468495504700353\n",
      "train loss:0.045585552949082515\n",
      "train loss:0.04958309364497068\n",
      "train loss:0.02231885445422943\n",
      "train loss:0.009231578402552969\n",
      "train loss:0.024964993593644534\n",
      "train loss:0.021977125563174677\n",
      "train loss:0.00962255476092312\n",
      "train loss:0.037876182909906825\n",
      "train loss:0.01010410459566496\n",
      "train loss:0.015002511026319144\n",
      "train loss:0.01507934242112976\n",
      "train loss:0.012340136597949697\n",
      "train loss:0.026302843081021973\n",
      "train loss:0.002417008371131971\n",
      "train loss:0.021387070158085938\n",
      "train loss:0.018033813275967748\n",
      "train loss:0.010937968680097956\n",
      "train loss:0.08759484283706435\n",
      "train loss:0.08238909239664721\n",
      "train loss:0.02525906866729437\n",
      "train loss:0.015432656526113309\n",
      "train loss:0.03885771762180197\n",
      "train loss:0.01386291133102747\n",
      "train loss:0.04745844801523644\n",
      "train loss:0.02753176972698822\n",
      "train loss:0.02536834419118866\n",
      "train loss:0.05228938014051247\n",
      "train loss:0.10192332381251487\n",
      "train loss:0.013525403137426208\n",
      "train loss:0.05552811060228868\n",
      "train loss:0.007949080355311097\n",
      "train loss:0.019283942433401133\n",
      "train loss:0.0224416414942162\n",
      "train loss:0.020342387909602785\n",
      "train loss:0.012394876992129666\n",
      "train loss:0.007390597431030259\n",
      "train loss:0.014273903789990565\n",
      "train loss:0.029500583429771843\n",
      "train loss:0.04220325145384682\n",
      "train loss:0.011540526198023683\n",
      "train loss:0.013361168082627817\n",
      "train loss:0.019520618625724483\n",
      "train loss:0.03536516344676543\n",
      "train loss:0.06607411272490787\n",
      "train loss:0.009181352972872987\n",
      "train loss:0.0050254154944313\n",
      "train loss:0.09515288739518502\n",
      "train loss:0.009542262809962818\n",
      "train loss:0.02546878259192788\n",
      "train loss:0.06027122220583177\n",
      "train loss:0.0031658168190201934\n",
      "train loss:0.00657717707573276\n",
      "train loss:0.06285500987084139\n",
      "train loss:0.12607545318665905\n",
      "train loss:0.009695044900543064\n",
      "train loss:0.0046106020223032175\n",
      "train loss:0.014120392123053884\n",
      "train loss:0.022648085054087438\n",
      "train loss:0.0104183619772799\n",
      "train loss:0.010878885431648411\n",
      "train loss:0.029190732603706752\n",
      "train loss:0.0179199303894786\n",
      "train loss:0.05531519155102632\n",
      "train loss:0.024471471251302924\n",
      "train loss:0.01657685738223877\n",
      "train loss:0.05990861087107147\n",
      "train loss:0.005720605504920689\n",
      "train loss:0.07592018429179223\n",
      "train loss:0.07331693898379968\n",
      "train loss:0.03010123464461437\n",
      "train loss:0.023533143243793412\n",
      "train loss:0.04436257318493391\n",
      "train loss:0.05975349128290939\n",
      "train loss:0.01843914299620286\n",
      "train loss:0.030299888969837686\n",
      "train loss:0.010819205360613688\n",
      "train loss:0.059475543117389014\n",
      "train loss:0.014728522340861647\n",
      "train loss:0.03266035208290445\n",
      "train loss:0.018767880891447354\n",
      "train loss:0.021178095058424738\n",
      "train loss:0.0115408264629667\n",
      "train loss:0.024214118542502553\n",
      "train loss:0.011929115722813386\n",
      "train loss:0.020077545261521806\n",
      "train loss:0.0605799522309533\n",
      "train loss:0.02806761747095906\n",
      "train loss:0.014173503415982995\n",
      "train loss:0.01344150339057571\n",
      "train loss:0.028758437441999556\n",
      "train loss:0.007430239621816876\n",
      "train loss:0.012870720533168314\n",
      "train loss:0.025611317891576436\n",
      "train loss:0.009977296107412308\n",
      "train loss:0.005881754827549926\n",
      "train loss:0.020142871220466332\n",
      "train loss:0.02244950043572866\n",
      "train loss:0.017499429695869498\n",
      "train loss:0.015560776635539899\n",
      "train loss:0.014400858989584748\n",
      "train loss:0.04154430680793103\n",
      "train loss:0.023861129033513652\n",
      "train loss:0.05664670976531326\n",
      "train loss:0.004714728762006821\n",
      "train loss:0.009747544807830888\n",
      "train loss:0.013911101069669253\n",
      "train loss:0.020045135524992243\n",
      "train loss:0.022898976055451006\n",
      "train loss:0.013927097367970251\n",
      "train loss:0.00385277331375393\n",
      "train loss:0.018286421904457912\n",
      "train loss:0.049220114562958515\n",
      "train loss:0.01437483732350771\n",
      "train loss:0.020696888817831355\n",
      "train loss:0.03183661716798744\n",
      "train loss:0.024853126296298735\n",
      "train loss:0.006385609983557673\n",
      "train loss:0.026007016328364375\n",
      "train loss:0.022104896452367427\n",
      "train loss:0.06943325674817331\n",
      "train loss:0.003918066055338317\n",
      "train loss:0.014950065685001521\n",
      "train loss:0.006513461025120619\n",
      "train loss:0.01752634067336676\n",
      "train loss:0.008715340247222229\n",
      "train loss:0.007306699873807652\n",
      "train loss:0.07145252345339544\n",
      "train loss:0.011904115758748916\n",
      "train loss:0.06477356299987949\n",
      "train loss:0.020302486467488317\n",
      "train loss:0.011938953634991402\n",
      "train loss:0.03692004547658658\n",
      "train loss:0.028400921049431115\n",
      "train loss:0.031798892864818025\n",
      "train loss:0.0029545512502790697\n",
      "train loss:0.013021568330487414\n",
      "train loss:0.04927393854032874\n",
      "train loss:0.03181799517029458\n",
      "train loss:0.026244786141871354\n",
      "train loss:0.02620079281210439\n",
      "train loss:0.01777247871224936\n",
      "train loss:0.024096293907479038\n",
      "train loss:0.04703302399112139\n",
      "train loss:0.016537653984133212\n",
      "train loss:0.007307533472206937\n",
      "train loss:0.01593606543374785\n",
      "train loss:0.028192162089019663\n",
      "train loss:0.013985440834221286\n",
      "train loss:0.005187946080138911\n",
      "train loss:0.005445876418821966\n",
      "train loss:0.024083766235241114\n",
      "train loss:0.007370671095996981\n",
      "train loss:0.045174077167041415\n",
      "train loss:0.0456087798741456\n",
      "train loss:0.01812216430148287\n",
      "train loss:0.009336870491145732\n",
      "train loss:0.016537331767439864\n",
      "train loss:0.012834971460439167\n",
      "train loss:0.008189759156335234\n",
      "train loss:0.013161293311783616\n",
      "train loss:0.03645912742507502\n",
      "train loss:0.0061711304596202155\n",
      "train loss:0.011158300767317345\n",
      "train loss:0.00810191432531569\n",
      "train loss:0.021127264554129374\n",
      "train loss:0.005687259435123453\n",
      "train loss:0.022452503372104127\n",
      "train loss:0.008421804142313208\n",
      "train loss:0.0035004976420827593\n",
      "train loss:0.01978484145642299\n",
      "train loss:0.039009137548348186\n",
      "train loss:0.07037768984945857\n",
      "train loss:0.007967932198941529\n",
      "train loss:0.012634106106433052\n",
      "train loss:0.003306729591624868\n",
      "train loss:0.0357306572619638\n",
      "train loss:0.006251185351830245\n",
      "train loss:0.004858748990917917\n",
      "train loss:0.01744237273621468\n",
      "train loss:0.0054994254360865805\n",
      "train loss:0.01035836509078567\n",
      "train loss:0.02699944818623447\n",
      "train loss:0.024613601317339792\n",
      "train loss:0.003992015825540329\n",
      "train loss:0.011183334445086351\n",
      "train loss:0.05008808790616723\n",
      "train loss:0.01735106785818532\n",
      "train loss:0.01573031748760624\n",
      "train loss:0.030592582908047002\n",
      "train loss:0.01961037021399958\n",
      "train loss:0.03181985452169764\n",
      "train loss:0.014122053806888494\n",
      "train loss:0.013852586107444009\n",
      "train loss:0.010084662865902485\n",
      "train loss:0.016827903090529887\n",
      "train loss:0.00996740979192379\n",
      "train loss:0.028797165455004926\n",
      "train loss:0.006609844226002046\n",
      "train loss:0.005573507309076531\n",
      "train loss:0.015302817385653871\n",
      "train loss:0.0040805566438996\n",
      "train loss:0.011635392169198473\n",
      "train loss:0.020114261409004366\n",
      "train loss:0.018952333851758783\n",
      "train loss:0.0026011790417754034\n",
      "train loss:0.011046208793593549\n",
      "train loss:0.011283818053708228\n",
      "train loss:0.008000570213098414\n",
      "train loss:0.011743480158417433\n",
      "train loss:0.006150516273088936\n",
      "train loss:0.029064927702229442\n",
      "train loss:0.0037121657154162014\n",
      "train loss:0.004964831418109772\n",
      "train loss:0.002879273495950326\n",
      "train loss:0.014467580741519244\n",
      "train loss:0.02397398859823587\n",
      "train loss:0.011549585574292108\n",
      "train loss:0.009917795845018477\n",
      "train loss:0.012259482866634627\n",
      "train loss:0.01963836667715287\n",
      "train loss:0.011852325619309951\n",
      "train loss:0.029427080414194274\n",
      "train loss:0.04609964738198835\n",
      "train loss:0.01895548698184344\n",
      "train loss:0.011507615479254439\n",
      "train loss:0.014697562453163018\n",
      "train loss:0.08847144672986401\n",
      "train loss:0.010419043755414073\n",
      "train loss:0.0012953621874634376\n",
      "=== epoch:6, train acc:0.988, test acc:0.985 ===\n",
      "train loss:0.01819114195780751\n",
      "train loss:0.023802804160025927\n",
      "train loss:0.006933667415033391\n",
      "train loss:0.004178300116551996\n",
      "train loss:0.015599224895919323\n",
      "train loss:0.055594422745811084\n",
      "train loss:0.01341554298744157\n",
      "train loss:0.008357425633519789\n",
      "train loss:0.023058122106590178\n",
      "train loss:0.017701619120453174\n",
      "train loss:0.024067836413059856\n",
      "train loss:0.04600915143378453\n",
      "train loss:0.03687007883145661\n",
      "train loss:0.03508402189927741\n",
      "train loss:0.015180232178512765\n",
      "train loss:0.01800993305228144\n",
      "train loss:0.023988397831076962\n",
      "train loss:0.01600692059375173\n",
      "train loss:0.0020225976118889307\n",
      "train loss:0.011293940598109602\n",
      "train loss:0.01895581841001386\n",
      "train loss:0.004221794162700179\n",
      "train loss:0.08913637915808598\n",
      "train loss:0.016994217566650153\n",
      "train loss:0.12140677407624237\n",
      "train loss:0.04473858837400645\n",
      "train loss:0.005531189524630445\n",
      "train loss:0.030661706431751857\n",
      "train loss:0.014336500930693321\n",
      "train loss:0.02869026523455368\n",
      "train loss:0.017941149775395352\n",
      "train loss:0.02201792912748596\n",
      "train loss:0.18039528627998488\n",
      "train loss:0.021929896830206328\n",
      "train loss:0.025251256311060965\n",
      "train loss:0.0033571081799146185\n",
      "train loss:0.009595466051383825\n",
      "train loss:0.013985312292851897\n",
      "train loss:0.08103791759283764\n",
      "train loss:0.005516727262772754\n",
      "train loss:0.0064115370918831184\n",
      "train loss:0.017608380202793945\n",
      "train loss:0.02130210725192641\n",
      "train loss:0.01090471894443384\n",
      "train loss:0.03325628883890453\n",
      "train loss:0.013288705714094599\n",
      "train loss:0.06259723961261733\n",
      "train loss:0.011800139792162571\n",
      "train loss:0.011251328375923545\n",
      "train loss:0.08931530652987954\n",
      "train loss:0.010175064638798033\n",
      "train loss:0.012773259087681224\n",
      "train loss:0.008172580190659674\n",
      "train loss:0.007643016906437775\n",
      "train loss:0.07585486247956556\n",
      "train loss:0.03475635909313952\n",
      "train loss:0.014706948369269578\n",
      "train loss:0.007844249003490638\n",
      "train loss:0.02299404638689267\n",
      "train loss:0.012725018442701304\n",
      "train loss:0.009680052750525919\n",
      "train loss:0.05344032363091424\n",
      "train loss:0.010479417843043695\n",
      "train loss:0.009194696227079566\n",
      "train loss:0.02921787208808855\n",
      "train loss:0.022812163813036815\n",
      "train loss:0.026180699795885675\n",
      "train loss:0.012479404549185535\n",
      "train loss:0.0424256894574577\n",
      "train loss:0.006766263802088181\n",
      "train loss:0.02091831018268201\n",
      "train loss:0.010194373396929966\n",
      "train loss:0.055761852448949684\n",
      "train loss:0.03769159785237337\n",
      "train loss:0.02367838368560241\n",
      "train loss:0.006411563842315727\n",
      "train loss:0.012858302664000548\n",
      "train loss:0.028816154164709413\n",
      "train loss:0.006016765429363346\n",
      "train loss:0.0526370833694952\n",
      "train loss:0.04210865005031785\n",
      "train loss:0.0025075720268354527\n",
      "train loss:0.005619501124948364\n",
      "train loss:0.009901374245486568\n",
      "train loss:0.0025265738041054795\n",
      "train loss:0.01084811449487359\n",
      "train loss:0.03191952657149267\n",
      "train loss:0.020773158548304412\n",
      "train loss:0.02627753451996805\n",
      "train loss:0.017016962060018227\n",
      "train loss:0.0124816748055563\n",
      "train loss:0.013203734492950732\n",
      "train loss:0.023239527400408834\n",
      "train loss:0.009882978102569651\n",
      "train loss:0.005468984162256543\n",
      "train loss:0.007263932177519465\n",
      "train loss:0.025494984499197244\n",
      "train loss:0.02075686409409165\n",
      "train loss:0.007174062558098934\n",
      "train loss:0.001545051566210595\n",
      "train loss:0.011538501011910125\n",
      "train loss:0.0123688980045421\n",
      "train loss:0.07010878102847927\n",
      "train loss:0.04588342968021166\n",
      "train loss:0.020841301056278686\n",
      "train loss:0.05032357406449045\n",
      "train loss:0.006107540681927873\n",
      "train loss:0.012666045172732152\n",
      "train loss:0.03932812650008443\n",
      "train loss:0.03036330271201839\n",
      "train loss:0.013591021747066865\n",
      "train loss:0.007288215924405821\n",
      "train loss:0.05009365757841569\n",
      "train loss:0.009490507143534093\n",
      "train loss:0.009375298538162413\n",
      "train loss:0.012104322441252828\n",
      "train loss:0.012376837686780022\n",
      "train loss:0.01258159114943327\n",
      "train loss:0.00782805262993961\n",
      "train loss:0.004962950663406108\n",
      "train loss:0.025434214456851758\n",
      "train loss:0.006414336138469934\n",
      "train loss:0.014900050605438642\n",
      "train loss:0.016974892326102095\n",
      "train loss:0.041362549305388956\n",
      "train loss:0.0102197246702661\n",
      "train loss:0.019757821003094936\n",
      "train loss:0.018199663102129723\n",
      "train loss:0.005336525614245458\n",
      "train loss:0.01830970267026361\n",
      "train loss:0.0071818411523661895\n",
      "train loss:0.010537021553648109\n",
      "train loss:0.04545346182023215\n",
      "train loss:0.010334918574566383\n",
      "train loss:0.015180567602963976\n",
      "train loss:0.03907478238003809\n",
      "train loss:0.014416472181349916\n",
      "train loss:0.02337493187169927\n",
      "train loss:0.016488401944287297\n",
      "train loss:0.020702679477573756\n",
      "train loss:0.030806468544719082\n",
      "train loss:0.010760176249228261\n",
      "train loss:0.042434910044111124\n",
      "train loss:0.014903466152174345\n",
      "train loss:0.036094183401688955\n",
      "train loss:0.015219561524615104\n",
      "train loss:0.03439071630862072\n",
      "train loss:0.009466639087395591\n",
      "train loss:0.008022608214588832\n",
      "train loss:0.004832413710726719\n",
      "train loss:0.08472697731112712\n",
      "train loss:0.023571060550436833\n",
      "train loss:0.010590553515711492\n",
      "train loss:0.019414146360138884\n",
      "train loss:0.006189380047790322\n",
      "train loss:0.009860995173239396\n",
      "train loss:0.04051840944158503\n",
      "train loss:0.011874742084981061\n",
      "train loss:0.004130315689909423\n",
      "train loss:0.02158069874356155\n",
      "train loss:0.011905527182126667\n",
      "train loss:0.006322682497809619\n",
      "train loss:0.018863201749327152\n",
      "train loss:0.03329181133112255\n",
      "train loss:0.008015435979437754\n",
      "train loss:0.006619927496079933\n",
      "train loss:0.00871178982234111\n",
      "train loss:0.012083415676904099\n",
      "train loss:0.007985977869347925\n",
      "train loss:0.007170275752935911\n",
      "train loss:0.01614741851084262\n",
      "train loss:0.010794177186580226\n",
      "train loss:0.021221081785034145\n",
      "train loss:0.04023849757214019\n",
      "train loss:0.05351579293089317\n",
      "train loss:0.014630846629697174\n",
      "train loss:0.005786210811315664\n",
      "train loss:0.023984393142956296\n",
      "train loss:0.020861757412162958\n",
      "train loss:0.032463159878466576\n",
      "train loss:0.005689779975565491\n",
      "train loss:0.017857850767561214\n",
      "train loss:0.021347911496459475\n",
      "train loss:0.044927465517472134\n",
      "train loss:0.009526566886533798\n",
      "train loss:0.00795958232259039\n",
      "train loss:0.022618993080152933\n",
      "train loss:0.012565282012870622\n",
      "train loss:0.031357796603744784\n",
      "train loss:0.06938062160819534\n",
      "train loss:0.018814866121046093\n",
      "train loss:0.0471445463594189\n",
      "train loss:0.05396188825098136\n",
      "train loss:0.01876281181723798\n",
      "train loss:0.040691230221492854\n",
      "train loss:0.02358079475196736\n",
      "train loss:0.008398616715218245\n",
      "train loss:0.018073996912296643\n",
      "train loss:0.012311159253012034\n",
      "train loss:0.017463616842550874\n",
      "train loss:0.003579538819357448\n",
      "train loss:0.05980653948106088\n",
      "train loss:0.027415415247345424\n",
      "train loss:0.024086890341237545\n",
      "train loss:0.06772501893361714\n",
      "train loss:0.02540265814590876\n",
      "train loss:0.007375390131382869\n",
      "train loss:0.03119696445769148\n",
      "train loss:0.06322689469397136\n",
      "train loss:0.019345384350205413\n",
      "train loss:0.13744755411437498\n",
      "train loss:0.12887521562135576\n",
      "train loss:0.01575835777575961\n",
      "train loss:0.04419774047754812\n",
      "train loss:0.015640252174691963\n",
      "train loss:0.03168403974115669\n",
      "train loss:0.02945374214182346\n",
      "train loss:0.04560896287170094\n",
      "train loss:0.006914229327012269\n",
      "train loss:0.018070764163812068\n",
      "train loss:0.0038371571621089113\n",
      "train loss:0.01726506383113262\n",
      "train loss:0.03994964961637605\n",
      "train loss:0.00498463851082706\n",
      "train loss:0.0026309984895663573\n",
      "train loss:0.011754939468951014\n",
      "train loss:0.009486230315382727\n",
      "train loss:0.015667138396183437\n",
      "train loss:0.02329982346996442\n",
      "train loss:0.012529973298282373\n",
      "train loss:0.015448654862217005\n",
      "train loss:0.02164421463247888\n",
      "train loss:0.037409629420827734\n",
      "train loss:0.01883587565322842\n",
      "train loss:0.014812183209703345\n",
      "train loss:0.03081177046366699\n",
      "train loss:0.021854327655317484\n",
      "train loss:0.03950813139003777\n",
      "train loss:0.010496025084109555\n",
      "train loss:0.015373103152218435\n",
      "train loss:0.006051327184395543\n",
      "train loss:0.006984143734342993\n",
      "train loss:0.008361982364738469\n",
      "train loss:0.007075300624872108\n",
      "train loss:0.022198009587611235\n",
      "train loss:0.03280526014331958\n",
      "train loss:0.0074188672891275535\n",
      "train loss:0.07563550370762577\n",
      "train loss:0.017791979301668866\n",
      "train loss:0.0217269976992376\n",
      "train loss:0.005580332159679433\n",
      "train loss:0.022443891363461206\n",
      "train loss:0.05819175922930185\n",
      "train loss:0.0053627069444653645\n",
      "train loss:0.008999853469379803\n",
      "train loss:0.012090420779201702\n",
      "train loss:0.020471892202276066\n",
      "train loss:0.03201867179752561\n",
      "train loss:0.017994150532578312\n",
      "train loss:0.014328882002152717\n",
      "train loss:0.009189029943177378\n",
      "train loss:0.00451295326253153\n",
      "train loss:0.022540624412416017\n",
      "train loss:0.04648819242987722\n",
      "train loss:0.013029426526979321\n",
      "train loss:0.03207201197695324\n",
      "train loss:0.015000344386425953\n",
      "train loss:0.005087013864149933\n",
      "train loss:0.017689556550999977\n",
      "train loss:0.01853349048412875\n",
      "train loss:0.05971204692889423\n",
      "train loss:0.027856814210466388\n",
      "train loss:0.0029370487504426336\n",
      "train loss:0.03324537802096385\n",
      "train loss:0.04183208099853296\n",
      "train loss:0.024430468604335984\n",
      "train loss:0.07085764731365667\n",
      "train loss:0.002021988395069819\n",
      "train loss:0.031005178475705407\n",
      "train loss:0.008291272128402038\n",
      "train loss:0.02012971576433571\n",
      "train loss:0.010935337713799682\n",
      "train loss:0.02908310775283469\n",
      "train loss:0.009601857843950953\n",
      "train loss:0.007734074323427751\n",
      "train loss:0.005931074741633525\n",
      "train loss:0.01116103085317679\n",
      "train loss:0.01843555936964089\n",
      "train loss:0.06605684457703237\n",
      "train loss:0.009171462105095975\n",
      "train loss:0.03090909519803417\n",
      "train loss:0.01736846628014482\n",
      "train loss:0.007134546854277913\n",
      "train loss:0.022149862480191938\n",
      "train loss:0.0033875591214200455\n",
      "train loss:0.050487952810386705\n",
      "train loss:0.07391062452794522\n",
      "train loss:0.018103196145304933\n",
      "train loss:0.008794110277225086\n",
      "train loss:0.005784486275419006\n",
      "train loss:0.03436029673009023\n",
      "train loss:0.006613738111762864\n",
      "train loss:0.04962582753601375\n",
      "train loss:0.016451693483660438\n",
      "train loss:0.0042431976071169684\n",
      "train loss:0.01850721658005991\n",
      "train loss:0.03695216830582069\n",
      "train loss:0.01273882378512289\n",
      "train loss:0.006805468193097981\n",
      "train loss:0.06400270370291287\n",
      "train loss:0.005866313858820551\n",
      "train loss:0.0024047293983882057\n",
      "train loss:0.042182031356982105\n",
      "train loss:0.04573597080174346\n",
      "train loss:0.010150598250619728\n",
      "train loss:0.009220233573924234\n",
      "train loss:0.00899436661774697\n",
      "train loss:0.00925405871824192\n",
      "train loss:0.0033394124688926673\n",
      "train loss:0.01913099101996154\n",
      "train loss:0.010349062619020011\n",
      "train loss:0.015216278319168419\n",
      "train loss:0.011676783157968582\n",
      "train loss:0.10274396183952023\n",
      "train loss:0.011974003917979807\n",
      "train loss:0.009211594904005692\n",
      "train loss:0.015281955702080572\n",
      "train loss:0.006467998157471022\n",
      "train loss:0.008976594096974046\n",
      "train loss:0.08820239474162664\n",
      "train loss:0.022144882427435585\n",
      "train loss:0.012714206036040432\n",
      "train loss:0.03589406680854632\n",
      "train loss:0.026580092669954408\n",
      "train loss:0.015014107143575935\n",
      "train loss:0.011503065400112781\n",
      "train loss:0.003221872636956632\n",
      "train loss:0.017711178913145543\n",
      "train loss:0.01325078586789502\n",
      "train loss:0.011687269072984001\n",
      "train loss:0.07008194089325244\n",
      "train loss:0.010009630045193885\n",
      "train loss:0.01902110792396379\n",
      "train loss:0.052985767125846496\n",
      "train loss:0.02845127438374048\n",
      "train loss:0.006102547205154666\n",
      "train loss:0.007687833817981105\n",
      "train loss:0.00804023108433074\n",
      "train loss:0.016214142600280215\n",
      "train loss:0.015735062849524438\n",
      "train loss:0.019372224668147385\n",
      "train loss:0.02602784042884141\n",
      "train loss:0.021239948291888536\n",
      "train loss:0.043390296554566755\n",
      "train loss:0.02625570239583409\n",
      "train loss:0.009727473290169422\n",
      "train loss:0.007196167273257062\n",
      "train loss:0.02210030753804536\n",
      "train loss:0.006649260841773265\n",
      "train loss:0.00961803137069192\n",
      "train loss:0.010016143289762112\n",
      "train loss:0.018588473665912264\n",
      "train loss:0.019494230662531344\n",
      "train loss:0.010448277765672141\n",
      "train loss:0.022077325854081033\n",
      "train loss:0.060656140341742174\n",
      "train loss:0.03007988785734502\n",
      "train loss:0.007371900440473423\n",
      "train loss:0.005862437398910808\n",
      "train loss:0.04429797471902342\n",
      "train loss:0.01044820233107547\n",
      "train loss:0.04348770503571087\n",
      "train loss:0.019141681758528692\n",
      "train loss:0.00769626098685499\n",
      "train loss:0.13370556073677786\n",
      "train loss:0.0027771100574494377\n",
      "train loss:0.013665617304360745\n",
      "train loss:0.007512429228709008\n",
      "train loss:0.006115511989658152\n",
      "train loss:0.06076507440460155\n",
      "train loss:0.04249301910207171\n",
      "train loss:0.040257185750199896\n",
      "train loss:0.05343465726200731\n",
      "train loss:0.04744910263610825\n",
      "train loss:0.03736455355688416\n",
      "train loss:0.07106919377472266\n",
      "train loss:0.010002243225218746\n",
      "train loss:0.014493618624056223\n",
      "train loss:0.02756976911029487\n",
      "train loss:0.00513706951838764\n",
      "train loss:0.09256266823459715\n",
      "train loss:0.014356031017558007\n",
      "train loss:0.016860651516186747\n",
      "train loss:0.04441561190650024\n",
      "train loss:0.01685680622848916\n",
      "train loss:0.024609418771666166\n",
      "train loss:0.016681113519105492\n",
      "train loss:0.017532411164409067\n",
      "train loss:0.018895220749668137\n",
      "train loss:0.013279013835506418\n",
      "train loss:0.00792405067807725\n",
      "train loss:0.005723466130439183\n",
      "train loss:0.004984953908851088\n",
      "train loss:0.02978704271635254\n",
      "train loss:0.0316750291771731\n",
      "train loss:0.029330189596984493\n",
      "train loss:0.011637726362468298\n",
      "train loss:0.022823217327389858\n",
      "train loss:0.05773798607697454\n",
      "train loss:0.04480982653974153\n",
      "train loss:0.011807849250042966\n",
      "train loss:0.014603664124943173\n",
      "train loss:0.011349970050492647\n",
      "train loss:0.060471244581225526\n",
      "train loss:0.011939442331474998\n",
      "train loss:0.1106394099825752\n",
      "train loss:0.00650941153318679\n",
      "train loss:0.005365411704196399\n",
      "train loss:0.02477460943618488\n",
      "train loss:0.008149513516987745\n",
      "train loss:0.025741130416691446\n",
      "train loss:0.033272716872373154\n",
      "train loss:0.0016669543621561892\n",
      "train loss:0.00747594170460265\n",
      "train loss:0.00447790179100371\n",
      "train loss:0.03140672461846949\n",
      "train loss:0.006150604954900072\n",
      "train loss:0.017896649042307287\n",
      "train loss:0.009369834276449315\n",
      "train loss:0.03154815215196826\n",
      "train loss:0.016774637386950775\n",
      "train loss:0.005379280068177391\n",
      "train loss:0.010343865333180952\n",
      "train loss:0.0052901810287533735\n",
      "train loss:0.007544510930915469\n",
      "train loss:0.02376472675057041\n",
      "train loss:0.03763071739695604\n",
      "train loss:0.048638922769251074\n",
      "train loss:0.0032329845099525823\n",
      "train loss:0.004552795981487354\n",
      "train loss:0.039931106569314014\n",
      "train loss:0.019415213975810384\n",
      "train loss:0.007516747787724682\n",
      "train loss:0.0038190732838118712\n",
      "train loss:0.03308129764194283\n",
      "train loss:0.00282453081593092\n",
      "train loss:0.018037068647800927\n",
      "train loss:0.027151581669335\n",
      "train loss:0.0053357799132360325\n",
      "train loss:0.025889722791334665\n",
      "train loss:0.00384367195061645\n",
      "train loss:0.017079765259790013\n",
      "train loss:0.03077431674992191\n",
      "train loss:0.042792784033937485\n",
      "train loss:0.053581706117733295\n",
      "train loss:0.010612667694767947\n",
      "train loss:0.023592984075330937\n",
      "train loss:0.003535576700175606\n",
      "train loss:0.025365264519877733\n",
      "train loss:0.007466199362062177\n",
      "train loss:0.011170328774662168\n",
      "train loss:0.003425461304965218\n",
      "train loss:0.013677419914331565\n",
      "train loss:0.026305665969788984\n",
      "train loss:0.00871511156817976\n",
      "train loss:0.0189833820886239\n",
      "train loss:0.011945609704606461\n",
      "train loss:0.018315026608535696\n",
      "train loss:0.01609019673845945\n",
      "train loss:0.014135482086789497\n",
      "train loss:0.03502038773870757\n",
      "train loss:0.05855917508500739\n",
      "train loss:0.015370919832643725\n",
      "train loss:0.16285183211548918\n",
      "train loss:0.005145519749011364\n",
      "train loss:0.07130206934521884\n",
      "train loss:0.05745667857780647\n",
      "train loss:0.02040190652475412\n",
      "train loss:0.008925614876034698\n",
      "train loss:0.003977232759752751\n",
      "train loss:0.0059617004682340205\n",
      "train loss:0.011505254821807441\n",
      "train loss:0.03946108460213584\n",
      "train loss:0.02333965351809521\n",
      "train loss:0.015000237767306914\n",
      "train loss:0.01712938889926348\n",
      "train loss:0.00475248125065436\n",
      "train loss:0.04070593550760557\n",
      "train loss:0.006275516100191815\n",
      "train loss:0.026833170839783062\n",
      "train loss:0.009795411634122099\n",
      "train loss:0.006712560560952462\n",
      "train loss:0.02270528082200247\n",
      "train loss:0.02044272237577238\n",
      "train loss:0.020282433412574758\n",
      "train loss:0.003561010960331098\n",
      "train loss:0.022005245139001695\n",
      "train loss:0.002783133368049065\n",
      "train loss:0.0633762878297746\n",
      "train loss:0.045832862964714144\n",
      "train loss:0.009831049127943832\n",
      "train loss:0.01235978046199681\n",
      "train loss:0.008587762386646953\n",
      "train loss:0.07458664674405453\n",
      "train loss:0.011223341025611167\n",
      "train loss:0.015107559770324137\n",
      "train loss:0.01331450440049446\n",
      "train loss:0.01957756883478277\n",
      "train loss:0.008712005769031665\n",
      "train loss:0.027630274432348736\n",
      "train loss:0.031196620393972408\n",
      "train loss:0.009540347353623895\n",
      "train loss:0.03332312663236052\n",
      "train loss:0.019876500194681564\n",
      "train loss:0.04882304573489219\n",
      "train loss:0.08702204123987534\n",
      "train loss:0.008381227769943907\n",
      "train loss:0.052935672723294205\n",
      "train loss:0.006206313336900561\n",
      "train loss:0.04749795443273062\n",
      "train loss:0.014677417281847427\n",
      "train loss:0.011682590527086196\n",
      "train loss:0.014438589080399983\n",
      "train loss:0.010973062587675277\n",
      "train loss:0.01393919629454582\n",
      "train loss:0.007578480120594171\n",
      "train loss:0.006504763066135331\n",
      "train loss:0.008159946823267766\n",
      "train loss:0.011562001447069118\n",
      "train loss:0.03268886179234974\n",
      "train loss:0.018048119740302484\n",
      "train loss:0.009943778801730996\n",
      "train loss:0.0054895509845430705\n",
      "train loss:0.02551440989067778\n",
      "train loss:0.01715435174451934\n",
      "train loss:0.0026698349675485372\n",
      "train loss:0.0024541569205240744\n",
      "train loss:0.015496719029169399\n",
      "train loss:0.008533079905712252\n",
      "train loss:0.05107762135479366\n",
      "train loss:0.02319033179555701\n",
      "train loss:0.012046854554916896\n",
      "train loss:0.015368159164464268\n",
      "train loss:0.005074320317122747\n",
      "train loss:0.033464633634094715\n",
      "train loss:0.006112789330958036\n",
      "train loss:0.010481661219002241\n",
      "train loss:0.00587332400707916\n",
      "train loss:0.022646317409441275\n",
      "train loss:0.029388007119064485\n",
      "train loss:0.019023289358001672\n",
      "train loss:0.00638035127861204\n",
      "train loss:0.010267809264242415\n",
      "train loss:0.008899848999116576\n",
      "train loss:0.026521670993984742\n",
      "train loss:0.003522264442875907\n",
      "train loss:0.01164579646759127\n",
      "train loss:0.006208729510937686\n",
      "train loss:0.006535958642147129\n",
      "train loss:0.02308867966834906\n",
      "train loss:0.010239330125553843\n",
      "train loss:0.042567245456819315\n",
      "train loss:0.008122259408395081\n",
      "train loss:0.003125221387530803\n",
      "train loss:0.04530249003316235\n",
      "train loss:0.00931202993501502\n",
      "train loss:0.009377078374161208\n",
      "train loss:0.053810352760136876\n",
      "train loss:0.0016701474530251923\n",
      "train loss:0.00796600652565652\n",
      "train loss:0.031405668432377846\n",
      "train loss:0.008328068643782247\n",
      "train loss:0.01028564398001413\n",
      "train loss:0.003349392425575007\n",
      "train loss:0.005051253784727044\n",
      "train loss:0.014175648964226984\n",
      "train loss:0.012390427321910846\n",
      "train loss:0.007497742942149488\n",
      "train loss:0.08364837630592568\n",
      "train loss:0.0034882454024014542\n",
      "train loss:0.015700278347849907\n",
      "train loss:0.017131731225528225\n",
      "train loss:0.005252075190737091\n",
      "train loss:0.0025258283103017564\n",
      "train loss:0.034428433096448566\n",
      "train loss:0.012291821080011655\n",
      "train loss:0.00585671057368115\n",
      "train loss:0.006596508278426506\n",
      "train loss:0.028918487881371028\n",
      "train loss:0.005349567379319769\n",
      "train loss:0.03684983998852916\n",
      "train loss:0.015255339317645954\n",
      "train loss:0.017338497766706006\n",
      "train loss:0.02841172047010927\n",
      "train loss:0.02346772875867778\n",
      "train loss:0.018649284527397315\n",
      "train loss:0.024998404350314165\n",
      "train loss:0.025989863483767826\n",
      "train loss:0.011830301066579904\n",
      "train loss:0.005094272157640329\n",
      "=== epoch:7, train acc:0.988, test acc:0.99 ===\n",
      "train loss:0.01279605771637543\n",
      "train loss:0.026067037253958626\n",
      "train loss:0.014813458410815366\n",
      "train loss:0.004323382339696674\n",
      "train loss:0.009928349854908708\n",
      "train loss:0.025810521004497485\n",
      "train loss:0.011432966777081544\n",
      "train loss:0.01273268219002625\n",
      "train loss:0.01481474083903598\n",
      "train loss:0.04599995982064997\n",
      "train loss:0.013978008132512239\n",
      "train loss:0.004395976200211167\n",
      "train loss:0.02295730303222083\n",
      "train loss:0.024016803929015885\n",
      "train loss:0.01878465303203949\n",
      "train loss:0.005149859645286261\n",
      "train loss:0.08958236924730219\n",
      "train loss:0.018931833344638237\n",
      "train loss:0.009647291149191108\n",
      "train loss:0.007343982428515011\n",
      "train loss:0.007875860657172993\n",
      "train loss:0.01005324243752173\n",
      "train loss:0.015983393693067872\n",
      "train loss:0.017626088635573117\n",
      "train loss:0.013513538356011816\n",
      "train loss:0.016279709896208815\n",
      "train loss:0.037698077115954645\n",
      "train loss:0.01827268950428413\n",
      "train loss:0.008343322547023907\n",
      "train loss:0.019308503645357016\n",
      "train loss:0.017230507333261903\n",
      "train loss:0.007507576697145524\n",
      "train loss:0.01273207304912968\n",
      "train loss:0.019010725397463332\n",
      "train loss:0.009139264720479065\n",
      "train loss:0.07880435035849158\n",
      "train loss:0.021194197871565575\n",
      "train loss:0.031259767164811596\n",
      "train loss:0.005404967153518934\n",
      "train loss:0.011653742463307614\n",
      "train loss:0.037405036163982504\n",
      "train loss:0.012893228449601333\n",
      "train loss:0.02371886683507291\n",
      "train loss:0.03702260067618897\n",
      "train loss:0.054537365776230896\n",
      "train loss:0.041512173241358225\n",
      "train loss:0.011158776013010534\n",
      "train loss:0.017522409362157397\n",
      "train loss:0.010567097229042652\n",
      "train loss:0.0034926774979764934\n",
      "train loss:0.00895786777998234\n",
      "train loss:0.01048294324718228\n",
      "train loss:0.01673825207238739\n",
      "train loss:0.008977511639851438\n",
      "train loss:0.01856519956536382\n",
      "train loss:0.011403725362203743\n",
      "train loss:0.009496243411379016\n",
      "train loss:0.010981145799523535\n",
      "train loss:0.005392136474081006\n",
      "train loss:0.05040332843414848\n",
      "train loss:0.018023017645995967\n",
      "train loss:0.0394849869363837\n",
      "train loss:0.017785846304626235\n",
      "train loss:0.0017517185170270167\n",
      "train loss:0.006731406802105323\n",
      "train loss:0.03001571306672006\n",
      "train loss:0.03321681168998992\n",
      "train loss:0.010846004344345173\n",
      "train loss:0.01044270326328666\n",
      "train loss:0.023235384463155716\n",
      "train loss:0.017239141929182952\n",
      "train loss:0.006809252520099389\n",
      "train loss:0.017419206756239414\n",
      "train loss:0.024234639211392902\n",
      "train loss:0.01654602139573987\n",
      "train loss:0.004380783648805061\n",
      "train loss:0.0049972722382045955\n",
      "train loss:0.06862611728369343\n",
      "train loss:0.003743461430292825\n",
      "train loss:0.022939572704894377\n",
      "train loss:0.011379721593799876\n",
      "train loss:0.010740955613222074\n",
      "train loss:0.010727896591972456\n",
      "train loss:0.018500632056025323\n",
      "train loss:0.022748174968118113\n",
      "train loss:0.00204268753891464\n",
      "train loss:0.006031608915858961\n",
      "train loss:0.0025966613318479047\n",
      "train loss:0.008806229439009113\n",
      "train loss:0.02189784266979317\n",
      "train loss:0.03684932849138217\n",
      "train loss:0.05102484286642141\n",
      "train loss:0.004403080292356037\n",
      "train loss:0.005131772160209436\n",
      "train loss:0.012667997503512116\n",
      "train loss:0.058476487166706566\n",
      "train loss:0.013449533056568246\n",
      "train loss:0.0033697164868021125\n",
      "train loss:0.02225249112175827\n",
      "train loss:0.024703823990300182\n",
      "train loss:0.003635685341364389\n",
      "train loss:0.014646250367466076\n",
      "train loss:0.020618918562831795\n",
      "train loss:0.009750708024776211\n",
      "train loss:0.020660532883494988\n",
      "train loss:0.002813200911641998\n",
      "train loss:0.017544584457304727\n",
      "train loss:0.007961159583766766\n",
      "train loss:0.01225230618551734\n",
      "train loss:0.005798096198315489\n",
      "train loss:0.016475937561015553\n",
      "train loss:0.006888492613338036\n",
      "train loss:0.010695768950275077\n",
      "train loss:0.03591816372766076\n",
      "train loss:0.021146499162465977\n",
      "train loss:0.0041814128652640425\n",
      "train loss:0.08473711645657689\n",
      "train loss:0.015004067198179658\n",
      "train loss:0.03495212933327763\n",
      "train loss:0.0032340619900406674\n",
      "train loss:0.005043463851021719\n",
      "train loss:0.0035514819805451915\n",
      "train loss:0.05584836430820341\n",
      "train loss:0.04855885766665817\n",
      "train loss:0.010695741912419406\n",
      "train loss:0.0026497373698523906\n",
      "train loss:0.016835567550941507\n",
      "train loss:0.004713826448577773\n",
      "train loss:0.004289148409740075\n",
      "train loss:0.0114551266222602\n",
      "train loss:0.013827924012300066\n",
      "train loss:0.013029112768007351\n",
      "train loss:0.024523110881768576\n",
      "train loss:0.011877722385165743\n",
      "train loss:0.04195223040260866\n",
      "train loss:0.0039843039269906065\n",
      "train loss:0.008794845808210132\n",
      "train loss:0.01480344208687321\n",
      "train loss:0.00536024919155434\n",
      "train loss:0.008642409789107956\n",
      "train loss:0.0060682262181672685\n",
      "train loss:0.023225058507557687\n",
      "train loss:0.0195232240641737\n",
      "train loss:0.003929579223708849\n",
      "train loss:0.00906904215169228\n",
      "train loss:0.015386582577040436\n",
      "train loss:0.007069825335014065\n",
      "train loss:0.013750332914644116\n",
      "train loss:0.0069668885594237825\n",
      "train loss:0.016380499064493178\n",
      "train loss:0.03333811177041069\n",
      "train loss:0.004313470231979843\n",
      "train loss:0.013539855180250613\n",
      "train loss:0.03493134328637855\n",
      "train loss:0.05345528080241328\n",
      "train loss:0.007153023951693978\n",
      "train loss:0.0028874569492055406\n",
      "train loss:0.042624218869560776\n",
      "train loss:0.011744897190807227\n",
      "train loss:0.0044718655675258095\n",
      "train loss:0.01748790020477303\n",
      "train loss:0.009002215612826166\n",
      "train loss:0.003938656527053522\n",
      "train loss:0.08846412995295466\n",
      "train loss:0.008801212207340413\n",
      "train loss:0.02508400999962905\n",
      "train loss:0.039945972220708635\n",
      "train loss:0.008688091029507414\n",
      "train loss:0.020960986076787952\n",
      "train loss:0.011033581433508294\n",
      "train loss:0.08555710548606868\n",
      "train loss:0.015651985189709075\n",
      "train loss:0.024364447802983726\n",
      "train loss:0.05654938632944395\n",
      "train loss:0.01575964210205437\n",
      "train loss:0.02752291100610936\n",
      "train loss:0.0044414329494476094\n",
      "train loss:0.09114970814021499\n",
      "train loss:0.01357115369203623\n",
      "train loss:0.01735342427632922\n",
      "train loss:0.0036442035802060385\n",
      "train loss:0.017158824031610384\n",
      "train loss:0.0018957186394396156\n",
      "train loss:0.038245376196316544\n",
      "train loss:0.03465776430225672\n",
      "train loss:0.018926812831992976\n",
      "train loss:0.08725705854701032\n",
      "train loss:0.050261675495393975\n",
      "train loss:0.006956444235245106\n",
      "train loss:0.024488159805730807\n",
      "train loss:0.03678068965851723\n",
      "train loss:0.033048966802901744\n",
      "train loss:0.004997258839673558\n",
      "train loss:0.024485760753210305\n",
      "train loss:0.03909446697842147\n",
      "train loss:0.01060190464005587\n",
      "train loss:0.015449849696511557\n",
      "train loss:0.019944539837081603\n",
      "train loss:0.008241115680704183\n",
      "train loss:0.0074348174658248585\n",
      "train loss:0.0024977076492755804\n",
      "train loss:0.008029121619310688\n",
      "train loss:0.010139094043572754\n",
      "train loss:0.014720583220594597\n",
      "train loss:0.007332715351054688\n",
      "train loss:0.0047005826786012785\n",
      "train loss:0.12186244991598469\n",
      "train loss:0.009041887789906257\n",
      "train loss:0.006094539984352661\n",
      "train loss:0.02050595140820057\n",
      "train loss:0.009240364864271289\n",
      "train loss:0.007478790865671884\n",
      "train loss:0.005740986571945658\n",
      "train loss:0.034442090925317796\n",
      "train loss:0.020547157902618737\n",
      "train loss:0.013050854062806894\n",
      "train loss:0.011383927372402987\n",
      "train loss:0.004906369916879929\n",
      "train loss:0.017385581003923206\n",
      "train loss:0.004388576247474514\n",
      "train loss:0.06393409708624666\n",
      "train loss:0.0028040665577180857\n",
      "train loss:0.0085383619387287\n",
      "train loss:0.009202880195574336\n",
      "train loss:0.01013885089835843\n",
      "train loss:0.027179983937835238\n",
      "train loss:0.00737307277713934\n",
      "train loss:0.014560582882840822\n",
      "train loss:0.008292669387165427\n",
      "train loss:0.010916609846759818\n",
      "train loss:0.004691411075088324\n",
      "train loss:0.023026658683690977\n",
      "train loss:0.006280140605011357\n",
      "train loss:0.08228142879021372\n",
      "train loss:0.02103060646561562\n",
      "train loss:0.036871427971149215\n",
      "train loss:0.00282935945370823\n",
      "train loss:0.02133118819423942\n",
      "train loss:0.027864282013878906\n",
      "train loss:0.02976484861770881\n",
      "train loss:0.009867544534703733\n",
      "train loss:0.008263689992623592\n",
      "train loss:0.01629229771224393\n",
      "train loss:0.01899210632426234\n",
      "train loss:0.013319391850016285\n",
      "train loss:0.022681962472400116\n",
      "train loss:0.007044320635504617\n",
      "train loss:0.05388205714907891\n",
      "train loss:0.02885395937334887\n",
      "train loss:0.006257732880309279\n",
      "train loss:0.01241769789029624\n",
      "train loss:0.0060211247837688355\n",
      "train loss:0.01200208581784948\n",
      "train loss:0.010336364919186571\n",
      "train loss:0.020935658315064477\n",
      "train loss:0.03561807228420362\n",
      "train loss:0.007523148788953474\n",
      "train loss:0.0034582971820983164\n",
      "train loss:0.010211210051894497\n",
      "train loss:0.014662035053268152\n",
      "train loss:0.013748657281597328\n",
      "train loss:0.007743922295952658\n",
      "train loss:0.021165788308814402\n",
      "train loss:0.004095939267035792\n",
      "train loss:0.08066555515173944\n",
      "train loss:0.0034741154777323483\n",
      "train loss:0.003319812379276704\n",
      "train loss:0.03572980608298649\n",
      "train loss:0.0069614127193859884\n",
      "train loss:0.01471681579357645\n",
      "train loss:0.02721626436053526\n",
      "train loss:0.02915392255116712\n",
      "train loss:0.025081972779542924\n",
      "train loss:0.0050161319753748565\n",
      "train loss:0.021114764463416508\n",
      "train loss:0.05081876771144528\n",
      "train loss:0.011912186558657589\n",
      "train loss:0.01053500485347563\n",
      "train loss:0.009750338447441935\n",
      "train loss:0.01661373009623969\n",
      "train loss:0.013193469799055937\n",
      "train loss:0.005679701110760192\n",
      "train loss:0.01905039412252514\n",
      "train loss:0.00489098750596632\n",
      "train loss:0.00610039969119158\n",
      "train loss:0.008070487340050258\n",
      "train loss:0.061638871375361864\n",
      "train loss:0.04770191052481076\n",
      "train loss:0.036444431583531244\n",
      "train loss:0.00755368093955237\n",
      "train loss:0.0026358733905600972\n",
      "train loss:0.009587811109893303\n",
      "train loss:0.011076919099401426\n",
      "train loss:0.0027383284114087763\n",
      "train loss:0.014478298794468436\n",
      "train loss:0.04439729111225825\n",
      "train loss:0.0075906537896380695\n",
      "train loss:0.010044853226069444\n",
      "train loss:0.0011718848086530158\n",
      "train loss:0.00948406480184143\n",
      "train loss:0.03298849282481452\n",
      "train loss:0.014638752444236564\n",
      "train loss:0.00395873306416601\n",
      "train loss:0.0018340826914453802\n",
      "train loss:0.017065188748851205\n",
      "train loss:0.016285411760406588\n",
      "train loss:0.007216498102176402\n",
      "train loss:0.015343912245499504\n",
      "train loss:0.004550346730482681\n",
      "train loss:0.016561106937968192\n",
      "train loss:0.048178477145527115\n",
      "train loss:0.007938792538577481\n",
      "train loss:0.021728449287946906\n",
      "train loss:0.0021942731760966243\n",
      "train loss:0.001714597442110742\n",
      "train loss:0.01775347387593488\n",
      "train loss:0.007209913478069291\n",
      "train loss:0.004023722009644254\n",
      "train loss:0.06021978928406806\n",
      "train loss:0.0013422870231598423\n",
      "train loss:0.023200526119252066\n",
      "train loss:0.0060391425984814085\n",
      "train loss:0.02553800342580967\n",
      "train loss:0.0196726124099898\n",
      "train loss:0.01589049206356612\n",
      "train loss:0.042137453064815006\n",
      "train loss:0.0026095352684345143\n",
      "train loss:0.0023803968204131037\n",
      "train loss:0.018310644344600197\n",
      "train loss:0.0077424530536309\n",
      "train loss:0.005369614647592652\n",
      "train loss:0.011918298971083707\n",
      "train loss:0.010604222027904411\n",
      "train loss:0.031813375039052225\n",
      "train loss:0.08236508056982515\n",
      "train loss:0.014844034362717678\n",
      "train loss:0.02752671196921798\n",
      "train loss:0.004456343526476833\n",
      "train loss:0.012491327417182625\n",
      "train loss:0.047566265964061785\n",
      "train loss:0.03213649299968962\n",
      "train loss:0.02808714684010428\n",
      "train loss:0.003571198425008826\n",
      "train loss:0.0074157701206030826\n",
      "train loss:0.00495508068831565\n",
      "train loss:0.01552540036119666\n",
      "train loss:0.008202307982186977\n",
      "train loss:0.0220963412274545\n",
      "train loss:0.006008906405372469\n",
      "train loss:0.007551173053806985\n",
      "train loss:0.004842164144233329\n",
      "train loss:0.0019485916239817664\n",
      "train loss:0.007584969926460019\n",
      "train loss:0.007500837173503761\n",
      "train loss:0.007373609886373887\n",
      "train loss:0.013519193378188694\n",
      "train loss:0.006304066696611825\n",
      "train loss:0.002053791024462301\n",
      "train loss:0.004306862673067498\n",
      "train loss:0.00806436869752845\n",
      "train loss:0.009079275703781678\n",
      "train loss:0.03541414608294403\n",
      "train loss:0.019579920576951335\n",
      "train loss:0.025244101005768158\n",
      "train loss:0.007503202469883407\n",
      "train loss:0.0038055781896825258\n",
      "train loss:0.007680729141664063\n",
      "train loss:0.011051160213788783\n",
      "train loss:0.0325863831969653\n",
      "train loss:0.002829256424571005\n",
      "train loss:0.019388673412479223\n",
      "train loss:0.016481250175120796\n",
      "train loss:0.07917243667636477\n",
      "train loss:0.01081513453713189\n",
      "train loss:0.01814094178300679\n",
      "train loss:0.06213563628827695\n",
      "train loss:0.00819535425155951\n",
      "train loss:0.01041751009258224\n",
      "train loss:0.0331187192499772\n",
      "train loss:0.014658123601051017\n",
      "train loss:0.010854410072966801\n",
      "train loss:0.0047016174633570445\n",
      "train loss:0.047715205728058326\n",
      "train loss:0.004477398945320576\n",
      "train loss:0.009668780537794764\n",
      "train loss:0.052318707502764565\n",
      "train loss:0.012139626012477948\n",
      "train loss:0.011168791809455687\n",
      "train loss:0.005580588181362947\n",
      "train loss:0.004745352264111272\n",
      "train loss:0.05825110537030511\n",
      "train loss:0.02341713276573262\n",
      "train loss:0.010035177595927014\n",
      "train loss:0.004005116630281575\n",
      "train loss:0.015250415465013522\n",
      "train loss:0.03630882509566424\n",
      "train loss:0.013839719867640418\n",
      "train loss:0.0017868470428908864\n",
      "train loss:0.009377686130661417\n",
      "train loss:0.0023688364531328585\n",
      "train loss:0.09929720965292577\n",
      "train loss:0.03592821647894163\n",
      "train loss:0.018392190250146337\n",
      "train loss:0.04258168377603933\n",
      "train loss:0.02634084749693766\n",
      "train loss:0.0025585441804505367\n",
      "train loss:0.03260126923223708\n",
      "train loss:0.02372249675943881\n",
      "train loss:0.053929483896961145\n",
      "train loss:0.02513962150841411\n",
      "train loss:0.038415788766718674\n",
      "train loss:0.018449323810779513\n",
      "train loss:0.03161032119621933\n",
      "train loss:0.013977468629294556\n",
      "train loss:0.028886454288659113\n",
      "train loss:0.02572080303940841\n",
      "train loss:0.04124563273171281\n",
      "train loss:0.006090165621820561\n",
      "train loss:0.0027950975398863065\n",
      "train loss:0.025739183339121507\n",
      "train loss:0.005179202706295003\n",
      "train loss:0.05574490958999714\n",
      "train loss:0.03845343619377369\n",
      "train loss:0.03140073910931402\n",
      "train loss:0.03496251768114313\n",
      "train loss:0.04086068028757718\n",
      "train loss:0.004317582926945742\n",
      "train loss:0.01677017873720102\n",
      "train loss:0.004390751073080085\n",
      "train loss:0.006725594265955609\n",
      "train loss:0.029835163072704455\n",
      "train loss:0.025604836055805987\n",
      "train loss:0.03411986979036709\n",
      "train loss:0.037880084303680735\n",
      "train loss:0.009210574230340465\n",
      "train loss:0.03605972164800803\n",
      "train loss:0.033155183384793036\n",
      "train loss:0.02978969156875783\n",
      "train loss:0.007845899738326627\n",
      "train loss:0.010398643687473176\n",
      "train loss:0.007350734206813404\n",
      "train loss:0.013116392548138867\n",
      "train loss:0.0063730302880156\n",
      "train loss:0.005928775712031867\n",
      "train loss:0.026855545106551505\n",
      "train loss:0.011174439801942082\n",
      "train loss:0.006912632842116283\n",
      "train loss:0.003928540170417031\n",
      "train loss:0.0055919175915970995\n",
      "train loss:0.021006705515493076\n",
      "train loss:0.11209249028004395\n",
      "train loss:0.005581595338234067\n",
      "train loss:0.01117861197311958\n",
      "train loss:0.04079017681337846\n",
      "train loss:0.011297397806597503\n",
      "train loss:0.022893459877624436\n",
      "train loss:0.004264233259914167\n",
      "train loss:0.0019613894741249754\n",
      "train loss:0.005900398508906143\n",
      "train loss:0.018505280880934803\n",
      "train loss:0.011164664180710047\n",
      "train loss:0.015500951319136117\n",
      "train loss:0.013003529837272798\n",
      "train loss:0.015819678024191583\n",
      "train loss:0.005135351623720876\n",
      "train loss:0.007930002699545167\n",
      "train loss:0.008991808536451516\n",
      "train loss:0.014790340703988651\n",
      "train loss:0.005001287934005264\n",
      "train loss:0.021597571202869044\n",
      "train loss:0.031283878744667315\n",
      "train loss:0.01628242758805568\n",
      "train loss:0.013546453303201554\n",
      "train loss:0.009783228296323058\n",
      "train loss:0.07258977497730372\n",
      "train loss:0.013897457630778674\n",
      "train loss:0.014671696606427288\n",
      "train loss:0.0022850987023499466\n",
      "train loss:0.008157236027061063\n",
      "train loss:0.005720332439574408\n",
      "train loss:0.006831989781000219\n",
      "train loss:0.0038885203065882395\n",
      "train loss:0.024138871199292276\n",
      "train loss:0.00506270889914406\n",
      "train loss:0.02019747753108105\n",
      "train loss:0.0068333508667383835\n",
      "train loss:0.027329366116344445\n",
      "train loss:0.010717300928404455\n",
      "train loss:0.004296543424828956\n",
      "train loss:0.03978696085766789\n",
      "train loss:0.013220636940678439\n",
      "train loss:0.0039975246234037\n",
      "train loss:0.029433793403357805\n",
      "train loss:0.003679022491930907\n",
      "train loss:0.03607015204816911\n",
      "train loss:0.007891238611823679\n",
      "train loss:0.0029076172243258226\n",
      "train loss:0.010302260809360298\n",
      "train loss:0.011964272508014719\n",
      "train loss:0.00805158757161456\n",
      "train loss:0.003291392029197484\n",
      "train loss:0.0031826585362432\n",
      "train loss:0.026269734573672886\n",
      "train loss:0.010192350032712063\n",
      "train loss:0.06952389586162744\n",
      "train loss:0.010260261826784155\n",
      "train loss:0.042843048672267736\n",
      "train loss:0.0281745277297154\n",
      "train loss:0.010926389122612396\n",
      "train loss:0.013586886944792358\n",
      "train loss:0.07698139395023999\n",
      "train loss:0.0030841931042464767\n",
      "train loss:0.006917627011818398\n",
      "train loss:0.0017576473440708879\n",
      "train loss:0.00978375312281901\n",
      "train loss:0.010872001554037315\n",
      "train loss:0.005361537238739816\n",
      "train loss:0.008168766563032845\n",
      "train loss:0.02380969784400717\n",
      "train loss:0.015318532884212947\n",
      "train loss:0.014843271451927351\n",
      "train loss:0.0015153672866616455\n",
      "train loss:0.014259604441177574\n",
      "train loss:0.004168546478000657\n",
      "train loss:0.009298856871702703\n",
      "train loss:0.007493676170823002\n",
      "train loss:0.008178399156263132\n",
      "train loss:0.005941394863171203\n",
      "train loss:0.01445602589763561\n",
      "train loss:0.01122367265955186\n",
      "train loss:0.018230398420416443\n",
      "train loss:0.007112423870198802\n",
      "train loss:0.08117984528622225\n",
      "train loss:0.0276059635821307\n",
      "train loss:0.002279928106614776\n",
      "train loss:0.005942192612631605\n",
      "train loss:0.004572022234876702\n",
      "train loss:0.013573430128324298\n",
      "train loss:0.0437441337940744\n",
      "train loss:0.010846543988131913\n",
      "train loss:0.00972561256147541\n",
      "train loss:0.018616804529041672\n",
      "train loss:0.006220467362585088\n",
      "train loss:0.011025754940290117\n",
      "train loss:0.002821832547729067\n",
      "train loss:0.01303767659386609\n",
      "train loss:0.01881058677266217\n",
      "train loss:0.014112096690300726\n",
      "train loss:0.011319186627481664\n",
      "train loss:0.013437606108488091\n",
      "train loss:0.061942862050219274\n",
      "train loss:0.008264137904681576\n",
      "train loss:0.007527478077943627\n",
      "train loss:0.004424238464489211\n",
      "train loss:0.018892001357967925\n",
      "train loss:0.004068512361999344\n",
      "train loss:0.009361144883348278\n",
      "train loss:0.04372580240146137\n",
      "train loss:0.010107263334506164\n",
      "train loss:0.0018101806555659585\n",
      "train loss:0.005467050576903438\n",
      "train loss:0.009966402626889363\n",
      "train loss:0.012402377942944914\n",
      "train loss:0.0017505567609270853\n",
      "train loss:0.03158422964590096\n",
      "train loss:0.03822678062346191\n",
      "train loss:0.004366322331676905\n",
      "train loss:0.0055948434873062105\n",
      "train loss:0.010588682851086212\n",
      "train loss:0.007057572885613764\n",
      "train loss:0.019763427540087225\n",
      "train loss:0.0033263538739213604\n",
      "train loss:0.003753822281820486\n",
      "train loss:0.001246295565314575\n",
      "train loss:0.004858347968275611\n",
      "train loss:0.012442533853872433\n",
      "train loss:0.013498518931824632\n",
      "train loss:0.006586812106244883\n",
      "train loss:0.008493399747840196\n",
      "train loss:0.006078619396427749\n",
      "train loss:0.008498176478866951\n",
      "train loss:0.021275214765573744\n",
      "train loss:0.02193578855071051\n",
      "train loss:0.0075735568972596715\n",
      "train loss:0.0032605773701420856\n",
      "train loss:0.02270195982182637\n",
      "train loss:0.05168170271291342\n",
      "train loss:0.004524255268544799\n",
      "train loss:0.0032639118184505144\n",
      "train loss:0.008571091756849112\n",
      "train loss:0.018354581669224556\n",
      "train loss:0.003574793335777319\n",
      "train loss:0.00514671262196293\n",
      "train loss:0.007366501403146288\n",
      "train loss:0.03867623522986617\n",
      "train loss:0.014500833013623649\n",
      "train loss:0.03674594645926368\n",
      "train loss:0.008061588670571572\n",
      "train loss:0.07000891541246962\n",
      "train loss:0.006218486327562072\n",
      "=== epoch:8, train acc:0.993, test acc:0.987 ===\n",
      "train loss:0.014415633526882994\n",
      "train loss:0.004684877837580948\n",
      "train loss:0.028764506144188004\n",
      "train loss:0.01108373279336141\n",
      "train loss:0.018793218604449866\n",
      "train loss:0.014738530620038054\n",
      "train loss:0.02566057109364858\n",
      "train loss:0.018203132683046547\n",
      "train loss:0.03164950966188616\n",
      "train loss:0.031827408286092554\n",
      "train loss:0.008513115948392751\n",
      "train loss:0.014991989547269536\n",
      "train loss:0.05211749869354624\n",
      "train loss:0.009924532447571218\n",
      "train loss:0.006060342672492414\n",
      "train loss:0.022852730437533265\n",
      "train loss:0.0055858251362000625\n",
      "train loss:0.016162986378775886\n",
      "train loss:0.011573742535313397\n",
      "train loss:0.07045028087220394\n",
      "train loss:0.006644103798961839\n",
      "train loss:0.006776645453014558\n",
      "train loss:0.008586662178835351\n",
      "train loss:0.013124983874867933\n",
      "train loss:0.005861283666365423\n",
      "train loss:0.010819179597596715\n",
      "train loss:0.011319618514556328\n",
      "train loss:0.0019281922653985998\n",
      "train loss:0.005423462199185693\n",
      "train loss:0.06862736041596186\n",
      "train loss:0.015418611504714836\n",
      "train loss:0.009208669469651647\n",
      "train loss:0.015684940220510854\n",
      "train loss:0.01597958757995397\n",
      "train loss:0.021175893594550556\n",
      "train loss:0.005943206016696481\n",
      "train loss:0.05372643407847148\n",
      "train loss:0.05258188981576953\n",
      "train loss:0.010569275502358635\n",
      "train loss:0.012005649683705892\n",
      "train loss:0.04518415910746088\n",
      "train loss:0.031077703354593783\n",
      "train loss:0.022013463695893652\n",
      "train loss:0.02728380542357578\n",
      "train loss:0.008687343483302253\n",
      "train loss:0.013327208256132952\n",
      "train loss:0.01232916830607671\n",
      "train loss:0.01584950452968357\n",
      "train loss:0.009853815435987437\n",
      "train loss:0.0037075817850802575\n",
      "train loss:0.01097387882991129\n",
      "train loss:0.016837971217721237\n",
      "train loss:0.007266299750739242\n",
      "train loss:0.0052838403593343\n",
      "train loss:0.004590102356156662\n",
      "train loss:0.012570035637571362\n",
      "train loss:0.013327453407795654\n",
      "train loss:0.018449489620831955\n",
      "train loss:0.005299957319114793\n",
      "train loss:0.005653069060748336\n",
      "train loss:0.009722388554657135\n",
      "train loss:0.017351678575303536\n",
      "train loss:0.021196586607236775\n",
      "train loss:0.005387012997578222\n",
      "train loss:0.0035167418100490234\n",
      "train loss:0.005215967178838281\n",
      "train loss:0.002474023278403718\n",
      "train loss:0.005752598792455036\n",
      "train loss:0.019873380036787677\n",
      "train loss:0.004873978724526336\n",
      "train loss:0.012928868457038008\n",
      "train loss:0.02487155916963549\n",
      "train loss:0.028668670048704673\n",
      "train loss:0.019583457411447146\n",
      "train loss:0.013104301519041534\n",
      "train loss:0.02023596185000725\n",
      "train loss:0.013910621957765896\n",
      "train loss:0.007769652262257387\n",
      "train loss:0.011445844997875059\n",
      "train loss:0.015836159521609657\n",
      "train loss:0.004257140550234814\n",
      "train loss:0.07717537308318936\n",
      "train loss:0.03125866382457737\n",
      "train loss:0.007195745766216622\n",
      "train loss:0.017333410978692388\n",
      "train loss:0.0046289571630289674\n",
      "train loss:0.05646831437796754\n",
      "train loss:0.012303186055218338\n",
      "train loss:0.02249575732511514\n",
      "train loss:0.042399186789915486\n",
      "train loss:0.01255103024445529\n",
      "train loss:0.017653983730619063\n",
      "train loss:0.07524963848744042\n",
      "train loss:0.0034037147246045707\n",
      "train loss:0.023609636804714177\n",
      "train loss:0.0063103713152174576\n",
      "train loss:0.007849004743504856\n",
      "train loss:0.004778706285589142\n",
      "train loss:0.06300201384796422\n",
      "train loss:0.017487713131514322\n",
      "train loss:0.015189270216056278\n",
      "train loss:0.001985490253278979\n",
      "train loss:0.0571601537962484\n",
      "train loss:0.021440506733903728\n",
      "train loss:0.00430259250359995\n",
      "train loss:0.04084612513698462\n",
      "train loss:0.005308131229637111\n",
      "train loss:0.012895061999957248\n",
      "train loss:0.050709047812897694\n",
      "train loss:0.019196343397789074\n",
      "train loss:0.0067227738440002694\n",
      "train loss:0.005720411176096618\n",
      "train loss:0.012459661787380748\n",
      "train loss:0.014371588530831602\n",
      "train loss:0.004245810637327065\n",
      "train loss:0.015903320772219245\n",
      "train loss:0.04501418908599836\n",
      "train loss:0.01997697599813367\n",
      "train loss:0.009469267590350869\n",
      "train loss:0.005430626127412405\n",
      "train loss:0.005435625612663599\n",
      "train loss:0.002637211744957102\n",
      "train loss:0.016778457407870026\n",
      "train loss:0.003024907952335039\n",
      "train loss:0.0031777032154218627\n",
      "train loss:0.004469252485008068\n",
      "train loss:0.02018026321631783\n",
      "train loss:0.010377499643532205\n",
      "train loss:0.03022653532272882\n",
      "train loss:0.007350901843621163\n",
      "train loss:0.005898981474733584\n",
      "train loss:0.010819277831065607\n",
      "train loss:0.0643568254064566\n",
      "train loss:0.011498355777595252\n",
      "train loss:0.006207624674226603\n",
      "train loss:0.02043496140106872\n",
      "train loss:0.010557394014316417\n",
      "train loss:0.004447118896447636\n",
      "train loss:0.01016385444523759\n",
      "train loss:0.007488324946973688\n",
      "train loss:0.009098010454495747\n",
      "train loss:0.007178182352213993\n",
      "train loss:0.004198581094432555\n",
      "train loss:0.08056127527057293\n",
      "train loss:0.04115372333227582\n",
      "train loss:0.002713458391903287\n",
      "train loss:0.007994734406985086\n",
      "train loss:0.009273966851320325\n",
      "train loss:0.01955484335713671\n",
      "train loss:0.006294881198169034\n",
      "train loss:0.00796390136357336\n",
      "train loss:0.005018601135746992\n",
      "train loss:0.045387507995592685\n",
      "train loss:0.004388463241551854\n",
      "train loss:0.008804995958206276\n",
      "train loss:0.0027321993618162742\n",
      "train loss:0.01520711461141677\n",
      "train loss:0.006265070614584951\n",
      "train loss:0.01589501379357707\n",
      "train loss:0.004002249817787946\n",
      "train loss:0.017222392317177884\n",
      "train loss:0.005746497810162624\n",
      "train loss:0.020683524210745784\n",
      "train loss:0.028574453758688417\n",
      "train loss:0.006970220743857777\n",
      "train loss:0.003963439689610648\n",
      "train loss:0.009808299970399803\n",
      "train loss:0.004798832153066656\n",
      "train loss:0.011763273783089107\n",
      "train loss:0.0017344764255403816\n",
      "train loss:0.021465645380966566\n",
      "train loss:0.006864004395704438\n",
      "train loss:0.08780403921456684\n",
      "train loss:0.10279026047717973\n",
      "train loss:0.002753620867359008\n",
      "train loss:0.019489331135805654\n",
      "train loss:0.004639979531667493\n",
      "train loss:0.008932920614429265\n",
      "train loss:0.009852365266900344\n",
      "train loss:0.0077820356653253465\n",
      "train loss:0.008490266906506864\n",
      "train loss:0.010339805423442166\n",
      "train loss:0.0041116846764707084\n",
      "train loss:0.04706018735066825\n",
      "train loss:0.010653900172079704\n",
      "train loss:0.006384232920796481\n",
      "train loss:0.003065864501699891\n",
      "train loss:0.0033002423146447873\n",
      "train loss:0.00783468178757721\n",
      "train loss:0.06536347577969952\n",
      "train loss:0.011832878050205973\n",
      "train loss:0.017723465504526137\n",
      "train loss:0.021434545295582707\n",
      "train loss:0.003167183062834583\n",
      "train loss:0.02229144606162622\n",
      "train loss:0.0038652057223667515\n",
      "train loss:0.013029371820943055\n",
      "train loss:0.017475092957947462\n",
      "train loss:0.013625916148400141\n",
      "train loss:0.017520551840634776\n",
      "train loss:0.011026580944774204\n",
      "train loss:0.011153221130576487\n",
      "train loss:0.014826882592574038\n",
      "train loss:0.00496444601054809\n",
      "train loss:0.013793633289645437\n",
      "train loss:0.006697825927106158\n",
      "train loss:0.007424453999775814\n",
      "train loss:0.005488382612191602\n",
      "train loss:0.0024540240975285464\n",
      "train loss:0.006741786138659909\n",
      "train loss:0.020644822353939548\n",
      "train loss:0.004452698326673609\n",
      "train loss:0.002469504196761353\n",
      "train loss:0.0031274554095071334\n",
      "train loss:0.00681157322993769\n",
      "train loss:0.009342921723843873\n",
      "train loss:0.005493424507075204\n",
      "train loss:0.004433544454061224\n",
      "train loss:0.012467909380619879\n",
      "train loss:0.0009206582366356867\n",
      "train loss:0.017696080933314772\n",
      "train loss:0.0108991952498463\n",
      "train loss:0.017225404122452277\n",
      "train loss:0.007437851760801991\n",
      "train loss:0.007790057084042529\n",
      "train loss:0.0026608405703718286\n",
      "train loss:0.00043023731530207095\n",
      "train loss:0.002434049409813873\n",
      "train loss:0.055459662934725366\n",
      "train loss:0.00915382617599197\n",
      "train loss:0.022315613177777353\n",
      "train loss:0.02877941302395835\n",
      "train loss:0.01201302941023097\n",
      "train loss:0.0038807400409373227\n",
      "train loss:0.024356589546570148\n",
      "train loss:0.020280422730329026\n",
      "train loss:0.002489615399777475\n",
      "train loss:0.028400481524491462\n",
      "train loss:0.011622237278355175\n",
      "train loss:0.006038288382414562\n",
      "train loss:0.008453700824706022\n",
      "train loss:0.007314901338189809\n",
      "train loss:0.019419733280388284\n",
      "train loss:0.018163740756828625\n",
      "train loss:0.010592916921049262\n",
      "train loss:0.012365094027824663\n",
      "train loss:0.012979650468158582\n",
      "train loss:0.02774261588450057\n",
      "train loss:0.005843642214594539\n",
      "train loss:0.05220615895287061\n",
      "train loss:0.04242769614471846\n",
      "train loss:0.0142116438493245\n",
      "train loss:0.0054531558418723855\n",
      "train loss:0.009129120057706915\n",
      "train loss:0.02943593139951336\n",
      "train loss:0.010481561035320809\n",
      "train loss:0.008443551106503055\n",
      "train loss:0.00850977864146861\n",
      "train loss:0.005932405803505148\n",
      "train loss:0.018503279831924783\n",
      "train loss:0.00813979375962804\n",
      "train loss:0.005299859631120821\n",
      "train loss:0.009972989602810458\n",
      "train loss:0.042753390871231665\n",
      "train loss:0.0049475445729368454\n",
      "train loss:0.03253248731083752\n",
      "train loss:0.004767111698453598\n",
      "train loss:0.003631195964573421\n",
      "train loss:0.013088996578920164\n",
      "train loss:0.007756475870558427\n",
      "train loss:0.01892862551939307\n",
      "train loss:0.005146583222391679\n",
      "train loss:0.04375428805198875\n",
      "train loss:0.0035765197006138146\n",
      "train loss:0.005514935278246891\n",
      "train loss:0.005511139814642584\n",
      "train loss:0.0007750978545728401\n",
      "train loss:0.006760034189276716\n",
      "train loss:0.01865041989558005\n",
      "train loss:0.008027114902011082\n",
      "train loss:0.029168651722538925\n",
      "train loss:0.01371724613326567\n",
      "train loss:0.009627662274970403\n",
      "train loss:0.015453611976778081\n",
      "train loss:0.05052774469907592\n",
      "train loss:0.04588692147432075\n",
      "train loss:0.01968388981979397\n",
      "train loss:0.009463332158978228\n",
      "train loss:0.02147753775408907\n",
      "train loss:0.006278779011800521\n",
      "train loss:0.013774622717756601\n",
      "train loss:0.00838397013485945\n",
      "train loss:0.0020541775442674175\n",
      "train loss:0.002453236676944266\n",
      "train loss:0.008975204635164068\n",
      "train loss:0.006905047193806327\n",
      "train loss:0.01513200811341562\n",
      "train loss:0.0015205843414691472\n",
      "train loss:0.015349258939896845\n",
      "train loss:0.0032523148324210527\n",
      "train loss:0.0020566421966286743\n",
      "train loss:0.00756311038033124\n",
      "train loss:0.03107263618580107\n",
      "train loss:0.003046841150343528\n",
      "train loss:0.012308495353829026\n",
      "train loss:0.0112270086974154\n",
      "train loss:0.005666236700868816\n",
      "train loss:0.004487486530535471\n",
      "train loss:0.02302388162776544\n",
      "train loss:0.045961027457416745\n",
      "train loss:0.0077961139374377675\n",
      "train loss:0.018579881761068177\n",
      "train loss:0.011163426366604182\n",
      "train loss:0.0019144691497953114\n",
      "train loss:0.0034052465176098684\n",
      "train loss:0.018720269795220467\n",
      "train loss:0.008627242220050528\n",
      "train loss:0.011136692221384126\n",
      "train loss:0.010338409841606248\n",
      "train loss:0.0133642504522996\n",
      "train loss:0.008512644690943896\n",
      "train loss:0.0025042872785127766\n",
      "train loss:0.0035689681015066465\n",
      "train loss:0.003415747369221601\n",
      "train loss:0.011035641889548323\n",
      "train loss:0.003886428460253775\n",
      "train loss:0.01825813949960372\n",
      "train loss:0.012162554447257999\n",
      "train loss:0.008637250675453652\n",
      "train loss:0.1782407819329458\n",
      "train loss:0.009263391289921513\n",
      "train loss:0.002876755063189525\n",
      "train loss:0.024615879620449893\n",
      "train loss:0.006307151209929496\n",
      "train loss:0.025157526612823444\n",
      "train loss:0.003735140473341117\n",
      "train loss:0.01547705038634839\n",
      "train loss:0.015435799305982899\n",
      "train loss:0.010034404751252658\n",
      "train loss:0.06694057961280969\n",
      "train loss:0.009992406958702476\n",
      "train loss:0.014129213928624545\n",
      "train loss:0.02286808216777833\n",
      "train loss:0.010402488046483986\n",
      "train loss:0.029901820295586568\n",
      "train loss:0.0031852433219466685\n",
      "train loss:0.0012511363542143214\n",
      "train loss:0.00434997533260092\n",
      "train loss:0.06776601606811243\n",
      "train loss:0.004854341261760597\n",
      "train loss:0.012082066889905858\n",
      "train loss:0.004568652440522379\n",
      "train loss:0.0072610286709919045\n",
      "train loss:0.01184844065356207\n",
      "train loss:0.050780697473590417\n",
      "train loss:0.0073283142686622615\n",
      "train loss:0.034479206953726985\n",
      "train loss:0.003012283707073002\n",
      "train loss:0.011067159593085789\n",
      "train loss:0.011788010549884726\n",
      "train loss:0.019076124876265876\n",
      "train loss:0.01568718281291245\n",
      "train loss:0.04901583650377541\n",
      "train loss:0.03696398613987794\n",
      "train loss:0.0540214944180551\n",
      "train loss:0.007363676049032895\n",
      "train loss:0.001754285244451701\n",
      "train loss:0.009109333061174668\n",
      "train loss:0.04789752493843565\n",
      "train loss:0.007297020154848127\n",
      "train loss:0.008630434855725467\n",
      "train loss:0.005621091314409286\n",
      "train loss:0.001622559627287694\n",
      "train loss:0.025956150992070774\n",
      "train loss:0.0031545698396700855\n",
      "train loss:0.016360400008072512\n",
      "train loss:0.006899588277488937\n",
      "train loss:0.06478468517630981\n",
      "train loss:0.011792785977294653\n",
      "train loss:0.014243670102633034\n",
      "train loss:0.019369246043424868\n",
      "train loss:0.005463959887279735\n",
      "train loss:0.0552679086143344\n",
      "train loss:0.003696914711283619\n",
      "train loss:0.026418402303476675\n",
      "train loss:0.007587233869336648\n",
      "train loss:0.0038101606240089287\n",
      "train loss:0.02466915026172857\n",
      "train loss:0.019887750936637753\n",
      "train loss:0.005177546270591652\n",
      "train loss:0.0038211212460456923\n",
      "train loss:0.012251193328812913\n",
      "train loss:0.02324013577462866\n",
      "train loss:0.023101803808622118\n",
      "train loss:0.0053543465711296135\n",
      "train loss:0.03290983519400199\n",
      "train loss:0.009029902680332597\n",
      "train loss:0.0043770203850392025\n",
      "train loss:0.0030718218378998\n",
      "train loss:0.0035029469649601364\n",
      "train loss:0.00423770538097829\n",
      "train loss:0.005336460360044196\n",
      "train loss:0.004488671879996016\n",
      "train loss:0.003943056765757438\n",
      "train loss:0.0573657116516538\n",
      "train loss:0.011937207773817203\n",
      "train loss:0.03436991180991234\n",
      "train loss:0.009546414429045264\n",
      "train loss:0.0006796173498271156\n",
      "train loss:0.004627761417896399\n",
      "train loss:0.028517317571883218\n",
      "train loss:0.018689020941111432\n",
      "train loss:0.004590711521727949\n",
      "train loss:0.004682005428515979\n",
      "train loss:0.020496938455515177\n",
      "train loss:0.004788284682954714\n",
      "train loss:0.006649937250924783\n",
      "train loss:0.06450173166284129\n",
      "train loss:0.006419627935073921\n",
      "train loss:0.00846841714000346\n",
      "train loss:0.003617317334299295\n",
      "train loss:0.014549165945193518\n",
      "train loss:0.0014007109018660685\n",
      "train loss:0.004900490911781466\n",
      "train loss:0.008654241445588975\n",
      "train loss:0.006500597546687192\n",
      "train loss:0.0032141079783693767\n",
      "train loss:0.029508945195904794\n",
      "train loss:0.013521345104354092\n",
      "train loss:0.011673893112248437\n",
      "train loss:0.019011743663053568\n",
      "train loss:0.005843655511766422\n",
      "train loss:0.00187288908451786\n",
      "train loss:0.023123819623844192\n",
      "train loss:0.006740266478261842\n",
      "train loss:0.008447002109447821\n",
      "train loss:0.004938955768931646\n",
      "train loss:0.004108751629236414\n",
      "train loss:0.008089535876748665\n",
      "train loss:0.008103446421723384\n",
      "train loss:0.006676301553701406\n",
      "train loss:0.009128460840632333\n",
      "train loss:0.008594291749938008\n",
      "train loss:0.005191540739804038\n",
      "train loss:0.0073432960263541324\n",
      "train loss:0.014074440309995802\n",
      "train loss:0.007925358046982488\n",
      "train loss:0.004258430258003229\n",
      "train loss:0.0021572334919751153\n",
      "train loss:0.0012129950975225243\n",
      "train loss:0.02412286458850099\n",
      "train loss:0.002737691775864403\n",
      "train loss:0.032153553965843\n",
      "train loss:0.014778825889127852\n",
      "train loss:0.004811920999574461\n",
      "train loss:0.007792310154901172\n",
      "train loss:0.005584480314095935\n",
      "train loss:0.005443263851747253\n",
      "train loss:0.0058977847733255205\n",
      "train loss:0.010240459598342104\n",
      "train loss:0.0018330971988158576\n",
      "train loss:0.007627020400348231\n",
      "train loss:0.014013684394610544\n",
      "train loss:0.022636702893734494\n",
      "train loss:0.008166694812264369\n",
      "train loss:0.019034467307029963\n",
      "train loss:0.006187083574301599\n",
      "train loss:0.011797718952100047\n",
      "train loss:0.02807606575439193\n",
      "train loss:0.0049686395342322005\n",
      "train loss:0.00918281060602795\n",
      "train loss:0.030556977223497386\n",
      "train loss:0.010317115387206059\n",
      "train loss:0.006021812492740573\n",
      "train loss:0.010388075031289459\n",
      "train loss:0.01903748225248062\n",
      "train loss:0.01410615272505155\n",
      "train loss:0.0032131732451424217\n",
      "train loss:0.007097019413032324\n",
      "train loss:0.006315033918702612\n",
      "train loss:0.002427117510188242\n",
      "train loss:0.023406134678513584\n",
      "train loss:0.026292089625423846\n",
      "train loss:0.0020807417650772083\n",
      "train loss:0.0012008901463596514\n",
      "train loss:0.011367288283922741\n",
      "train loss:0.005250108812922205\n",
      "train loss:0.008775872817047404\n",
      "train loss:0.016232714331408615\n",
      "train loss:0.015245675860133492\n",
      "train loss:0.00593357696486794\n",
      "train loss:0.028477433529096673\n",
      "train loss:0.01810935176757658\n",
      "train loss:0.012956037941335182\n",
      "train loss:0.0021996304402642215\n",
      "train loss:0.011662147961263431\n",
      "train loss:0.0543592303499656\n",
      "train loss:0.0015196238971159322\n",
      "train loss:0.021727080708995632\n",
      "train loss:0.005979764373160374\n",
      "train loss:0.006428031697446424\n",
      "train loss:0.009487970152737668\n",
      "train loss:0.0016851425100225192\n",
      "train loss:0.015037811602804605\n",
      "train loss:0.0049071443291224945\n",
      "train loss:0.013784446941678208\n",
      "train loss:0.015076892948777142\n",
      "train loss:0.004439181266335294\n",
      "train loss:0.005827846329026693\n",
      "train loss:0.019532885893525892\n",
      "train loss:0.020500862740379065\n",
      "train loss:0.0006769782357934658\n",
      "train loss:0.0034660106494059594\n",
      "train loss:0.0025957870976198844\n",
      "train loss:0.03305985738879454\n",
      "train loss:0.052328940420589616\n",
      "train loss:0.004279929985004488\n",
      "train loss:0.006898112274734247\n",
      "train loss:0.004705975447583599\n",
      "train loss:0.004785642968074618\n",
      "train loss:0.002310661755858769\n",
      "train loss:0.011029326264516802\n",
      "train loss:0.017872565546667538\n",
      "train loss:0.0052631774184323235\n",
      "train loss:0.011730795149731162\n",
      "train loss:0.0011472946290441456\n",
      "train loss:0.027135372547495193\n",
      "train loss:0.005101658711871361\n",
      "train loss:0.009686977010200132\n",
      "train loss:0.0018002232743014107\n",
      "train loss:0.02464759294274656\n",
      "train loss:0.019501844055633932\n",
      "train loss:0.004714247788288516\n",
      "train loss:0.010471410545983802\n",
      "train loss:0.021258547400691757\n",
      "train loss:0.006507960890544777\n",
      "train loss:0.001855250725030095\n",
      "train loss:0.004021194245428053\n",
      "train loss:0.010800772733325333\n",
      "train loss:0.013144598853186348\n",
      "train loss:0.006771908679203834\n",
      "train loss:0.006500377116201653\n",
      "train loss:0.006340733409508667\n",
      "train loss:0.0031081107548384702\n",
      "train loss:0.0066169445671422225\n",
      "train loss:0.005719554577406201\n",
      "train loss:0.012774163071222247\n",
      "train loss:0.004081503494088575\n",
      "train loss:0.01298742205882623\n",
      "train loss:0.006697607387883197\n",
      "train loss:0.008535644384343693\n",
      "train loss:0.014306320690245768\n",
      "train loss:0.011750148325138265\n",
      "train loss:0.04060197814000522\n",
      "train loss:0.009938848028853213\n",
      "train loss:0.003987308991628997\n",
      "train loss:0.006737081575384558\n",
      "train loss:0.0010607376268281922\n",
      "train loss:0.010467030359402042\n",
      "train loss:0.008226546749703589\n",
      "train loss:0.008232779675022077\n",
      "train loss:0.0016848863656429242\n",
      "train loss:0.02308549100767094\n",
      "train loss:0.014808107731859223\n",
      "train loss:0.0028667124637790477\n",
      "train loss:0.0023661938134478923\n",
      "train loss:0.007538539607726205\n",
      "train loss:0.005339413831664958\n",
      "train loss:0.013263470168060908\n",
      "train loss:0.014527628816529503\n",
      "train loss:0.09155417599414994\n",
      "train loss:0.004061248839428898\n",
      "train loss:0.0010577716103263076\n",
      "train loss:0.0020202182213155907\n",
      "train loss:0.0017658073339223857\n",
      "train loss:0.0021432521541359523\n",
      "train loss:0.019396108764636658\n",
      "train loss:0.0824121677118848\n",
      "train loss:0.004797855395099125\n",
      "train loss:0.006620745486342019\n",
      "train loss:0.032235753363345054\n",
      "train loss:0.02201944009224897\n",
      "train loss:0.008475927413894377\n",
      "train loss:0.005056080249818331\n",
      "train loss:0.0026447639199552665\n",
      "train loss:0.014132102153674412\n",
      "train loss:0.004525503325351217\n",
      "train loss:0.014084168988393667\n",
      "train loss:0.010962984791215224\n",
      "train loss:0.0028883393556376007\n",
      "train loss:0.0020994123227717778\n",
      "train loss:0.008800383432396475\n",
      "train loss:0.0010477332319227006\n",
      "train loss:0.01849022833056657\n",
      "train loss:0.010032693168020225\n",
      "train loss:0.051245169007783094\n",
      "train loss:0.01263609207098778\n",
      "train loss:0.023008698075804194\n",
      "train loss:0.00519181348645425\n",
      "train loss:0.01802283127848061\n",
      "=== epoch:9, train acc:0.996, test acc:0.989 ===\n",
      "train loss:0.02149048056148851\n",
      "train loss:0.003873417895262224\n",
      "train loss:0.062228739284958944\n",
      "train loss:0.01948769972919261\n",
      "train loss:0.02512720907444685\n",
      "train loss:0.004630876985848524\n",
      "train loss:0.01698990152012238\n",
      "train loss:0.002401728848820878\n",
      "train loss:0.014495702306640061\n",
      "train loss:0.012804104286456081\n",
      "train loss:0.07566293031301786\n",
      "train loss:0.01381022575446781\n",
      "train loss:0.009165998563875006\n",
      "train loss:0.006781261951544228\n",
      "train loss:0.018731173857546198\n",
      "train loss:0.004568136331606289\n",
      "train loss:0.01354373931152437\n",
      "train loss:0.04899581295394453\n",
      "train loss:0.0027980545051719498\n",
      "train loss:0.015435513888699143\n",
      "train loss:0.008988161821146616\n",
      "train loss:0.0057690352126690755\n",
      "train loss:0.024942689900013623\n",
      "train loss:0.0278925480456816\n",
      "train loss:0.011772397620428483\n",
      "train loss:0.011416148755682014\n",
      "train loss:0.014312981045695639\n",
      "train loss:0.018103328762658237\n",
      "train loss:0.0019100446593753324\n",
      "train loss:0.005308544014126847\n",
      "train loss:0.014578875128617677\n",
      "train loss:0.0015698513407413303\n",
      "train loss:0.00422459656299274\n",
      "train loss:0.02098589038261913\n",
      "train loss:0.012209022307358817\n",
      "train loss:0.02688098132668011\n",
      "train loss:0.0023550854644432085\n",
      "train loss:0.0006509013920766284\n",
      "train loss:0.0077288199630789756\n",
      "train loss:0.009935063177704186\n",
      "train loss:0.004976910364538124\n",
      "train loss:0.0035141324099192484\n",
      "train loss:0.008489636702602542\n",
      "train loss:0.06321388424225101\n",
      "train loss:0.012054016371748516\n",
      "train loss:0.01392356391945881\n",
      "train loss:0.0068002308651495084\n",
      "train loss:0.05836986799874233\n",
      "train loss:0.0038280877006615865\n",
      "train loss:0.011458339428277585\n",
      "train loss:0.014058108821304767\n",
      "train loss:0.05143016896367825\n",
      "train loss:0.002973182775045709\n",
      "train loss:0.02022072709279332\n",
      "train loss:0.0024050723947866143\n",
      "train loss:0.008132286534467695\n",
      "train loss:0.006682212135295904\n",
      "train loss:0.009447132029465255\n",
      "train loss:0.008479105555923011\n",
      "train loss:0.015003638820800387\n",
      "train loss:0.05582980424118101\n",
      "train loss:0.008866241271554973\n",
      "train loss:0.0027288705403822142\n",
      "train loss:0.0018678524145167095\n",
      "train loss:0.023583886224459994\n",
      "train loss:0.016143149637493515\n",
      "train loss:0.002618026150256815\n",
      "train loss:0.018310993064703858\n",
      "train loss:0.01691145981371974\n",
      "train loss:0.005632183960507451\n",
      "train loss:0.004276647454707517\n",
      "train loss:0.021646025929631846\n",
      "train loss:0.020676637239224784\n",
      "train loss:0.004395341239467015\n",
      "train loss:0.003877664604931969\n",
      "train loss:0.005680965655993646\n",
      "train loss:0.008750113081854277\n",
      "train loss:0.004057357368982891\n",
      "train loss:0.05450015983741083\n",
      "train loss:0.03510690740550062\n",
      "train loss:0.01782857187309686\n",
      "train loss:0.023938505247435366\n",
      "train loss:0.016464280102475134\n",
      "train loss:0.016274125351462016\n",
      "train loss:0.014846550511589666\n",
      "train loss:0.015364689730894422\n",
      "train loss:0.03507328203323129\n",
      "train loss:0.0027857320081713112\n",
      "train loss:0.005624961719902963\n",
      "train loss:0.011886010085691637\n",
      "train loss:0.01830791763642118\n",
      "train loss:0.006886356987972602\n",
      "train loss:0.004680494795755194\n",
      "train loss:0.012006271178453645\n",
      "train loss:0.0035759657473699276\n",
      "train loss:0.00485742429997084\n",
      "train loss:0.004471007417581018\n",
      "train loss:0.008190284708495086\n",
      "train loss:0.025149565508858152\n",
      "train loss:0.019530746063349872\n",
      "train loss:0.0029728840331796004\n",
      "train loss:0.0045378404600149415\n",
      "train loss:0.005145781847597094\n",
      "train loss:0.006897768181071101\n",
      "train loss:0.001960555876797081\n",
      "train loss:0.010785170092252194\n",
      "train loss:0.014061137029487952\n",
      "train loss:0.014758609628267916\n",
      "train loss:0.004282259928542177\n",
      "train loss:0.004336931014401146\n",
      "train loss:0.005115267101628399\n",
      "train loss:0.011892966167878365\n",
      "train loss:0.011456875964486054\n",
      "train loss:0.011113367046218429\n",
      "train loss:0.003036418465555366\n",
      "train loss:0.00237772141488437\n",
      "train loss:0.012533817834803238\n",
      "train loss:0.012085170819541755\n",
      "train loss:0.007880370386244718\n",
      "train loss:0.005944351332980095\n",
      "train loss:0.0029432410363397763\n",
      "train loss:0.0299385139190153\n",
      "train loss:0.011259606331921038\n",
      "train loss:0.025167564358935853\n",
      "train loss:0.0025839674685810767\n",
      "train loss:0.007993046456273618\n",
      "train loss:0.003710660182812401\n",
      "train loss:0.06578382821077518\n",
      "train loss:0.012457287537351577\n",
      "train loss:0.009518181899836563\n",
      "train loss:0.0033950779785883924\n",
      "train loss:0.005599507031326934\n",
      "train loss:0.02435245202595298\n",
      "train loss:0.008173440498956165\n",
      "train loss:0.05111824501868365\n",
      "train loss:0.012282695685909988\n",
      "train loss:0.0030203920288482235\n",
      "train loss:0.0011649481711207823\n",
      "train loss:0.0018510586858799985\n",
      "train loss:0.003384292421793717\n",
      "train loss:0.006651758092096851\n",
      "train loss:0.003464646572342492\n",
      "train loss:0.002816767166971967\n",
      "train loss:0.001498777387201472\n",
      "train loss:0.0027170275369171747\n",
      "train loss:0.006752715259195137\n",
      "train loss:0.01780915730082676\n",
      "train loss:0.001707114926571765\n",
      "train loss:0.006117787364546381\n",
      "train loss:0.013620733050683193\n",
      "train loss:0.010099001264801873\n",
      "train loss:0.0014864401037114083\n",
      "train loss:0.01583122913337192\n",
      "train loss:0.02611856060004542\n",
      "train loss:0.01259199524170827\n",
      "train loss:0.025295488033176904\n",
      "train loss:0.0063823860008644165\n",
      "train loss:0.0064472199796570375\n",
      "train loss:0.010967685346609392\n",
      "train loss:0.003234934680675695\n",
      "train loss:0.01908606523549424\n",
      "train loss:0.03879310094422356\n",
      "train loss:0.0011166407116184446\n",
      "train loss:0.01730945526045261\n",
      "train loss:0.0035181303897752012\n",
      "train loss:0.0014929783596819032\n",
      "train loss:0.02966632488867103\n",
      "train loss:0.0020887797314923674\n",
      "train loss:0.002643279482293157\n",
      "train loss:0.013443425987054636\n",
      "train loss:0.005603875760737505\n",
      "train loss:0.006037081463019608\n",
      "train loss:0.006996344659663618\n",
      "train loss:0.0023849691286795694\n",
      "train loss:0.008258529166374588\n",
      "train loss:0.03012194578887716\n",
      "train loss:0.016102550793919188\n",
      "train loss:0.006936483806079461\n",
      "train loss:0.010578958040192028\n",
      "train loss:0.01048579411051556\n",
      "train loss:0.0041451818315893065\n",
      "train loss:0.010263618375415316\n",
      "train loss:0.002505195817654387\n",
      "train loss:0.006287872308506949\n",
      "train loss:0.005308552512766008\n",
      "train loss:0.027274562449498697\n",
      "train loss:0.012089729775768387\n",
      "train loss:0.016367687640793615\n",
      "train loss:0.002540280424499175\n",
      "train loss:0.0008058448348128415\n",
      "train loss:0.0321669798894792\n",
      "train loss:0.004131818068463834\n",
      "train loss:0.002982684216005356\n",
      "train loss:0.002609442100406829\n",
      "train loss:0.007104081796592943\n",
      "train loss:0.009276470466906127\n",
      "train loss:0.0118192645894226\n",
      "train loss:0.005769605332310423\n",
      "train loss:0.008763051143990081\n",
      "train loss:0.029987737708015168\n",
      "train loss:0.0015334365206850062\n",
      "train loss:0.0029867997701332294\n",
      "train loss:0.014591558339265764\n",
      "train loss:0.005339852780186601\n",
      "train loss:0.0077793892741117796\n",
      "train loss:0.0036904205327910624\n",
      "train loss:0.004221090890298554\n",
      "train loss:0.003681214533485462\n",
      "train loss:0.002843842003016969\n",
      "train loss:0.0049314999345909235\n",
      "train loss:0.008383594941798201\n",
      "train loss:0.00572749780164924\n",
      "train loss:0.026098899013453424\n",
      "train loss:0.0042436632033771475\n",
      "train loss:0.0027329333102948383\n",
      "train loss:0.0010832385328684545\n",
      "train loss:0.0019399820247368927\n",
      "train loss:0.017609132198463114\n",
      "train loss:0.008119021945378398\n",
      "train loss:0.004039528790726653\n",
      "train loss:0.000599161462869376\n",
      "train loss:0.004339958969693202\n",
      "train loss:0.011780219107008041\n",
      "train loss:0.007481470991336182\n",
      "train loss:0.005456156185604073\n",
      "train loss:0.011956032661212397\n",
      "train loss:0.002415259946368454\n",
      "train loss:0.0023410368510671377\n",
      "train loss:0.009695563100920863\n",
      "train loss:0.005019678447872139\n",
      "train loss:0.005223470864850318\n",
      "train loss:0.001862773737772851\n",
      "train loss:0.02881093847472072\n",
      "train loss:0.009766008161879525\n",
      "train loss:0.0038520763777188764\n",
      "train loss:0.039639888734003516\n",
      "train loss:0.0076298398358231835\n",
      "train loss:0.001475522250241845\n",
      "train loss:0.00393026998525072\n",
      "train loss:0.0032732746696383184\n",
      "train loss:0.010046125403772801\n",
      "train loss:0.0020249674039210853\n",
      "train loss:0.0037185503904870187\n",
      "train loss:0.0017707626110683627\n",
      "train loss:0.0014131364257824628\n",
      "train loss:0.007696600040353527\n",
      "train loss:0.00065367233117184\n",
      "train loss:0.01281101901154484\n",
      "train loss:0.01150310077028067\n",
      "train loss:0.0014819347502825688\n",
      "train loss:0.0038309798447934513\n",
      "train loss:0.012138241511479145\n",
      "train loss:0.01287469160287879\n",
      "train loss:0.012238220959682633\n",
      "train loss:0.017752406867337586\n",
      "train loss:0.01823445311422914\n",
      "train loss:0.00220913437584801\n",
      "train loss:0.010310055448830386\n",
      "train loss:0.0022513559800867093\n",
      "train loss:0.011672105287502343\n",
      "train loss:0.0005700681928878146\n",
      "train loss:0.006372251653062969\n",
      "train loss:0.0752373315160597\n",
      "train loss:0.010755966142190194\n",
      "train loss:0.0040290799949199065\n",
      "train loss:0.0021750663479974076\n",
      "train loss:0.0109181274506646\n",
      "train loss:0.011365216823951035\n",
      "train loss:0.009221818795527265\n",
      "train loss:0.001966048339273117\n",
      "train loss:0.002529391971103875\n",
      "train loss:0.002020204307212558\n",
      "train loss:0.013221010954226984\n",
      "train loss:0.015749438887789666\n",
      "train loss:0.001994516357406497\n",
      "train loss:0.015593637215853946\n",
      "train loss:0.007121768286568315\n",
      "train loss:0.013826906737022679\n",
      "train loss:0.010975360621009544\n",
      "train loss:0.005747247515036619\n",
      "train loss:0.002555166449854409\n",
      "train loss:0.008044363863626404\n",
      "train loss:0.003557835652944582\n",
      "train loss:0.016380503509905848\n",
      "train loss:0.006173283533908233\n",
      "train loss:0.005858039542276432\n",
      "train loss:0.03152146807380419\n",
      "train loss:0.012457205289732227\n",
      "train loss:0.004225925438339702\n",
      "train loss:0.00231413160579739\n",
      "train loss:0.0013502373003976057\n",
      "train loss:0.013024024272182496\n",
      "train loss:0.007509619032306733\n",
      "train loss:0.0017349219868074484\n",
      "train loss:0.0009880408737080863\n",
      "train loss:0.01135754335248635\n",
      "train loss:0.001182830716654428\n",
      "train loss:0.018834617537068894\n",
      "train loss:0.009194852264621536\n",
      "train loss:0.023325517008548787\n",
      "train loss:0.004539111118233566\n",
      "train loss:0.003819395235239549\n",
      "train loss:0.00434853518353674\n",
      "train loss:0.022843625763787533\n",
      "train loss:0.0015799586102492836\n",
      "train loss:0.010485988383484302\n",
      "train loss:0.0034143143831282404\n",
      "train loss:0.04191759653977179\n",
      "train loss:0.003452885601527363\n",
      "train loss:0.014471924461486934\n",
      "train loss:0.009215643704715237\n",
      "train loss:0.0030961266843489346\n",
      "train loss:0.001108948035952141\n",
      "train loss:0.011091317575084357\n",
      "train loss:0.0004927893668014811\n",
      "train loss:0.021788662675742615\n",
      "train loss:0.0021552304553464742\n",
      "train loss:0.017980619953418077\n",
      "train loss:0.004224310882867291\n",
      "train loss:0.005939895125222001\n",
      "train loss:0.000696750512194111\n",
      "train loss:0.003927552737506878\n",
      "train loss:0.013920184094858392\n",
      "train loss:0.00383660903571505\n",
      "train loss:0.0031212986268524313\n",
      "train loss:0.0018895543534895828\n",
      "train loss:0.004404091585006799\n",
      "train loss:0.03762903911022417\n",
      "train loss:0.0027354319216110596\n",
      "train loss:0.013494786063351374\n",
      "train loss:0.004265565666770944\n",
      "train loss:0.01681472405416133\n",
      "train loss:0.009282140222843377\n",
      "train loss:0.004772159457474827\n",
      "train loss:0.015205523724904847\n",
      "train loss:0.0014364757795738482\n",
      "train loss:0.005418610315584374\n",
      "train loss:0.0066096827734383875\n",
      "train loss:0.0026227482977912854\n",
      "train loss:0.015452443121650918\n",
      "train loss:0.017223006336580724\n",
      "train loss:0.0011145098493850045\n",
      "train loss:0.03911456849468654\n",
      "train loss:0.010974174510621054\n",
      "train loss:0.0035770917121343126\n",
      "train loss:0.003859988880401463\n",
      "train loss:0.0011044905092668256\n",
      "train loss:0.003364998267280579\n",
      "train loss:0.005696928019948805\n",
      "train loss:0.012401885092780512\n",
      "train loss:0.005968503561593096\n",
      "train loss:0.026508654815293256\n",
      "train loss:0.022153532859137584\n",
      "train loss:0.017457865737883427\n",
      "train loss:0.019467349860646327\n",
      "train loss:0.004755899428551721\n",
      "train loss:0.00871942699313291\n",
      "train loss:0.01715322098041198\n",
      "train loss:0.009696991656514737\n",
      "train loss:0.010817515848144867\n",
      "train loss:0.003254796860499904\n",
      "train loss:0.009127203949522777\n",
      "train loss:0.1913882337582632\n",
      "train loss:0.013954728297515461\n",
      "train loss:0.015352088356178028\n",
      "train loss:0.0022687513474728914\n",
      "train loss:0.04578819131607262\n",
      "train loss:0.02524188511948436\n",
      "train loss:0.009007206890031021\n",
      "train loss:0.010946529015848553\n",
      "train loss:0.0036294671131700666\n",
      "train loss:0.007332939258757354\n",
      "train loss:0.0028529119769038175\n",
      "train loss:0.010293708383910085\n",
      "train loss:0.005897493984775515\n",
      "train loss:0.005790151175084034\n",
      "train loss:0.0010528370817183047\n",
      "train loss:0.011114038865814904\n",
      "train loss:0.00422943684291893\n",
      "train loss:0.0009309118639903205\n",
      "train loss:0.007528638692529558\n",
      "train loss:0.0026609942131451555\n",
      "train loss:0.004980060984582062\n",
      "train loss:0.005889139124538685\n",
      "train loss:0.0038008676887109414\n",
      "train loss:0.006324887857523802\n",
      "train loss:0.021771496963479894\n",
      "train loss:0.021494221683622413\n",
      "train loss:0.004425185951026678\n",
      "train loss:0.006661509717290292\n",
      "train loss:0.008143781102079736\n",
      "train loss:0.008838088949330332\n",
      "train loss:0.003998608251434277\n",
      "train loss:0.004215084123429897\n",
      "train loss:0.028862711351715694\n",
      "train loss:0.027425900637721757\n",
      "train loss:0.011074094717709072\n",
      "train loss:0.010055547998919903\n",
      "train loss:0.006304692594464779\n",
      "train loss:0.012114420534465247\n",
      "train loss:0.0024350102474976145\n",
      "train loss:0.01884408512157533\n",
      "train loss:0.004481477751066394\n",
      "train loss:0.09127964403659228\n",
      "train loss:0.002875511887309072\n",
      "train loss:0.005077730232283067\n",
      "train loss:0.019556817091644762\n",
      "train loss:0.005165368361293999\n",
      "train loss:0.0014137848844020173\n",
      "train loss:0.00882259234210899\n",
      "train loss:0.016133028940404703\n",
      "train loss:0.01644957455766265\n",
      "train loss:0.016204176692973225\n",
      "train loss:0.0019138298238113715\n",
      "train loss:0.0013143411335236775\n",
      "train loss:0.004219604647451478\n",
      "train loss:0.01220808195962621\n",
      "train loss:0.06202055440528185\n",
      "train loss:0.011106919357353336\n",
      "train loss:0.005997612318795841\n",
      "train loss:0.015181138531861295\n",
      "train loss:0.004270989268928788\n",
      "train loss:0.0016298301450155509\n",
      "train loss:0.010902305055144916\n",
      "train loss:0.018789533253997088\n",
      "train loss:0.003070118484655098\n",
      "train loss:0.009146704030881754\n",
      "train loss:0.037967273590676684\n",
      "train loss:0.026749815450074373\n",
      "train loss:0.022935655562802672\n",
      "train loss:0.002992331049328227\n",
      "train loss:0.002273984987532947\n",
      "train loss:0.0016304272815302154\n",
      "train loss:0.0042875206251355834\n",
      "train loss:0.004061395565768205\n",
      "train loss:0.005160409783514208\n",
      "train loss:0.004247402022140891\n",
      "train loss:0.006741559673984868\n",
      "train loss:0.002284495739565978\n",
      "train loss:0.008997409143946599\n",
      "train loss:0.008930504672209523\n",
      "train loss:0.001484868997289536\n",
      "train loss:0.0007769343724834281\n",
      "train loss:0.0077716229979034\n",
      "train loss:0.004152533283243351\n",
      "train loss:0.0030642576683958655\n",
      "train loss:0.002498888732375846\n",
      "train loss:0.01760125639618954\n",
      "train loss:0.03273608885529684\n",
      "train loss:0.004730619496461688\n",
      "train loss:0.00680020671939166\n",
      "train loss:0.007398856508064968\n",
      "train loss:0.0030411536585905606\n",
      "train loss:0.0033513185190594467\n",
      "train loss:0.001992171445896833\n",
      "train loss:0.003483488099346349\n",
      "train loss:0.005526213896742533\n",
      "train loss:0.03785023755343675\n",
      "train loss:0.0072100403121622405\n",
      "train loss:0.013087047456796714\n",
      "train loss:0.0012421736872230888\n",
      "train loss:0.0049025911701858\n",
      "train loss:0.012436661045972342\n",
      "train loss:0.0028223726503745685\n",
      "train loss:0.01610708613882832\n",
      "train loss:0.0032674981557482595\n",
      "train loss:0.01587108558765205\n",
      "train loss:0.006254602893754199\n",
      "train loss:0.006848120112246281\n",
      "train loss:0.023468379072481317\n",
      "train loss:0.004940323076991961\n",
      "train loss:0.0032584291037310014\n",
      "train loss:0.0023038240719251753\n",
      "train loss:0.0016566485000997055\n",
      "train loss:0.013969961718942133\n",
      "train loss:0.0021636495492861714\n",
      "train loss:0.02136924000522007\n",
      "train loss:0.01704853676342752\n",
      "train loss:0.003049262684277592\n",
      "train loss:0.0030156528806217124\n",
      "train loss:0.00415340979196409\n",
      "train loss:0.011480225124897261\n",
      "train loss:0.006407432475763586\n",
      "train loss:0.005491890310975387\n",
      "train loss:0.012542150921998068\n",
      "train loss:0.012687736590061062\n",
      "train loss:0.0020278270700043654\n",
      "train loss:0.03389134617493805\n",
      "train loss:0.008377562790945375\n",
      "train loss:0.011018946433107267\n",
      "train loss:0.003099986153117761\n",
      "train loss:0.007638344693523309\n",
      "train loss:0.001496238305196073\n",
      "train loss:0.009859249567037093\n",
      "train loss:0.005946545105455264\n",
      "train loss:0.006612483090156004\n",
      "train loss:0.008144081369659028\n",
      "train loss:0.0028551059746671848\n",
      "train loss:0.0024849596545682144\n",
      "train loss:0.0012434117650004732\n",
      "train loss:0.006116100994465348\n",
      "train loss:0.0007458900143082073\n",
      "train loss:0.0068284037847826835\n",
      "train loss:0.0007219859290714096\n",
      "train loss:0.00833740889808774\n",
      "train loss:0.0021813289902827947\n",
      "train loss:0.0061934228051559286\n",
      "train loss:0.0523888456112888\n",
      "train loss:0.004628502616757128\n",
      "train loss:0.0017193590071855954\n",
      "train loss:0.016307225583258603\n",
      "train loss:0.00356433298035927\n",
      "train loss:0.01712325072297085\n",
      "train loss:0.04347725624350753\n",
      "train loss:0.0029190310910080586\n",
      "train loss:0.003924419042340872\n",
      "train loss:0.0020392316338195726\n",
      "train loss:0.003761417161186852\n",
      "train loss:0.0037267706495286886\n",
      "train loss:0.020962627518679076\n",
      "train loss:0.009382405172376042\n",
      "train loss:0.04468056013516487\n",
      "train loss:0.009753772714500944\n",
      "train loss:0.016326342761277913\n",
      "train loss:0.007162937828211713\n",
      "train loss:0.005203091680628177\n",
      "train loss:0.0010471520344490061\n",
      "train loss:0.020592388450058122\n",
      "train loss:0.03734169995639362\n",
      "train loss:0.005744500687007869\n",
      "train loss:0.005043617501285707\n",
      "train loss:0.006750277180793764\n",
      "train loss:0.010453259776584976\n",
      "train loss:0.0012092227120737665\n",
      "train loss:0.0214314134092617\n",
      "train loss:0.0207094079895008\n",
      "train loss:0.010369192537994294\n",
      "train loss:0.013281195217592589\n",
      "train loss:0.0010129748099332032\n",
      "train loss:0.003073716495143024\n",
      "train loss:0.0030925551759449343\n",
      "train loss:0.021211824252798776\n",
      "train loss:0.008761674788456148\n",
      "train loss:0.001893437018951863\n",
      "train loss:0.0005682141760258011\n",
      "train loss:0.002609628872177172\n",
      "train loss:0.001617803238008101\n",
      "train loss:0.0019959931497464983\n",
      "train loss:0.0009712794345369591\n",
      "train loss:0.0053221444771806145\n",
      "train loss:0.004317332137294554\n",
      "train loss:0.0016168765438031888\n",
      "train loss:0.00876206909422521\n",
      "train loss:0.001506111596030842\n",
      "train loss:0.008686860151444199\n",
      "train loss:0.005787181774647409\n",
      "train loss:0.006994790654876588\n",
      "train loss:0.0006119980658247842\n",
      "train loss:0.004112500470365596\n",
      "train loss:0.0012335391015635953\n",
      "train loss:0.0053072603489669155\n",
      "train loss:0.0333792157577715\n",
      "train loss:0.0006250288715261956\n",
      "train loss:0.0014224463388741062\n",
      "train loss:0.0019266491345955835\n",
      "train loss:0.005475804490856409\n",
      "train loss:0.005665908762578497\n",
      "train loss:0.0012397450945765326\n",
      "train loss:0.0011328870594403025\n",
      "train loss:0.019872164415742036\n",
      "train loss:0.006501519226532189\n",
      "train loss:0.008056540971031633\n",
      "train loss:0.03678277318738992\n",
      "train loss:0.013677500049970127\n",
      "train loss:0.002643094855488022\n",
      "train loss:0.007892339980971486\n",
      "train loss:0.022704490397001297\n",
      "train loss:0.009474213288757635\n",
      "train loss:0.07464302478631046\n",
      "train loss:0.0026735675488434143\n",
      "train loss:0.009944637782864942\n",
      "train loss:0.03912384610227435\n",
      "train loss:0.0004316173097482141\n",
      "train loss:0.009458035635298255\n",
      "train loss:0.0022828696326062424\n",
      "train loss:0.04983282164375673\n",
      "train loss:0.0032552775970249807\n",
      "train loss:0.011721244899316127\n",
      "train loss:0.010191473418336358\n",
      "train loss:0.028146331110064416\n",
      "train loss:0.021022757464619138\n",
      "train loss:0.0075669760563654145\n",
      "train loss:0.00330413901261427\n",
      "train loss:0.003524535024722905\n",
      "train loss:0.038740478769174407\n",
      "train loss:0.0035555529426280507\n",
      "train loss:0.0016249868740122186\n",
      "train loss:0.0023996106511675283\n",
      "train loss:0.014643269764255196\n",
      "train loss:0.002724772458657063\n",
      "=== epoch:10, train acc:0.997, test acc:0.982 ===\n",
      "train loss:0.03899254497233877\n",
      "train loss:0.018296975881157456\n",
      "train loss:0.0009049461872349177\n",
      "train loss:0.0014843289551886673\n",
      "train loss:0.009654413557616218\n",
      "train loss:0.010936372859990154\n",
      "train loss:0.0706147295488296\n",
      "train loss:0.0015810523850310501\n",
      "train loss:0.008628485613900911\n",
      "train loss:0.009205234253321317\n",
      "train loss:0.0007315947492840579\n",
      "train loss:0.005468547327335148\n",
      "train loss:0.001311894732952871\n",
      "train loss:0.008298118024598289\n",
      "train loss:0.0016375288790449547\n",
      "train loss:0.021416851046790426\n",
      "train loss:0.019448027919140635\n",
      "train loss:0.024579391385817297\n",
      "train loss:0.0053141330854668415\n",
      "train loss:0.014179763205339266\n",
      "train loss:0.02713417729734064\n",
      "train loss:0.007305825945319716\n",
      "train loss:0.00877008548282\n",
      "train loss:0.010166203418532012\n",
      "train loss:0.016654499549198166\n",
      "train loss:0.006951853084288592\n",
      "train loss:0.0018297981571834942\n",
      "train loss:0.00800671854158227\n",
      "train loss:0.0070662124987890115\n",
      "train loss:0.008023989527603982\n",
      "train loss:0.005931500267607342\n",
      "train loss:0.002122684032436539\n",
      "train loss:0.01534582826800403\n",
      "train loss:0.008153696962243695\n",
      "train loss:0.004301795927684798\n",
      "train loss:0.003132466430013172\n",
      "train loss:0.005910113482543717\n",
      "train loss:0.006545083458808426\n",
      "train loss:0.008619356440157346\n",
      "train loss:0.0015474639186569014\n",
      "train loss:0.0018717130271563914\n",
      "train loss:0.0024676470756157362\n",
      "train loss:0.06356400930889566\n",
      "train loss:0.01383654774210967\n",
      "train loss:0.008927940802331114\n",
      "train loss:0.014834108053914035\n",
      "train loss:0.013739778423777023\n",
      "train loss:0.014691216840934178\n",
      "train loss:0.003967493810772613\n",
      "train loss:0.004777506100194294\n",
      "train loss:0.012527501571615538\n",
      "train loss:0.030046018670130337\n",
      "train loss:0.011550432422931878\n",
      "train loss:0.028497261912629124\n",
      "train loss:0.009825657152644713\n",
      "train loss:0.015454002956690716\n",
      "train loss:0.0032107155993823426\n",
      "train loss:0.019268828169185806\n",
      "train loss:0.004054635908790116\n",
      "train loss:0.007704642008116313\n",
      "train loss:0.01795698783333299\n",
      "train loss:0.018410726181959762\n",
      "train loss:0.01369589167206334\n",
      "train loss:0.00224329530501195\n",
      "train loss:0.003255946152113786\n",
      "train loss:0.05035008029987709\n",
      "train loss:0.02295504576644122\n",
      "train loss:0.008654471789933126\n",
      "train loss:0.0038323602542721436\n",
      "train loss:0.013553699796439172\n",
      "train loss:0.004513629381206781\n",
      "train loss:0.01059272399134749\n",
      "train loss:0.028760243661446815\n",
      "train loss:0.014290390007222152\n",
      "train loss:0.01723085929048619\n",
      "train loss:0.0036040668894165256\n",
      "train loss:0.0021882712255385335\n",
      "train loss:0.005067660635106814\n",
      "train loss:0.005550777981897472\n",
      "train loss:0.010665763652534625\n",
      "train loss:0.014666382821900123\n",
      "train loss:0.003965490460568432\n",
      "train loss:0.003697046963472469\n",
      "train loss:0.021344422959057065\n",
      "train loss:0.021077458287154148\n",
      "train loss:0.0037103571716067874\n",
      "train loss:0.020763093229225715\n",
      "train loss:0.013466996808496617\n",
      "train loss:0.007843579097474618\n",
      "train loss:0.0022108195126666276\n",
      "train loss:0.0047571395799390695\n",
      "train loss:0.014710606114914107\n",
      "train loss:0.009402183634539884\n",
      "train loss:0.013191815556674284\n",
      "train loss:0.005282241995396265\n",
      "train loss:0.02563339342303298\n",
      "train loss:0.005502475822536113\n",
      "train loss:0.02435227503554976\n",
      "train loss:0.023688010316469135\n",
      "train loss:0.0057736594621936475\n",
      "train loss:0.017375434006333613\n",
      "train loss:0.0022786720792879953\n",
      "train loss:0.017844704425516996\n",
      "train loss:0.0020056585156604857\n",
      "train loss:0.006663392283621331\n",
      "train loss:0.0012822401807919214\n",
      "train loss:0.009612175597597446\n",
      "train loss:0.01238568354281828\n",
      "train loss:0.006227045268947034\n",
      "train loss:0.0022333062823484883\n",
      "train loss:0.003133053225610212\n",
      "train loss:0.010565536477090645\n",
      "train loss:0.0028500975846052694\n",
      "train loss:0.0019750395007497703\n",
      "train loss:0.009665851535536236\n",
      "train loss:0.007663302300228517\n",
      "train loss:0.04337955728400288\n",
      "train loss:0.004097413036184135\n",
      "train loss:0.00564477776610493\n",
      "train loss:0.0012309041200445924\n",
      "train loss:0.010772394341192447\n",
      "train loss:0.010403584900876476\n",
      "train loss:0.0008462814445114008\n",
      "train loss:0.005080110975601565\n",
      "train loss:0.000888381740639753\n",
      "train loss:0.011191657801503375\n",
      "train loss:0.031234585961200385\n",
      "train loss:0.0008669407359722742\n",
      "train loss:0.007440135492595805\n",
      "train loss:0.027226490448973073\n",
      "train loss:0.001742652337520586\n",
      "train loss:0.007966056251631303\n",
      "train loss:0.00916945269290625\n",
      "train loss:0.008033999266663623\n",
      "train loss:0.026832517802367586\n",
      "train loss:0.008702548945066987\n",
      "train loss:0.003399338426124399\n",
      "train loss:0.08173767436469159\n",
      "train loss:0.0027247861610056184\n",
      "train loss:0.00952394422283684\n",
      "train loss:0.005057975052095071\n",
      "train loss:0.003304921388000489\n",
      "train loss:0.002872640934724201\n",
      "train loss:0.015760774845739692\n",
      "train loss:0.004910679462570803\n",
      "train loss:0.0653506955418257\n",
      "train loss:0.022151676373518007\n",
      "train loss:0.0005113873800105276\n",
      "train loss:0.006742298091664025\n",
      "train loss:0.005373704398679759\n",
      "train loss:0.007747687244720111\n",
      "train loss:0.0025504489802795583\n",
      "train loss:0.00346495259007914\n",
      "train loss:0.0069656009138189135\n",
      "train loss:0.007352502678205952\n",
      "train loss:0.027422780831471934\n",
      "train loss:0.011320633225931052\n",
      "train loss:0.007643701999278255\n",
      "train loss:0.012283270645775772\n",
      "train loss:0.0009619960322413065\n",
      "train loss:0.008873367416207178\n",
      "train loss:0.006725375233039705\n",
      "train loss:0.0014469563751626292\n",
      "train loss:0.007003932485642373\n",
      "train loss:0.0036288506169565977\n",
      "train loss:0.0018773927336783378\n",
      "train loss:0.011970435836696342\n",
      "train loss:0.011853798504078791\n",
      "train loss:0.013395293024376706\n",
      "train loss:0.004124245722343875\n",
      "train loss:0.004008864625892976\n",
      "train loss:0.006288606944012493\n",
      "train loss:0.0038654845244021152\n",
      "train loss:0.0018535908509302937\n",
      "train loss:0.0029088093564476826\n",
      "train loss:0.009506779174978656\n",
      "train loss:0.0160280243299775\n",
      "train loss:0.016942783291103193\n",
      "train loss:0.029865899439626174\n",
      "train loss:0.0016921589671249045\n",
      "train loss:0.00412309014492635\n",
      "train loss:0.01460793928530643\n",
      "train loss:0.0013810283465247558\n",
      "train loss:0.0019048131498975967\n",
      "train loss:0.0027612908093700345\n",
      "train loss:0.0015962412795503544\n",
      "train loss:0.008298168103822529\n",
      "train loss:0.005149967083549285\n",
      "train loss:0.005117669090583355\n",
      "train loss:0.005734092267300662\n",
      "train loss:0.003489272320408488\n",
      "train loss:0.008462875114341495\n",
      "train loss:0.006894239237210815\n",
      "train loss:0.0026122576783323996\n",
      "train loss:0.0037824207912921237\n",
      "train loss:0.0011549920345772394\n",
      "train loss:0.0021337118036760393\n",
      "train loss:0.005327093426067512\n",
      "train loss:0.027638427069864472\n",
      "train loss:0.029171287557955172\n",
      "train loss:0.003365302945218216\n",
      "train loss:0.038206986643108065\n",
      "train loss:0.003839609881207962\n",
      "train loss:0.008904078583864085\n",
      "train loss:0.0017311656540579174\n",
      "train loss:0.034869918936587536\n",
      "train loss:0.009528791096286012\n",
      "train loss:0.006086459551343103\n",
      "train loss:0.0068864766802010195\n",
      "train loss:0.0019592309624446524\n",
      "train loss:0.0018336060273619267\n",
      "train loss:0.038924844621594554\n",
      "train loss:0.004148406707457803\n",
      "train loss:0.025964417004234935\n",
      "train loss:0.005712319480031035\n",
      "train loss:0.0011666544976287466\n",
      "train loss:0.0050177477916521695\n",
      "train loss:0.006371422409031305\n",
      "train loss:0.0014810820978064807\n",
      "train loss:0.040914778548852934\n",
      "train loss:0.0016746529600877936\n",
      "train loss:0.015739185383669044\n",
      "train loss:0.0028062484784168286\n",
      "train loss:0.03247679321789003\n",
      "train loss:0.0036748793697577355\n",
      "train loss:0.005241301431929737\n",
      "train loss:0.004032614333994169\n",
      "train loss:0.004211554045968084\n",
      "train loss:0.0018240111639918242\n",
      "train loss:0.0030374153961524276\n",
      "train loss:0.049395887224530935\n",
      "train loss:0.0066507398910506275\n",
      "train loss:0.004098797690665325\n",
      "train loss:0.004470578311460054\n",
      "train loss:0.007013134379682356\n",
      "train loss:0.016248367933228428\n",
      "train loss:0.004152339929692704\n",
      "train loss:0.012301109539748962\n",
      "train loss:0.0026323689416547515\n",
      "train loss:0.0033632572612168703\n",
      "train loss:0.010211284029159549\n",
      "train loss:0.003153982706015559\n",
      "train loss:0.009378672094167831\n",
      "train loss:0.06386249174064797\n",
      "train loss:0.004649512132187266\n",
      "train loss:0.005197678747861021\n",
      "train loss:0.0015049461130754874\n",
      "train loss:0.010007858784247028\n",
      "train loss:0.004406005295459827\n",
      "train loss:0.04824755490943118\n",
      "train loss:0.0025133050072554232\n",
      "train loss:0.0055916120271673475\n",
      "train loss:0.006200299905585916\n",
      "train loss:0.010237651255597438\n",
      "train loss:0.0015594603103580911\n",
      "train loss:0.015007820344119863\n",
      "train loss:0.021556284276217363\n",
      "train loss:0.0014939718358661178\n",
      "train loss:0.0006203997673698748\n",
      "train loss:0.02757803300008348\n",
      "train loss:0.006687874226073126\n",
      "train loss:0.0017208621343457018\n",
      "train loss:0.006893895014844917\n",
      "train loss:0.01018282237518274\n",
      "train loss:0.04648071185939587\n",
      "train loss:0.0007838077508648703\n",
      "train loss:0.0017164720272748193\n",
      "train loss:0.00386037212339778\n",
      "train loss:0.015006566485188838\n",
      "train loss:0.0038446489381257763\n",
      "train loss:0.004972209564719857\n",
      "train loss:0.10438608414583143\n",
      "train loss:0.004702393869098772\n",
      "train loss:0.0024983589421086376\n",
      "train loss:0.0024625956086575295\n",
      "train loss:0.007400018143891411\n",
      "train loss:0.005230367002661358\n",
      "train loss:0.006169552280197764\n",
      "train loss:0.0044465684971390865\n",
      "train loss:0.007621632775087615\n",
      "train loss:0.03260146739125301\n",
      "train loss:0.009249605029355904\n",
      "train loss:0.00771811194456696\n",
      "train loss:0.001869845378407006\n",
      "train loss:0.003963282688549668\n",
      "train loss:0.02185466886526709\n",
      "train loss:0.003671028867063135\n",
      "train loss:0.0038045529555939704\n",
      "train loss:0.008046815627692152\n",
      "train loss:0.010599709225115187\n",
      "train loss:0.005252061092433963\n",
      "train loss:0.002633105723411662\n",
      "train loss:0.004305152757912876\n",
      "train loss:0.008042862348179147\n",
      "train loss:0.028046925729409967\n",
      "train loss:0.0072134484035932134\n",
      "train loss:0.023234630212885896\n",
      "train loss:0.004510234048738356\n",
      "train loss:0.03617753298908691\n",
      "train loss:0.018324581191081782\n",
      "train loss:0.03479546297398167\n",
      "train loss:0.0028321643122796347\n",
      "train loss:0.001303115156841602\n",
      "train loss:0.04124457060601712\n",
      "train loss:0.0029464661516566846\n",
      "train loss:0.015216308832923615\n",
      "train loss:0.02858619594296606\n",
      "train loss:0.022218491616558565\n",
      "train loss:0.009287633165653687\n",
      "train loss:0.009012500766943033\n",
      "train loss:0.0021328179867010954\n",
      "train loss:0.05948834178479359\n",
      "train loss:0.0032612946743000386\n",
      "train loss:0.0047657637025262465\n",
      "train loss:0.012743707067159655\n",
      "train loss:0.016465445156957983\n",
      "train loss:0.007783520186851804\n",
      "train loss:0.007404564280817927\n",
      "train loss:0.001603918098432304\n",
      "train loss:0.013480285958665683\n",
      "train loss:0.002972398876296636\n",
      "train loss:0.006020556273916528\n",
      "train loss:0.009990127408742744\n",
      "train loss:0.024426260432800363\n",
      "train loss:0.0029790775452334488\n",
      "train loss:0.011619218585976617\n",
      "train loss:0.011434388875378631\n",
      "train loss:0.021613757543059567\n",
      "train loss:0.0009339889087705222\n",
      "train loss:0.0032764313453072296\n",
      "train loss:0.0007564315725404574\n",
      "train loss:0.010824083138989476\n",
      "train loss:0.005075012648621278\n",
      "train loss:0.01656108301754493\n",
      "train loss:0.012251987443153652\n",
      "train loss:0.001360492833801511\n",
      "train loss:0.04014883595491237\n",
      "train loss:0.004513365821050949\n",
      "train loss:0.009428009379229954\n",
      "train loss:0.008381574921248221\n",
      "train loss:0.013162526201267915\n",
      "train loss:0.010235066802020016\n",
      "train loss:0.005889221732986419\n",
      "train loss:0.0020366898157034223\n",
      "train loss:0.04419431758872206\n",
      "train loss:0.008988320999504536\n",
      "train loss:0.005874519229350134\n",
      "train loss:0.01134031168015108\n",
      "train loss:0.0026560923028475358\n",
      "train loss:0.02593274970367952\n",
      "train loss:0.008153347734682485\n",
      "train loss:0.004871428578292581\n",
      "train loss:0.016192532083351262\n",
      "train loss:0.0019038966495822638\n",
      "train loss:0.0017380185533820902\n",
      "train loss:0.00246085197460133\n",
      "train loss:0.003962978556108699\n",
      "train loss:0.004347746718779004\n",
      "train loss:0.004062332518529296\n",
      "train loss:0.003438192027391066\n",
      "train loss:0.000833539894269235\n",
      "train loss:0.005374045490959235\n",
      "train loss:0.0033355143966389833\n",
      "train loss:0.009139294521053894\n",
      "train loss:0.002631963706039035\n",
      "train loss:0.0010729336964146515\n",
      "train loss:0.003191582859891612\n",
      "train loss:0.00255542244863967\n",
      "train loss:0.009303284750843303\n",
      "train loss:0.0224059988954539\n",
      "train loss:0.01605349512826622\n",
      "train loss:0.01270817207452181\n",
      "train loss:0.005884823544422529\n",
      "train loss:0.012786193745738363\n",
      "train loss:0.01548541552055514\n",
      "train loss:0.004433981535274582\n",
      "train loss:0.002510794838545818\n",
      "train loss:0.0011976856716243744\n",
      "train loss:0.007805837821581884\n",
      "train loss:0.004738223306370974\n",
      "train loss:0.0034394074697178824\n",
      "train loss:0.004879084074936622\n",
      "train loss:0.006873067073238712\n",
      "train loss:0.0077353671757829125\n",
      "train loss:0.008457021480163994\n",
      "train loss:0.01829344927656531\n",
      "train loss:0.02864514267996315\n",
      "train loss:0.02031157203746091\n",
      "train loss:0.001542778736494244\n",
      "train loss:0.0017809630943470746\n",
      "train loss:0.0034805436036861497\n",
      "train loss:0.0007934149940876889\n",
      "train loss:0.0064289521052055985\n",
      "train loss:0.00024970136138348587\n",
      "train loss:0.00364149001072084\n",
      "train loss:0.024568877760676545\n",
      "train loss:0.002213173136217296\n",
      "train loss:0.002662173044840364\n",
      "train loss:0.004136690552691367\n",
      "train loss:0.0119825206960108\n",
      "train loss:0.005139415073721645\n",
      "train loss:0.04942402644224297\n",
      "train loss:0.0012571099757956684\n",
      "train loss:0.002071833633172636\n",
      "train loss:0.006959297374564612\n",
      "train loss:0.009012549495026018\n",
      "train loss:0.00482971144350475\n",
      "train loss:0.004003420643400326\n",
      "train loss:0.0095213314705562\n",
      "train loss:0.008495804283411571\n",
      "train loss:0.015802594368146086\n",
      "train loss:0.015791674095032415\n",
      "train loss:0.014945454998240437\n",
      "train loss:0.006367381861145294\n",
      "train loss:0.006254019871871336\n",
      "train loss:0.017186869880432627\n",
      "train loss:0.012675899711699365\n",
      "train loss:0.004064803250467982\n",
      "train loss:0.0019327972631712142\n",
      "train loss:0.0012866967570059782\n",
      "train loss:0.0053619923843299475\n",
      "train loss:0.0024145641190101764\n",
      "train loss:0.008771735083992135\n",
      "train loss:0.003557190358757139\n",
      "train loss:0.024492725719293163\n",
      "train loss:0.007871201900970917\n",
      "train loss:0.013332137418541746\n",
      "train loss:0.003588651991196308\n",
      "train loss:0.006869885895685008\n",
      "train loss:0.011766833847349388\n",
      "train loss:0.0015531288708996267\n",
      "train loss:0.006216010140732137\n",
      "train loss:0.0033708426040004063\n",
      "train loss:0.005556659643526859\n",
      "train loss:0.03802038158670476\n",
      "train loss:0.0038166083851945303\n",
      "train loss:0.0022142205688272416\n",
      "train loss:0.012411065274806848\n",
      "train loss:0.0005461303132395243\n",
      "train loss:0.0025137603523781877\n",
      "train loss:0.002770304539224621\n",
      "train loss:0.0007481460495413899\n",
      "train loss:0.002941942906975454\n",
      "train loss:0.0073510108741131185\n",
      "train loss:0.0025644024603702848\n",
      "train loss:0.009863576028772567\n",
      "train loss:0.0005069721943070275\n",
      "train loss:0.003913243446402018\n",
      "train loss:0.005964421360077432\n",
      "train loss:0.0030942943298612365\n",
      "train loss:0.013675602306424674\n",
      "train loss:0.00702852371632718\n",
      "train loss:0.007209699971570373\n",
      "train loss:0.0317687982830967\n",
      "train loss:0.010133589446255427\n",
      "train loss:0.002235212922715179\n",
      "train loss:0.009262655446001033\n",
      "train loss:0.0027135481887594908\n",
      "train loss:0.008879708191381981\n",
      "train loss:0.014925659889232607\n",
      "train loss:0.0017550617778005545\n",
      "train loss:0.0005062235089817613\n",
      "train loss:0.016672678795291378\n",
      "train loss:0.010577956054226343\n",
      "train loss:0.0037310798096640897\n",
      "train loss:0.003477314956770418\n",
      "train loss:0.025523558122760616\n",
      "train loss:0.0014589510272262887\n",
      "train loss:0.0107572757108384\n",
      "train loss:0.007096237494326867\n",
      "train loss:0.001660258839457724\n",
      "train loss:0.0011012967933339735\n",
      "train loss:0.0007130850254090884\n",
      "train loss:0.005824352209016876\n",
      "train loss:0.000693294496421911\n",
      "train loss:0.008134437011379338\n",
      "train loss:0.028032067508991457\n",
      "train loss:0.00643512633418273\n",
      "train loss:0.0053693476762485415\n",
      "train loss:0.005435752818341812\n",
      "train loss:0.003995142561983016\n",
      "train loss:0.001018724725771755\n",
      "train loss:0.019871190892957108\n",
      "train loss:0.004120526338637278\n",
      "train loss:0.002397605170481934\n",
      "train loss:0.00760376432597673\n",
      "train loss:0.0010729734866152503\n",
      "train loss:0.004774517322621658\n",
      "train loss:0.005684433652016751\n",
      "train loss:0.003331456152490982\n",
      "train loss:0.000993876345049073\n",
      "train loss:0.0006378822730958028\n",
      "train loss:0.0013787298679856044\n",
      "train loss:0.001540822918198027\n",
      "train loss:0.005284254404247417\n",
      "train loss:0.0009613769364148158\n",
      "train loss:0.0012969465031822311\n",
      "train loss:0.002813731063871057\n",
      "train loss:0.0007126694617837685\n",
      "train loss:0.00013577937378210828\n",
      "train loss:0.0018997814311791643\n",
      "train loss:0.005977085155728609\n",
      "train loss:0.005887948775235319\n",
      "train loss:0.004496981550750641\n",
      "train loss:0.010442269688229614\n",
      "train loss:0.015530759505096123\n",
      "train loss:0.001406912609978205\n",
      "train loss:0.004664856417668163\n",
      "train loss:0.002917406382417906\n",
      "train loss:0.010574561852171767\n",
      "train loss:0.010828470686866048\n",
      "train loss:0.0019516985363841137\n",
      "train loss:0.002083852295297752\n",
      "train loss:0.0028223002355207503\n",
      "train loss:0.013372303159801525\n",
      "train loss:0.0009517030936593638\n",
      "train loss:0.0064586227722469355\n",
      "train loss:0.0009856663266639496\n",
      "train loss:0.0007385588193830928\n",
      "train loss:0.013343060092826238\n",
      "train loss:0.002739374052059589\n",
      "train loss:0.0027356640955359178\n",
      "train loss:0.0042945109758899345\n",
      "train loss:0.004424296076780082\n",
      "train loss:0.00105816474768421\n",
      "train loss:0.004105873257819466\n",
      "train loss:0.0026266008610709587\n",
      "train loss:0.0022179326632898264\n",
      "train loss:0.003906610372430447\n",
      "train loss:0.0015467238675256107\n",
      "train loss:0.009096778064897898\n",
      "train loss:0.0008886366050133869\n",
      "train loss:0.019638436542548193\n",
      "train loss:0.0022363584384046085\n",
      "train loss:0.0007534176963369845\n",
      "train loss:0.0036757442900534456\n",
      "train loss:0.0056786353741954156\n",
      "train loss:0.0018712206334848\n",
      "train loss:0.0025856702856236436\n",
      "train loss:0.0028005057184600883\n",
      "train loss:0.011363069493136731\n",
      "train loss:0.0023955163277283574\n",
      "train loss:0.0051147542582582065\n",
      "train loss:0.0010905051742761108\n",
      "train loss:0.006231593231643754\n",
      "train loss:0.007906420317103112\n",
      "train loss:0.02706341808303954\n",
      "train loss:0.0011580576149746707\n",
      "train loss:0.04631068953460657\n",
      "train loss:0.0020625855161814817\n",
      "train loss:0.002080214811669529\n",
      "train loss:0.0013825561301763912\n",
      "train loss:0.00333277508648846\n",
      "train loss:0.0018163313749687423\n",
      "train loss:0.011904779076532706\n",
      "train loss:0.0011584158075507542\n",
      "train loss:0.011165202045320826\n",
      "train loss:0.0008398366739184211\n",
      "train loss:0.01049297587876825\n",
      "train loss:0.00034541751705731275\n",
      "train loss:0.0019429327650319268\n",
      "train loss:0.020240074415421427\n",
      "train loss:0.0025942868235526227\n",
      "train loss:0.004782958738678847\n",
      "train loss:0.0034466149287190294\n",
      "train loss:0.0016450420790810039\n",
      "train loss:0.001088216549947621\n",
      "train loss:0.008828519762817255\n",
      "train loss:0.02147726583335894\n",
      "train loss:0.005735680717101839\n",
      "train loss:0.005917343870254335\n",
      "train loss:0.009246590213991001\n",
      "train loss:0.006460134522167565\n",
      "train loss:0.0010571975292885533\n",
      "train loss:0.00797298702112073\n",
      "train loss:0.0018375015960063238\n",
      "train loss:0.006541293801212658\n",
      "train loss:0.003616732326865928\n",
      "train loss:0.0025515538684625413\n",
      "train loss:0.02567033400542026\n",
      "train loss:0.026778745341738932\n",
      "train loss:0.06392186379058958\n",
      "train loss:0.008079829622980825\n",
      "train loss:0.04320449535153607\n",
      "train loss:0.005402026858517539\n",
      "train loss:0.0016478569325082918\n",
      "train loss:0.0022121749582671595\n",
      "train loss:0.01146858335064163\n",
      "train loss:0.0036923002212612137\n",
      "train loss:0.0023151362521275534\n",
      "train loss:0.0019911307614547086\n",
      "train loss:0.0010125134638007223\n",
      "train loss:0.0012779674825912992\n",
      "train loss:0.004076999021930237\n",
      "train loss:0.00234262521467934\n",
      "train loss:0.00709770068955997\n",
      "train loss:0.003806356984003766\n",
      "train loss:0.006802479026438289\n",
      "train loss:0.003402985499695721\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9863\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "\n",
    "# 커널 개수: 30, 커널 사이즈: 5x5, 커널 스트라이드: 1 \n",
    "# FC-Layer 노드 개수: 100\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "         \n",
    "# 에폭 수: 10, 미니배치 사이즈: 100, 최적화기법: Adam(Ir: 0.001) \n",
    "# 매 에폭 시 평가에 사용되는 샘플 수: 1000              \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "# trainer 클래스의 train 함수 호출\n",
    "# train 함수는 epochs만큼 반복하면서 미니배치 학습을 수행한다.\n",
    "# 매 에폭마다 평가 데이터를 사용하여 정확도를 측정\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# test 데이터 정확도 리스트 저장\n",
    "simple_conv_net_result = trainer.test_acc_list\n",
    "\n",
    "# # 그래프 그리기\n",
    "# markers = {'train': 'o', 'test': 's'}\n",
    "# x = np.arange(max_epochs)\n",
    "# # 학습 데이터 정확도 결과 \n",
    "# plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "# # 평가 데이터 정확도 결과 \n",
    "# plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "# plt.xlabel(\"epochs\")\n",
    "# plt.ylabel(\"accuracy\")\n",
    "# plt.ylim(0, 1.0)\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet:\n",
    "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\n",
    "\n",
    "    네트워크 구성은 아래와 같음\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 # Convolution Layer\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1}, # 커널 개수: 16, 커널 사이즈: 3x3, 커널 스트라이드: 1\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1}, # 커널 개수: 16, 커널 사이즈: 3x3, 커널 스트라이드: 1\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1}, # 커널 개수: 32, 커널 사이즈: 3x3, 커널 스트라이드: 1\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1}, # 커널 개수: 32, 커널 사이즈: 3x3, 커널 스트라이드: 1\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1}, # 커널 개수: 64, 커널 사이즈: 3x3, 커널 스트라이드: 1\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1}, # 커널 개수: 64, 커널 사이즈: 3x3, 커널 스트라이드: 1\n",
    "                 hidden_size=50, output_size=10):                                        # FC-Layer 노드 개수: 50\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        # Conv-Relu\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        # Conv-Relu-Pooling\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        # Conv-Relu\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        # Conv-Relu-Pooling\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        # Conv-Relu\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        \n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5)) # Dropout 50%\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5)) # Dropout 50%\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2640569649142885\n",
      "=== epoch:1, train acc:0.084, test acc:0.088 ===\n",
      "train loss:2.304892568350864\n",
      "train loss:2.2857807896456834\n",
      "train loss:2.270851313554969\n",
      "train loss:2.2769513760058984\n",
      "train loss:2.284779451402149\n",
      "train loss:2.290203937858655\n",
      "train loss:2.296548057866648\n",
      "train loss:2.2959342613762543\n",
      "train loss:2.2490837961003103\n",
      "train loss:2.2732773960255885\n",
      "train loss:2.250910774062492\n",
      "train loss:2.220450276751299\n",
      "train loss:2.244941881438421\n",
      "train loss:2.183869996005701\n",
      "train loss:2.2306720330927696\n",
      "train loss:2.206342129746011\n",
      "train loss:2.1877206579006967\n",
      "train loss:2.137342794134327\n",
      "train loss:2.1702146085571012\n",
      "train loss:2.118814613551667\n",
      "train loss:2.157619832659704\n",
      "train loss:2.0605234331142204\n",
      "train loss:2.148711398376527\n",
      "train loss:2.0772220215001935\n",
      "train loss:1.937778330912698\n",
      "train loss:2.032039247027345\n",
      "train loss:2.0028685444481447\n",
      "train loss:2.1813196076961328\n",
      "train loss:2.04824608915208\n",
      "train loss:2.0375512512826606\n",
      "train loss:1.9344359403020783\n",
      "train loss:2.007582926303941\n",
      "train loss:1.9495666027885408\n",
      "train loss:1.8984192870630283\n",
      "train loss:1.9147178983562105\n",
      "train loss:2.0130225209519534\n",
      "train loss:1.9720154817433126\n",
      "train loss:2.047490040516047\n",
      "train loss:1.8689179989773392\n",
      "train loss:1.9593363134384256\n",
      "train loss:1.9448166949423222\n",
      "train loss:2.123871993508547\n",
      "train loss:1.8224771666112045\n",
      "train loss:1.9145548604893798\n",
      "train loss:1.8488093514834958\n",
      "train loss:1.7535951830926602\n",
      "train loss:1.9078350515484421\n",
      "train loss:1.8241871369405143\n",
      "train loss:1.8753442620908887\n",
      "train loss:1.9554096692535337\n",
      "train loss:1.8832609218292746\n",
      "train loss:1.950749756575383\n",
      "train loss:1.8347797796958494\n",
      "train loss:1.937419910031161\n",
      "train loss:1.7350257018093855\n",
      "train loss:1.9419117184307764\n",
      "train loss:1.7955446877618781\n",
      "train loss:1.8039667225043698\n",
      "train loss:1.912182876157855\n",
      "train loss:1.8678914152372383\n",
      "train loss:2.0327668298218207\n",
      "train loss:1.5790180328225447\n",
      "train loss:1.8346442424314469\n",
      "train loss:1.656382629511556\n",
      "train loss:1.663368517196732\n",
      "train loss:1.6872011700816012\n",
      "train loss:1.6208035423296645\n",
      "train loss:1.7454844712635191\n",
      "train loss:1.631669018675247\n",
      "train loss:1.764132159030536\n",
      "train loss:1.6094916745899233\n",
      "train loss:1.7759261525914452\n",
      "train loss:1.7277097590693793\n",
      "train loss:1.8444261555118464\n",
      "train loss:1.46510561054493\n",
      "train loss:1.7270556679911875\n",
      "train loss:1.770898261750649\n",
      "train loss:1.8475974866711737\n",
      "train loss:1.8513090269827646\n",
      "train loss:1.7229827318841782\n",
      "train loss:1.5897022530872533\n",
      "train loss:1.729059310340156\n",
      "train loss:1.7606895231662796\n",
      "train loss:1.603888426239785\n",
      "train loss:1.641759135440837\n",
      "train loss:1.74868362687372\n",
      "train loss:1.7103024674876506\n",
      "train loss:1.7304112011683075\n",
      "train loss:1.711212653342988\n",
      "train loss:1.8417413914244976\n",
      "train loss:1.702896060570814\n",
      "train loss:1.6736018400744923\n",
      "train loss:1.658429296672935\n",
      "train loss:1.5568453755330305\n",
      "train loss:1.651042416951218\n",
      "train loss:1.7580425938379387\n",
      "train loss:1.856260307851582\n",
      "train loss:1.7474397825072585\n",
      "train loss:1.5175618334648764\n",
      "train loss:1.6397976603050168\n",
      "train loss:1.6440633813725798\n",
      "train loss:1.727849515870113\n",
      "train loss:1.5953091711985885\n",
      "train loss:1.619577398140441\n",
      "train loss:1.5593920352507111\n",
      "train loss:1.5726224482282332\n",
      "train loss:1.7610719966241228\n",
      "train loss:1.6125143547457303\n",
      "train loss:1.6407313397709489\n",
      "train loss:1.6411673588927107\n",
      "train loss:1.4392947776393956\n",
      "train loss:1.6408782318641588\n",
      "train loss:1.581667330810706\n",
      "train loss:1.6430382034587374\n",
      "train loss:1.5430023844984067\n",
      "train loss:1.4177376492103806\n",
      "train loss:1.489127579742299\n",
      "train loss:1.7205290631108938\n",
      "train loss:1.4789936361948341\n",
      "train loss:1.534412769706463\n",
      "train loss:1.4760229098953852\n",
      "train loss:1.4505244819120102\n",
      "train loss:1.6054756577273057\n",
      "train loss:1.6466307668808644\n",
      "train loss:1.8210939338273675\n",
      "train loss:1.561756133686375\n",
      "train loss:1.5596714586243439\n",
      "train loss:1.5080708852643963\n",
      "train loss:1.4294811908417906\n",
      "train loss:1.5828346342027724\n",
      "train loss:1.5635275464366893\n",
      "train loss:1.597212850991792\n",
      "train loss:1.4870742073565295\n",
      "train loss:1.507321667492129\n",
      "train loss:1.5111998774214643\n",
      "train loss:1.6145564415918303\n",
      "train loss:1.7191333482870919\n",
      "train loss:1.4777391016210029\n",
      "train loss:1.5582693855712142\n",
      "train loss:1.4920550033705866\n",
      "train loss:1.5457354515180795\n",
      "train loss:1.4784441261749126\n",
      "train loss:1.6113026155516823\n",
      "train loss:1.5899411503415772\n",
      "train loss:1.253911014548795\n",
      "train loss:1.5137223895920935\n",
      "train loss:1.6373048187996775\n",
      "train loss:1.5789323288326316\n",
      "train loss:1.5609443947364954\n",
      "train loss:1.5728418619144409\n",
      "train loss:1.500191443563037\n",
      "train loss:1.4633507459294373\n",
      "train loss:1.4413913055553258\n",
      "train loss:1.4800415960079634\n",
      "train loss:1.4778514237812947\n",
      "train loss:1.4125753263867573\n",
      "train loss:1.5567104879353104\n",
      "train loss:1.649624604231436\n",
      "train loss:1.4884572675071641\n",
      "train loss:1.5536360713093602\n",
      "train loss:1.370657184704008\n",
      "train loss:1.5331209319635704\n",
      "train loss:1.7121960309085347\n",
      "train loss:1.3706255457514305\n",
      "train loss:1.3497043860114497\n",
      "train loss:1.672497063354122\n",
      "train loss:1.4557481199107236\n",
      "train loss:1.5808065165056808\n",
      "train loss:1.313812074598754\n",
      "train loss:1.595665494941764\n",
      "train loss:1.5431291714683981\n",
      "train loss:1.6046667789272155\n",
      "train loss:1.4344906674471625\n",
      "train loss:1.357315325990683\n",
      "train loss:1.508376668365797\n",
      "train loss:1.3029763790818163\n",
      "train loss:1.5811270865655371\n",
      "train loss:1.5186283669267024\n",
      "train loss:1.397233985975808\n",
      "train loss:1.504829504089989\n",
      "train loss:1.5216663574069114\n",
      "train loss:1.5332491754075135\n",
      "train loss:1.4401298136313005\n",
      "train loss:1.4140084857085367\n",
      "train loss:1.22460604130004\n",
      "train loss:1.4907912339442373\n",
      "train loss:1.3195595952507773\n",
      "train loss:1.451627472710445\n",
      "train loss:1.2700871205742608\n",
      "train loss:1.4688042270396873\n",
      "train loss:1.4993656887453057\n",
      "train loss:1.3674231340351077\n",
      "train loss:1.3637267642796282\n",
      "train loss:1.4104844387259985\n",
      "train loss:1.2814554670346694\n",
      "train loss:1.2602790204762888\n",
      "train loss:1.4065830943910955\n",
      "train loss:1.3783421677048515\n",
      "train loss:1.403495427788502\n",
      "train loss:1.4000384636667298\n",
      "train loss:1.2358966417943393\n",
      "train loss:1.4542991110576273\n",
      "train loss:1.344195371719283\n",
      "train loss:1.4785880027276506\n",
      "train loss:1.5069586962083914\n",
      "train loss:1.2913233083848623\n",
      "train loss:1.4304642043507536\n",
      "train loss:1.3618831644513043\n",
      "train loss:1.4819790490672522\n",
      "train loss:1.3722862601642964\n",
      "train loss:1.3567992912185824\n",
      "train loss:1.4455389448060254\n",
      "train loss:1.4062384815003555\n",
      "train loss:1.4782768776785866\n",
      "train loss:1.3145793440065874\n",
      "train loss:1.4387397505524666\n",
      "train loss:1.1236830885375724\n",
      "train loss:1.4372306177633294\n",
      "train loss:1.1659320632856784\n",
      "train loss:1.5124279092849409\n",
      "train loss:1.4424330823373817\n",
      "train loss:1.2684596595042605\n",
      "train loss:1.4819457796382516\n",
      "train loss:1.2957481369149877\n",
      "train loss:1.2998965002203258\n",
      "train loss:1.3552365143180685\n",
      "train loss:1.2355407003008043\n",
      "train loss:1.5329677424343522\n",
      "train loss:1.3248999835354285\n",
      "train loss:1.503296967095352\n",
      "train loss:1.3073729528518012\n",
      "train loss:1.3783055498444354\n",
      "train loss:1.468674069545723\n",
      "train loss:1.4080803894586456\n",
      "train loss:1.4511349638570723\n",
      "train loss:1.3914542005887858\n",
      "train loss:1.1440268273180145\n",
      "train loss:1.437951751403393\n",
      "train loss:1.2307635014438858\n",
      "train loss:1.4349216050401505\n",
      "train loss:1.405640718495927\n",
      "train loss:1.1896179043906985\n",
      "train loss:1.2105974617190494\n",
      "train loss:1.2906911854172347\n",
      "train loss:1.3500767966665665\n",
      "train loss:1.4625703607000249\n",
      "train loss:1.3246356262802428\n",
      "train loss:1.2304551644633677\n",
      "train loss:1.3872790459842432\n",
      "train loss:1.4110425785494831\n",
      "train loss:1.4805895097111963\n",
      "train loss:1.4464036024080213\n",
      "train loss:1.345042239513087\n",
      "train loss:1.318514159696602\n",
      "train loss:1.4507735887940962\n",
      "train loss:1.396692279629756\n",
      "train loss:1.3637753090360052\n",
      "train loss:1.4329271545040008\n",
      "train loss:1.2340655762009158\n",
      "train loss:1.3357038220706972\n",
      "train loss:1.3814075832001254\n",
      "train loss:1.1913533752619474\n",
      "train loss:1.30137856874222\n",
      "train loss:1.3052909562439754\n",
      "train loss:1.3510006677463469\n",
      "train loss:1.516290672039572\n",
      "train loss:1.2689288406403125\n",
      "train loss:1.304399687692293\n",
      "train loss:1.3313328979388985\n",
      "train loss:1.3416311813781967\n",
      "train loss:1.3833287470848468\n",
      "train loss:1.1460262995445119\n",
      "train loss:1.3253087579461857\n",
      "train loss:1.3754612553060235\n",
      "train loss:1.305355567281748\n",
      "train loss:1.357795602416228\n",
      "train loss:1.4027845888284167\n",
      "train loss:1.2094959393856972\n",
      "train loss:1.0758354800573926\n",
      "train loss:1.3489075847502052\n",
      "train loss:1.2016275951696003\n",
      "train loss:1.2696661237611822\n",
      "train loss:1.1393999339667304\n",
      "train loss:1.2457423844824063\n",
      "train loss:1.2918095883644312\n",
      "train loss:0.9909903596472267\n",
      "train loss:1.2430692839059139\n",
      "train loss:1.1835922874569862\n",
      "train loss:1.3156397689237185\n",
      "train loss:1.3793712475639242\n",
      "train loss:1.172360079963331\n",
      "train loss:1.0930398716083212\n",
      "train loss:1.1007602005502493\n",
      "train loss:1.1779237492114287\n",
      "train loss:1.30891102634152\n",
      "train loss:1.2820561514114552\n",
      "train loss:1.0356959512365134\n",
      "train loss:1.102556991741354\n",
      "train loss:1.2209857876059593\n",
      "train loss:1.3061728247190965\n",
      "train loss:1.2330924856839622\n",
      "train loss:1.050892357818104\n",
      "train loss:1.225482482719763\n",
      "train loss:1.3092868113995582\n",
      "train loss:1.2841708179025002\n",
      "train loss:1.1920142110684742\n",
      "train loss:1.2564164846631056\n",
      "train loss:1.3230960841092732\n",
      "train loss:1.2667412396855735\n",
      "train loss:1.2747320372796749\n",
      "train loss:1.2647237111159833\n",
      "train loss:1.2359405110034705\n",
      "train loss:1.3547915580567664\n",
      "train loss:1.3071589714584624\n",
      "train loss:1.1669578495137132\n",
      "train loss:1.2580131228465612\n",
      "train loss:1.2397032140529227\n",
      "train loss:1.2248135390825086\n",
      "train loss:1.1785197163943955\n",
      "train loss:1.2416816890854634\n",
      "train loss:1.3807569672901618\n",
      "train loss:1.0565790622517133\n",
      "train loss:1.187503821528737\n",
      "train loss:1.203709135175278\n",
      "train loss:0.9598359031363585\n",
      "train loss:1.3009712652973968\n",
      "train loss:1.1961258081239083\n",
      "train loss:1.238965098488198\n",
      "train loss:1.1690292597431806\n",
      "train loss:1.3448957540859439\n",
      "train loss:1.1818965230067118\n",
      "train loss:1.3329054087617194\n",
      "train loss:1.375416059392764\n",
      "train loss:1.3281887633431009\n",
      "train loss:0.9646707502855246\n",
      "train loss:1.1221685872241585\n",
      "train loss:1.1601435660986419\n",
      "train loss:1.216740610822415\n",
      "train loss:1.2700823835092834\n",
      "train loss:1.052637218418075\n",
      "train loss:1.162422135923888\n",
      "train loss:1.0371690430505178\n",
      "train loss:1.096832723644783\n",
      "train loss:1.1717721912893717\n",
      "train loss:1.3206789371613568\n",
      "train loss:1.3123336752739574\n",
      "train loss:1.2864171805069688\n",
      "train loss:1.2403354492844763\n",
      "train loss:1.1772745576822512\n",
      "train loss:1.2003100466517702\n",
      "train loss:1.219148734087579\n",
      "train loss:1.1478498690177457\n",
      "train loss:1.1966034525959421\n",
      "train loss:1.1269182261995732\n",
      "train loss:1.422609661519093\n",
      "train loss:1.1605912389634607\n",
      "train loss:1.2993936588762998\n",
      "train loss:1.2351724051222241\n",
      "train loss:1.138273946554011\n",
      "train loss:1.1384194593523578\n",
      "train loss:1.0179092371215732\n",
      "train loss:1.149662854265321\n",
      "train loss:1.0774239418050107\n",
      "train loss:1.2502637922775197\n",
      "train loss:1.1973288418575505\n",
      "train loss:1.1628934600958232\n",
      "train loss:1.0401294307594346\n",
      "train loss:1.1985368806405354\n",
      "train loss:1.2431354566493482\n",
      "train loss:1.2619022729119136\n",
      "train loss:1.2786005783209002\n",
      "train loss:1.18753608472614\n",
      "train loss:1.2226200460916226\n",
      "train loss:1.326491998773654\n",
      "train loss:1.2278882838933964\n",
      "train loss:1.2257892025780721\n",
      "train loss:1.0529439539680177\n",
      "train loss:1.1060373490464352\n",
      "train loss:1.2401343847790107\n",
      "train loss:1.1263500584417137\n",
      "train loss:1.0143854578180125\n",
      "train loss:1.030991779993344\n",
      "train loss:1.2744346610735209\n",
      "train loss:1.068380275114041\n",
      "train loss:1.2555930194598468\n",
      "train loss:1.0270713147270616\n",
      "train loss:1.1469301379260566\n",
      "train loss:1.3010678334110077\n",
      "train loss:1.0873342393483671\n",
      "train loss:1.1601847089193382\n",
      "train loss:1.40434646938486\n",
      "train loss:1.0936382042638504\n",
      "train loss:0.9949099664227735\n",
      "train loss:1.23615618305379\n",
      "train loss:1.101962569955792\n",
      "train loss:1.2092874463495382\n",
      "train loss:1.3857865047336912\n",
      "train loss:1.1701276880501652\n",
      "train loss:1.2107824302066903\n",
      "train loss:1.4314826243291734\n",
      "train loss:1.5346109913410395\n",
      "train loss:1.1075282366853434\n",
      "train loss:1.1086450137074104\n",
      "train loss:1.1328318738783707\n",
      "train loss:1.1603829756663198\n",
      "train loss:1.2119711328854452\n",
      "train loss:1.2545984275947795\n",
      "train loss:1.1583021029117582\n",
      "train loss:1.2004785958837951\n",
      "train loss:1.1634158893998399\n",
      "train loss:0.9618302485901544\n",
      "train loss:1.0322334826953068\n",
      "train loss:1.051902504726231\n",
      "train loss:1.1716540795160153\n",
      "train loss:1.1097659580995414\n",
      "train loss:1.1554148537299234\n",
      "train loss:1.2259599679385764\n",
      "train loss:1.1478966038537817\n",
      "train loss:1.1984004429504613\n",
      "train loss:1.2593662854203347\n",
      "train loss:0.8947669209806883\n",
      "train loss:1.3504420947769074\n",
      "train loss:1.111766743573737\n",
      "train loss:1.0609979440522108\n",
      "train loss:1.2077224663433241\n",
      "train loss:1.139136016566758\n",
      "train loss:1.2114518940158365\n",
      "train loss:1.210541069965447\n",
      "train loss:1.0387173609351443\n",
      "train loss:0.9139280839040738\n",
      "train loss:1.067276769651019\n",
      "train loss:1.0322933412284339\n",
      "train loss:1.1053874360381837\n",
      "train loss:1.0266432731167878\n",
      "train loss:1.2272273963072582\n",
      "train loss:1.2183715676914568\n",
      "train loss:1.1931079364414479\n",
      "train loss:0.9633123752389281\n",
      "train loss:1.318228006257091\n",
      "train loss:1.1616101876295784\n",
      "train loss:1.0849048105305423\n",
      "train loss:1.1413534315182878\n",
      "train loss:1.2024714870598832\n",
      "train loss:0.9658318349355726\n",
      "train loss:1.0819255386407516\n",
      "train loss:1.0702955529024116\n",
      "train loss:1.2358554446569996\n",
      "train loss:1.1396137765757524\n",
      "train loss:1.024479740839206\n",
      "train loss:0.982823291356549\n",
      "train loss:1.0803821819488892\n",
      "train loss:1.129647661417134\n",
      "train loss:1.18916888121849\n",
      "train loss:1.2786579283423871\n",
      "train loss:1.139195251276043\n",
      "train loss:1.0613788177331254\n",
      "train loss:1.1874403442704051\n",
      "train loss:1.1308864834107946\n",
      "train loss:1.2420397854567438\n",
      "train loss:1.1984302141135297\n",
      "train loss:1.1238072340670981\n",
      "train loss:1.131949471691427\n",
      "train loss:1.1339569469234432\n",
      "train loss:0.9757365853617945\n",
      "train loss:1.1006252921608521\n",
      "train loss:1.3340102439697747\n",
      "train loss:1.2825469806795378\n",
      "train loss:1.0030591767606734\n",
      "train loss:1.2874076882598422\n",
      "train loss:1.2375358269123122\n",
      "train loss:1.1736948310709647\n",
      "train loss:1.1794281352473441\n",
      "train loss:1.200840574251116\n",
      "train loss:1.0733436514259163\n",
      "train loss:1.0806782233568122\n",
      "train loss:1.182766541738517\n",
      "train loss:1.1106077815064086\n",
      "train loss:1.075719235989968\n",
      "train loss:1.1162594047730083\n",
      "train loss:1.002014076677593\n",
      "train loss:1.3012773693898247\n",
      "train loss:1.051259960557569\n",
      "train loss:1.2718502732572956\n",
      "train loss:1.2073454337115044\n",
      "train loss:1.0828818694232634\n",
      "train loss:0.9918856259083886\n",
      "train loss:1.1478134909441728\n",
      "train loss:1.110274310070319\n",
      "train loss:1.0570136859155297\n",
      "train loss:1.2261388147775154\n",
      "train loss:1.2545916183355592\n",
      "train loss:1.1396789471718147\n",
      "train loss:0.9836397532197041\n",
      "train loss:1.033319727153413\n",
      "train loss:1.095137204364522\n",
      "train loss:1.3131555151366603\n",
      "train loss:1.0993284572696047\n",
      "train loss:1.0889878682187595\n",
      "train loss:1.0400367438626115\n",
      "train loss:1.1415097425169791\n",
      "train loss:1.1282486508598208\n",
      "train loss:1.0084952039241135\n",
      "train loss:0.9372802901614178\n",
      "train loss:1.0663600754500864\n",
      "train loss:1.21028222519807\n",
      "train loss:1.362010529781756\n",
      "train loss:1.0601717468610443\n",
      "train loss:1.1380849838441194\n",
      "train loss:1.172052800127233\n",
      "train loss:1.1408173025934005\n",
      "train loss:1.167411916545224\n",
      "train loss:1.1052138497930937\n",
      "train loss:1.229217371929351\n",
      "train loss:1.1840369241386433\n",
      "train loss:1.1684793880569524\n",
      "train loss:1.1486679643780628\n",
      "train loss:1.2605431663745323\n",
      "train loss:1.1951996846610675\n",
      "train loss:1.2440512288921814\n",
      "train loss:0.96516721118695\n",
      "train loss:1.086024830770781\n",
      "train loss:1.0432733264908662\n",
      "train loss:1.3390196289149472\n",
      "train loss:0.9562720784009362\n",
      "train loss:1.1208135496350158\n",
      "train loss:1.0579110722893457\n",
      "train loss:1.1887774385264696\n",
      "train loss:1.1368066962298045\n",
      "train loss:1.2591788097255636\n",
      "train loss:1.0913874829524517\n",
      "train loss:1.109769669758206\n",
      "train loss:1.017721694822012\n",
      "train loss:1.2000861883515608\n",
      "train loss:1.2054426722611147\n",
      "train loss:1.2281787449294697\n",
      "train loss:1.0336201687876816\n",
      "train loss:1.008976182124647\n",
      "train loss:0.9123184615779303\n",
      "train loss:1.092771647927583\n",
      "train loss:1.24054131965004\n",
      "train loss:1.1405227324924494\n",
      "train loss:1.1746450837292466\n",
      "train loss:1.0162305821533821\n",
      "train loss:1.1490379412943184\n",
      "train loss:1.0733971530550683\n",
      "train loss:1.074080754677974\n",
      "train loss:1.076359734076142\n",
      "train loss:1.1711923430269602\n",
      "train loss:1.1690071921304206\n",
      "train loss:1.0864193605966626\n",
      "train loss:1.0100146688720946\n",
      "train loss:1.1736583368429971\n",
      "train loss:1.062879771408713\n",
      "train loss:0.9025976339957237\n",
      "train loss:1.1338720079230051\n",
      "train loss:1.1284875698616292\n",
      "train loss:1.1816475965037265\n",
      "train loss:1.0459017384060767\n",
      "train loss:0.8595043998919979\n",
      "train loss:1.0241360443932725\n",
      "train loss:1.1109549008975403\n",
      "train loss:1.1613920348703566\n",
      "train loss:1.2046168808112332\n",
      "train loss:1.1086305808869028\n",
      "train loss:1.2693338932989788\n",
      "train loss:1.3070482366117355\n",
      "train loss:1.0084729435306974\n",
      "train loss:1.1683159327776145\n",
      "train loss:1.166133974003681\n",
      "train loss:1.1693749789692593\n",
      "train loss:1.2447350674918973\n",
      "train loss:1.2486845973681107\n",
      "train loss:0.9859766966000914\n",
      "train loss:1.0503661646226237\n",
      "train loss:1.1571191733646016\n",
      "train loss:1.1520543945529857\n",
      "train loss:1.1401583940578155\n",
      "train loss:1.1163628678550457\n",
      "train loss:1.0321076445544763\n",
      "train loss:0.9919054095843087\n",
      "train loss:1.021908353047226\n",
      "train loss:1.0966968981298135\n",
      "train loss:1.243805631914479\n",
      "train loss:1.1184152765241182\n",
      "train loss:1.047276369589691\n",
      "train loss:1.1391557719854841\n",
      "train loss:1.2199425535569557\n",
      "train loss:1.1667438281731732\n",
      "train loss:1.0418244635151634\n",
      "train loss:1.1486111387952256\n",
      "train loss:1.0168888967006284\n",
      "train loss:1.0885355488769088\n",
      "train loss:1.2188781567227849\n",
      "train loss:1.0095002456191342\n",
      "train loss:1.181261609085075\n",
      "train loss:1.1843087407396136\n",
      "train loss:1.1918955420704205\n",
      "train loss:0.8733962947800467\n",
      "train loss:1.0129496061983325\n",
      "train loss:1.259904194315156\n",
      "=== epoch:2, train acc:0.977, test acc:0.977 ===\n",
      "train loss:1.0828645334108244\n",
      "train loss:0.926845185727574\n",
      "train loss:1.1839555161166901\n",
      "train loss:1.0013711937338687\n",
      "train loss:1.097200239803431\n",
      "train loss:1.1011161004032295\n",
      "train loss:1.124853740788541\n",
      "train loss:0.9447408041466187\n",
      "train loss:0.9949422584302817\n",
      "train loss:1.2200582951147252\n",
      "train loss:1.3878468235613834\n",
      "train loss:1.1832317461237234\n",
      "train loss:1.0805068618125528\n",
      "train loss:1.1563544534693118\n",
      "train loss:1.0204276620157218\n",
      "train loss:1.0585455167399578\n",
      "train loss:1.15962400687573\n",
      "train loss:1.1156474784090868\n",
      "train loss:1.1023355283554432\n",
      "train loss:0.9789281505461693\n",
      "train loss:1.2847222044377313\n",
      "train loss:1.1617008678269478\n",
      "train loss:1.0628185112856265\n",
      "train loss:0.9404048274161153\n",
      "train loss:1.1282466099309802\n",
      "train loss:1.1097520200669442\n",
      "train loss:1.1759035127997042\n",
      "train loss:1.1058508939003933\n",
      "train loss:0.9104372652652231\n",
      "train loss:1.0188326064275228\n",
      "train loss:0.8836592740452002\n",
      "train loss:1.011313533992796\n",
      "train loss:1.0540253082830977\n",
      "train loss:1.1579760720072485\n",
      "train loss:1.3523921570423152\n",
      "train loss:1.1998770076416339\n",
      "train loss:1.0615355348957811\n",
      "train loss:1.122646323151332\n",
      "train loss:1.1709694765379703\n",
      "train loss:1.3083344094288507\n",
      "train loss:1.1664102739984856\n",
      "train loss:1.0776433235146863\n",
      "train loss:1.0663516861083324\n",
      "train loss:1.0816492680474383\n",
      "train loss:1.0221598373099934\n",
      "train loss:1.1120183844726903\n",
      "train loss:1.157865552716271\n",
      "train loss:1.0791318046123037\n",
      "train loss:1.1485245769851475\n",
      "train loss:1.159080529142799\n",
      "train loss:1.0613047701956724\n",
      "train loss:1.0252180374477116\n",
      "train loss:1.2165115026301399\n",
      "train loss:1.0798678561976198\n",
      "train loss:1.0840030024439309\n",
      "train loss:1.1504808967490698\n",
      "train loss:0.9854739820199419\n",
      "train loss:0.9541237593779955\n",
      "train loss:1.2266609545142666\n",
      "train loss:1.0878637550207508\n",
      "train loss:0.9387574400351708\n",
      "train loss:1.054232589658985\n",
      "train loss:1.0040383786611544\n",
      "train loss:1.122980846995096\n",
      "train loss:1.065752698565265\n",
      "train loss:1.147865527490236\n",
      "train loss:1.119959971541202\n",
      "train loss:1.1608276223942469\n",
      "train loss:1.1708840185688811\n",
      "train loss:1.1613123455625496\n",
      "train loss:0.9337058860831953\n",
      "train loss:0.9099693066431098\n",
      "train loss:1.294500360182505\n",
      "train loss:1.2406618655887047\n",
      "train loss:1.114948216278788\n",
      "train loss:1.2950163894631976\n",
      "train loss:1.0417457041206\n",
      "train loss:1.1287704077697298\n",
      "train loss:1.2577179551847244\n",
      "train loss:1.0589041674412158\n",
      "train loss:1.2352801038382488\n",
      "train loss:0.9738649810152825\n",
      "train loss:1.118809665078474\n",
      "train loss:1.1661312646985451\n",
      "train loss:1.1682422791840477\n",
      "train loss:1.0566623039194658\n",
      "train loss:1.1275650640629609\n",
      "train loss:1.082040193533254\n",
      "train loss:1.0581348744599324\n",
      "train loss:1.1482741597910462\n",
      "train loss:1.0024082896586193\n",
      "train loss:1.0355700269965957\n",
      "train loss:1.1634007568922395\n",
      "train loss:1.1375814381208904\n",
      "train loss:0.985051296405584\n",
      "train loss:1.0673664053828649\n",
      "train loss:1.0767360242236592\n",
      "train loss:1.0022035908528817\n",
      "train loss:1.108013659589836\n",
      "train loss:1.0354354907392769\n",
      "train loss:1.0506913941830192\n",
      "train loss:1.0833694100001068\n",
      "train loss:0.8929159952658291\n",
      "train loss:1.0179518609829066\n",
      "train loss:1.1551729012245868\n",
      "train loss:1.1875396899830677\n",
      "train loss:0.9258236344346912\n",
      "train loss:1.1305010387537069\n",
      "train loss:1.2213748835717337\n",
      "train loss:1.003252271082822\n",
      "train loss:0.9450238461752578\n",
      "train loss:1.0796604148575555\n",
      "train loss:1.015477467531892\n",
      "train loss:1.0433463041936566\n",
      "train loss:1.0598666766680598\n",
      "train loss:0.9960020968111373\n",
      "train loss:1.084648527730984\n",
      "train loss:1.1748696021553078\n",
      "train loss:0.9897565251545289\n",
      "train loss:1.039593930482896\n",
      "train loss:0.9719160921999273\n",
      "train loss:0.8674262507475993\n",
      "train loss:1.0219431214238681\n",
      "train loss:0.8846862842644585\n",
      "train loss:1.0205876626800716\n",
      "train loss:1.1451083288580688\n",
      "train loss:1.0960010574574202\n",
      "train loss:0.9417723578960648\n",
      "train loss:1.2290158431317866\n",
      "train loss:1.164739796982641\n",
      "train loss:1.0972066480519636\n",
      "train loss:1.023285788102117\n",
      "train loss:1.1051742976491332\n",
      "train loss:1.0000184439502615\n",
      "train loss:1.064436645364792\n",
      "train loss:0.9680018167352469\n",
      "train loss:1.0715458481908695\n",
      "train loss:1.2832859868845918\n",
      "train loss:0.977009961216974\n",
      "train loss:0.974670622210251\n",
      "train loss:1.033613257495755\n",
      "train loss:1.2117593703956415\n",
      "train loss:1.0840790959522475\n",
      "train loss:1.1004218941888388\n",
      "train loss:0.9855686015366315\n",
      "train loss:1.1608538638269081\n",
      "train loss:0.8940665377493815\n",
      "train loss:0.9592094102708245\n",
      "train loss:1.062241569554291\n",
      "train loss:1.2365595613714804\n",
      "train loss:1.3277203219835116\n",
      "train loss:0.9703995804443644\n",
      "train loss:1.1302881481422682\n",
      "train loss:1.1316425525137384\n",
      "train loss:1.0099441685172665\n",
      "train loss:1.1001215634418255\n",
      "train loss:0.9726732582860869\n",
      "train loss:1.0663840377260914\n",
      "train loss:0.9563451977157692\n",
      "train loss:1.2167786219323258\n",
      "train loss:0.9685228686295959\n",
      "train loss:1.1676936038320527\n",
      "train loss:1.1826670195548452\n",
      "train loss:0.9819946777598237\n",
      "train loss:1.1492664881177759\n",
      "train loss:1.097804081457704\n",
      "train loss:0.9680665746274044\n",
      "train loss:1.1383292609101907\n",
      "train loss:1.1605855738482378\n",
      "train loss:0.9951610677538169\n",
      "train loss:1.0283247979893149\n",
      "train loss:1.2922450599130706\n",
      "train loss:1.0115158479074429\n",
      "train loss:1.1372091908505861\n",
      "train loss:1.0504091083244689\n",
      "train loss:0.9394564171960833\n",
      "train loss:1.0504670223793473\n",
      "train loss:0.8877692352813397\n",
      "train loss:1.0362334354784986\n",
      "train loss:1.050917939672675\n",
      "train loss:0.9807433808708805\n",
      "train loss:1.117685400635463\n",
      "train loss:0.8666352606485028\n",
      "train loss:1.2023508716543427\n",
      "train loss:1.1237954278415183\n",
      "train loss:1.131753958348564\n",
      "train loss:1.0967672634363121\n",
      "train loss:1.1680419776509852\n",
      "train loss:1.0041504835846065\n",
      "train loss:1.1106485167687596\n",
      "train loss:0.9857760480326033\n",
      "train loss:1.1921498683661826\n",
      "train loss:1.0849293326282354\n",
      "train loss:1.0764103339627065\n",
      "train loss:0.9719151661957772\n",
      "train loss:0.9578891083531229\n",
      "train loss:1.1468397017318854\n",
      "train loss:0.9551051815910304\n",
      "train loss:1.0924024206380125\n",
      "train loss:0.9609771645900744\n",
      "train loss:1.1233189365389507\n",
      "train loss:1.0920123259871517\n",
      "train loss:0.8614048958717951\n",
      "train loss:0.8565345470858285\n",
      "train loss:1.175734136894367\n",
      "train loss:1.1043934657348613\n",
      "train loss:0.917277150317385\n",
      "train loss:1.0611673740275163\n",
      "train loss:1.0150151492570412\n",
      "train loss:0.8833092055646777\n",
      "train loss:1.0921027282353586\n",
      "train loss:1.172146818175269\n",
      "train loss:0.9595098329155287\n",
      "train loss:1.2076262326065157\n",
      "train loss:1.0787279668185348\n",
      "train loss:1.1308412387310174\n",
      "train loss:1.2268778004383392\n",
      "train loss:1.1822167108970225\n",
      "train loss:1.0680420691711638\n",
      "train loss:1.0200477962746648\n",
      "train loss:1.011486422731385\n",
      "train loss:1.1992433603286494\n",
      "train loss:1.0217848720541707\n",
      "train loss:0.8849110826421186\n",
      "train loss:1.1150353244132098\n",
      "train loss:0.9641360607937415\n",
      "train loss:0.9503255589558726\n",
      "train loss:1.01653933768327\n",
      "train loss:1.172548303692019\n",
      "train loss:0.9058667703508597\n",
      "train loss:1.1065821452744284\n",
      "train loss:1.100775944094103\n",
      "train loss:1.1629339462278943\n",
      "train loss:1.131809591527433\n",
      "train loss:0.9936048631879103\n",
      "train loss:1.0280358764899646\n",
      "train loss:1.0127159252997346\n",
      "train loss:1.1061052452868516\n",
      "train loss:1.117532710089606\n",
      "train loss:0.9448263505432615\n",
      "train loss:1.0916554414644333\n",
      "train loss:1.1058652107337887\n",
      "train loss:0.9292309694079343\n",
      "train loss:1.147141890771009\n",
      "train loss:0.8824646399472204\n",
      "train loss:0.9821864377967233\n",
      "train loss:1.090742082907962\n",
      "train loss:0.967890582267134\n",
      "train loss:1.0586041365022885\n",
      "train loss:1.028898354507237\n",
      "train loss:1.0883724580083503\n",
      "train loss:0.8829816468484695\n",
      "train loss:0.9066630821178556\n",
      "train loss:1.0394295211233235\n",
      "train loss:0.8867511857886923\n",
      "train loss:1.2057281739534402\n",
      "train loss:1.1400060986019778\n",
      "train loss:0.9642960290657376\n",
      "train loss:1.0925164764237811\n",
      "train loss:1.0834585991069037\n",
      "train loss:1.1668137217239003\n",
      "train loss:1.0608605871307106\n",
      "train loss:1.0696961451769769\n",
      "train loss:1.1432887178597646\n",
      "train loss:1.2121905278644975\n",
      "train loss:0.9359725813994628\n",
      "train loss:0.8463785444178431\n",
      "train loss:1.129443080171873\n",
      "train loss:0.9949295614772731\n",
      "train loss:1.1391688567379537\n",
      "train loss:1.1243715190199242\n",
      "train loss:1.0561728543959348\n",
      "train loss:1.0280339322063001\n",
      "train loss:1.093627744561889\n",
      "train loss:1.0205916105549664\n",
      "train loss:1.0713941649025989\n",
      "train loss:0.9757522316225515\n",
      "train loss:1.149174402524367\n",
      "train loss:1.0614183510393569\n",
      "train loss:1.1649886081907832\n",
      "train loss:0.90364334829308\n",
      "train loss:0.9377427570316784\n",
      "train loss:1.1218306243148861\n",
      "train loss:0.9919690121389573\n",
      "train loss:0.886713596572373\n",
      "train loss:1.1006614193667112\n",
      "train loss:1.0291643254835123\n",
      "train loss:0.9321272605418268\n",
      "train loss:0.976738301199516\n",
      "train loss:0.9318029388913718\n",
      "train loss:1.2496476804777237\n",
      "train loss:1.2226980597217212\n",
      "train loss:1.2522970628093268\n",
      "train loss:1.0125060570457658\n",
      "train loss:1.1255235073171637\n",
      "train loss:1.0844116901287628\n",
      "train loss:1.1263381453310037\n",
      "train loss:0.9924186947756444\n",
      "train loss:0.9225670146380237\n",
      "train loss:1.0114791146673836\n",
      "train loss:1.118829455670807\n",
      "train loss:0.9105687667534527\n",
      "train loss:1.101638598665767\n",
      "train loss:0.9548180639680269\n",
      "train loss:1.0813488533744846\n",
      "train loss:1.077799980113307\n",
      "train loss:1.0143535331240308\n",
      "train loss:0.9672197640179918\n",
      "train loss:1.3056484688244994\n",
      "train loss:1.114490789678177\n",
      "train loss:1.0814668312752125\n",
      "train loss:1.044739986991037\n",
      "train loss:1.0296365578503661\n",
      "train loss:0.7993508664968724\n",
      "train loss:1.007999641722574\n",
      "train loss:1.0083420342618623\n",
      "train loss:0.9966391438255002\n",
      "train loss:0.8904694906597583\n",
      "train loss:0.9749694823148063\n",
      "train loss:1.137838535772362\n",
      "train loss:1.0746464330994336\n",
      "train loss:1.0678497013438573\n",
      "train loss:1.0461395054978417\n",
      "train loss:1.0124454339796325\n",
      "train loss:1.0883036303289249\n",
      "train loss:1.2533582672098305\n",
      "train loss:1.0753360138781678\n",
      "train loss:1.140897438596012\n",
      "train loss:1.0469078795367377\n",
      "train loss:1.1210354422148654\n",
      "train loss:1.1781256807265001\n",
      "train loss:0.9953764750439027\n",
      "train loss:1.017040913057419\n",
      "train loss:1.262564832812733\n",
      "train loss:1.0046596522720725\n",
      "train loss:0.851165872920967\n",
      "train loss:0.9415966602499549\n",
      "train loss:1.0735589107709018\n",
      "train loss:0.9989533425651608\n",
      "train loss:0.9875470582283161\n",
      "train loss:0.9390843235501137\n",
      "train loss:1.061641112742308\n",
      "train loss:0.9566833239025658\n",
      "train loss:1.1933264230252938\n",
      "train loss:1.0226167101469965\n",
      "train loss:0.9496369976755235\n",
      "train loss:1.0448634949152424\n",
      "train loss:0.9892794825299202\n",
      "train loss:1.0977522265512663\n",
      "train loss:0.8928930835644215\n",
      "train loss:0.8018852567306802\n",
      "train loss:1.0890523573251938\n",
      "train loss:0.9703183597026144\n",
      "train loss:1.1442909021894472\n",
      "train loss:0.9900522445606568\n",
      "train loss:0.9466552665995295\n",
      "train loss:0.8913873446160685\n",
      "train loss:1.090966339293501\n",
      "train loss:1.0512140491431914\n",
      "train loss:1.117081516390123\n",
      "train loss:1.1790757278079191\n",
      "train loss:1.031687155870629\n",
      "train loss:1.0824660570771292\n",
      "train loss:0.9218423803350798\n",
      "train loss:1.0437830982651426\n",
      "train loss:1.2396706443838625\n",
      "train loss:0.9844998870052697\n",
      "train loss:1.0648327913110125\n",
      "train loss:0.9970732813679956\n",
      "train loss:0.8781710680968472\n",
      "train loss:0.956949996923721\n",
      "train loss:1.0240537904152425\n",
      "train loss:0.791040958045174\n",
      "train loss:0.9604412323879865\n",
      "train loss:1.2387034621942814\n",
      "train loss:1.0195684285063968\n",
      "train loss:1.0458640554724068\n",
      "train loss:1.155734329742624\n",
      "train loss:1.0466030682445115\n",
      "train loss:0.9248861594217075\n",
      "train loss:0.8081707187286453\n",
      "train loss:1.019011810615635\n",
      "train loss:1.1670163062304666\n",
      "train loss:0.9496084916147218\n",
      "train loss:1.0604359892689466\n",
      "train loss:0.9993739806190629\n",
      "train loss:0.8789414909939359\n",
      "train loss:0.8079710629558814\n",
      "train loss:1.0969503851287987\n",
      "train loss:1.001545142155369\n",
      "train loss:0.9018259555390911\n",
      "train loss:1.1222007536773473\n",
      "train loss:0.9834725528233563\n",
      "train loss:0.887147590108802\n",
      "train loss:1.1512614172623592\n",
      "train loss:0.9098275164042839\n",
      "train loss:1.1573136177607757\n",
      "train loss:0.9962895549012093\n",
      "train loss:1.1066776053174072\n",
      "train loss:0.9083838799142644\n",
      "train loss:0.8922962606959838\n",
      "train loss:0.9630873908950381\n",
      "train loss:0.9788067577589747\n",
      "train loss:1.0049993659069372\n",
      "train loss:0.9973273642788598\n",
      "train loss:0.9001727671804258\n",
      "train loss:1.0482729603498586\n",
      "train loss:0.9488300279411067\n",
      "train loss:0.9344354380391809\n",
      "train loss:1.2228537926238627\n",
      "train loss:1.0457741548582271\n",
      "train loss:0.9196035090608601\n",
      "train loss:1.1690198701290917\n",
      "train loss:1.0034809670187577\n",
      "train loss:0.8916971058507782\n",
      "train loss:0.8529790315623186\n",
      "train loss:1.088758859772029\n",
      "train loss:0.9509516796529668\n",
      "train loss:0.9114061616456535\n",
      "train loss:0.9915534106861963\n",
      "train loss:1.084678200151673\n",
      "train loss:1.1748767911679885\n",
      "train loss:1.0998606126223773\n",
      "train loss:0.9916350826651166\n",
      "train loss:1.1006135465267626\n",
      "train loss:0.982971059138658\n",
      "train loss:0.9686751479371307\n",
      "train loss:0.8970246963354517\n",
      "train loss:1.0597798412690107\n",
      "train loss:1.0492311733037507\n",
      "train loss:1.0032522846547882\n",
      "train loss:1.131259721645926\n",
      "train loss:1.0134064402218594\n",
      "train loss:1.0994543185184222\n",
      "train loss:1.1315237117560155\n",
      "train loss:1.143217346902488\n",
      "train loss:0.9062405807577466\n",
      "train loss:1.0518813703944792\n",
      "train loss:0.998608828846207\n",
      "train loss:1.1364591446803303\n",
      "train loss:1.0241871399976825\n",
      "train loss:1.0822553798444257\n",
      "train loss:1.2257823460434423\n",
      "train loss:0.9263778119225019\n",
      "train loss:1.0530380495110208\n",
      "train loss:1.2214834556178105\n",
      "train loss:1.0942405477599062\n",
      "train loss:1.0260189441342507\n",
      "train loss:1.1726320308360805\n",
      "train loss:1.112014281936481\n",
      "train loss:1.1788665649885866\n",
      "train loss:1.0211942702586916\n",
      "train loss:1.0052176785148017\n",
      "train loss:1.1133781494546062\n",
      "train loss:1.1057510827356982\n",
      "train loss:1.1856168598067598\n",
      "train loss:1.05683494655283\n",
      "train loss:1.026252402589916\n",
      "train loss:1.0572212523220104\n",
      "train loss:1.05430977961733\n",
      "train loss:0.9936629520911948\n",
      "train loss:1.0159942745748902\n",
      "train loss:1.0221138431606724\n",
      "train loss:0.9695480945921249\n",
      "train loss:0.8507044359656222\n",
      "train loss:1.2188270795145955\n",
      "train loss:0.8756692453781261\n",
      "train loss:1.1846167093510358\n",
      "train loss:0.8628249135513936\n",
      "train loss:0.9202183596533596\n",
      "train loss:0.929049822586072\n",
      "train loss:1.1434103118749064\n",
      "train loss:1.1534175577171721\n",
      "train loss:0.8940660858366671\n",
      "train loss:1.0936075233826492\n",
      "train loss:0.9084115539084491\n",
      "train loss:0.8673203505145889\n",
      "train loss:0.9769516502318027\n",
      "train loss:0.9265523263480172\n",
      "train loss:0.9628603220049397\n",
      "train loss:1.0297860217586476\n",
      "train loss:1.047051213143472\n",
      "train loss:0.9349688294006719\n",
      "train loss:0.7952225413807471\n",
      "train loss:1.1505643285669174\n",
      "train loss:0.9833024199922729\n",
      "train loss:0.951463026105681\n",
      "train loss:1.1275263224505654\n",
      "train loss:0.9136494967875807\n",
      "train loss:0.9259336874376531\n",
      "train loss:0.8318068001652777\n",
      "train loss:1.0006676052160512\n",
      "train loss:1.215124786257029\n",
      "train loss:0.9150219957125424\n",
      "train loss:0.9687039439174139\n",
      "train loss:1.008214766408062\n",
      "train loss:1.0694007630253934\n",
      "train loss:1.1643044690042856\n",
      "train loss:0.8773725418444626\n",
      "train loss:0.9714494388979587\n",
      "train loss:1.0207469657588226\n",
      "train loss:0.9577069724138673\n",
      "train loss:0.9219742571381676\n",
      "train loss:0.9069549815684586\n",
      "train loss:0.8980016423017849\n",
      "train loss:1.1726377742731848\n",
      "train loss:0.9416925541443162\n",
      "train loss:0.8825817014688745\n",
      "train loss:0.9642493680786367\n",
      "train loss:0.9691015877170653\n",
      "train loss:0.8653199420979008\n",
      "train loss:0.8551654928569938\n",
      "train loss:0.9439962567546941\n",
      "train loss:0.9356092803709156\n",
      "train loss:0.8652981757437285\n",
      "train loss:1.0363340818169946\n",
      "train loss:1.14073896178811\n",
      "train loss:0.9528684829970762\n",
      "train loss:0.9547437051324779\n",
      "train loss:0.8867275481959558\n",
      "train loss:1.1603118009141282\n",
      "train loss:1.2717186108116787\n",
      "train loss:0.970510127279962\n",
      "train loss:0.9786514907570791\n",
      "train loss:1.0965557744681662\n",
      "train loss:0.9878902268035376\n",
      "train loss:1.0104061705658323\n",
      "train loss:1.138165321804635\n",
      "train loss:0.7667421696054757\n",
      "train loss:1.0011158480485864\n",
      "train loss:0.9754084948894846\n",
      "train loss:0.9480930265386459\n",
      "train loss:1.0999955268418578\n",
      "train loss:1.0517879796206548\n",
      "train loss:1.0017988642040647\n",
      "train loss:1.0050021622683931\n",
      "train loss:1.0034135812666962\n",
      "train loss:1.048597693860609\n",
      "train loss:1.174420471077646\n",
      "train loss:0.9034945389694741\n",
      "train loss:1.0628292633287089\n",
      "train loss:1.1346165829470491\n",
      "train loss:0.9395890789604213\n",
      "train loss:0.9972904432586557\n",
      "train loss:0.9725106814955956\n",
      "train loss:0.9452731593999512\n",
      "train loss:0.9853981904346928\n",
      "train loss:1.0060323741262018\n",
      "train loss:0.8278706427553147\n",
      "train loss:1.012404778412084\n",
      "train loss:1.0236455560904179\n",
      "train loss:1.0364920545354155\n",
      "train loss:0.8768144766660702\n",
      "train loss:0.9238203677981132\n",
      "train loss:1.0649925713122341\n",
      "train loss:1.0070236097581853\n",
      "train loss:0.9567839241708262\n",
      "train loss:0.9928234874090358\n",
      "train loss:0.9579876550589003\n",
      "train loss:0.9708327682255649\n",
      "train loss:0.92730275148581\n",
      "train loss:1.0089003998692394\n",
      "train loss:1.033542701934044\n",
      "train loss:1.091742229153554\n",
      "train loss:1.0417354390148936\n",
      "train loss:0.9435973880064696\n",
      "train loss:0.9010559629503804\n",
      "train loss:1.016367984267442\n",
      "train loss:1.0098568431070882\n",
      "train loss:0.9741915863839259\n",
      "train loss:0.887578589547007\n",
      "train loss:1.0281648620695347\n",
      "train loss:0.9243388202504552\n",
      "train loss:1.0883433684927533\n",
      "train loss:0.9959224918984999\n",
      "train loss:1.040822945823871\n",
      "train loss:1.0024302894186718\n",
      "train loss:1.1089832692679709\n",
      "train loss:1.0589181115374833\n",
      "train loss:1.1297107890748912\n",
      "train loss:1.0722633139825826\n",
      "train loss:0.9252418835684167\n",
      "train loss:0.9560801786828835\n",
      "train loss:1.0038527801122668\n",
      "train loss:1.0479166786490308\n",
      "train loss:1.0294384832400307\n",
      "train loss:0.9810699720099226\n",
      "train loss:0.8263725949940535\n",
      "train loss:1.0943625859508979\n",
      "train loss:1.0553224922742068\n",
      "train loss:1.0983286470289597\n",
      "train loss:1.0248343754400049\n",
      "train loss:1.1035079603347986\n",
      "train loss:1.101329237174964\n",
      "train loss:0.984501623776547\n",
      "train loss:0.9834368382047204\n",
      "train loss:1.181206148734991\n",
      "train loss:1.2781643908071993\n",
      "train loss:0.9302993954153241\n",
      "train loss:0.9551332056954024\n",
      "=== epoch:3, train acc:0.986, test acc:0.982 ===\n",
      "train loss:0.8734363432545611\n",
      "train loss:1.0091148796264018\n",
      "train loss:1.1244925613932213\n",
      "train loss:0.9422987937314014\n",
      "train loss:1.0346538948762043\n",
      "train loss:0.9401626177045508\n",
      "train loss:0.9728606751264386\n",
      "train loss:1.0960001219487971\n",
      "train loss:0.9500889924640488\n",
      "train loss:0.9145897279218631\n",
      "train loss:1.0414550380965997\n",
      "train loss:0.85611352779717\n",
      "train loss:1.08861382373727\n",
      "train loss:0.9279162217115663\n",
      "train loss:0.8403494191211355\n",
      "train loss:0.8988073611869432\n",
      "train loss:1.0499830041326734\n",
      "train loss:1.2132648700885673\n",
      "train loss:0.9734479984960867\n",
      "train loss:1.2083620462255593\n",
      "train loss:0.8980383454679466\n",
      "train loss:0.9857528667562844\n",
      "train loss:0.9588862404273716\n",
      "train loss:0.9072550832409927\n",
      "train loss:1.0633258417571128\n",
      "train loss:0.9973473934877558\n",
      "train loss:0.9500782901422495\n",
      "train loss:0.7979612628218712\n",
      "train loss:1.103596678391023\n",
      "train loss:0.8721930972141788\n",
      "train loss:0.9809131518295299\n",
      "train loss:1.0329463481734158\n",
      "train loss:0.9197455139453972\n",
      "train loss:0.9718839651058363\n",
      "train loss:1.0524086915067181\n",
      "train loss:0.9045772874189091\n",
      "train loss:0.9432034753403302\n",
      "train loss:0.9478292582449402\n",
      "train loss:0.9776090024449623\n",
      "train loss:0.9494391352926448\n",
      "train loss:1.0035628699380112\n",
      "train loss:1.0137672998929956\n",
      "train loss:1.038903499785887\n",
      "train loss:1.0961759017984365\n",
      "train loss:0.9503850205503883\n",
      "train loss:0.8855564296497852\n",
      "train loss:0.8887180825275484\n",
      "train loss:0.8706304791484901\n",
      "train loss:0.8598283942245382\n",
      "train loss:0.964392617934654\n",
      "train loss:0.7555019762294859\n",
      "train loss:1.0551915304130157\n",
      "train loss:1.1165364902687864\n",
      "train loss:0.8798514340707484\n",
      "train loss:0.9350014562586114\n",
      "train loss:1.1041578921733677\n",
      "train loss:0.8971887481817586\n",
      "train loss:0.9940132587845961\n",
      "train loss:0.8842174752002373\n",
      "train loss:1.1011979814179071\n",
      "train loss:0.8723902930287356\n",
      "train loss:1.0858701362229048\n",
      "train loss:0.9198312931465498\n",
      "train loss:0.9116737538875513\n",
      "train loss:1.1334285616898356\n",
      "train loss:0.8913031874414921\n",
      "train loss:1.0149996544504292\n",
      "train loss:1.045694808688036\n",
      "train loss:1.017696676812019\n",
      "train loss:1.0598893265805147\n",
      "train loss:0.9251283890836821\n",
      "train loss:1.0005080091138583\n",
      "train loss:0.9469241273164939\n",
      "train loss:0.8553166963078743\n",
      "train loss:1.0406595897300601\n",
      "train loss:0.977168596128466\n",
      "train loss:0.9570479237705847\n",
      "train loss:1.0399736229075085\n",
      "train loss:0.928248733676992\n",
      "train loss:0.9156701726779382\n",
      "train loss:0.9478265801423014\n",
      "train loss:1.16845855939009\n",
      "train loss:1.0696534841410255\n",
      "train loss:0.9621455385082911\n",
      "train loss:1.0934739310305062\n",
      "train loss:1.1053199268736633\n",
      "train loss:1.0628257359060536\n",
      "train loss:0.9142522632641024\n",
      "train loss:0.7762875467965858\n",
      "train loss:0.9258964074059765\n",
      "train loss:1.0281261612563348\n",
      "train loss:1.0772642995516706\n",
      "train loss:0.9800190616203562\n",
      "train loss:0.8787159248305804\n",
      "train loss:0.8377042927430604\n",
      "train loss:0.9428707703703639\n",
      "train loss:0.9750184643889247\n",
      "train loss:1.050024674738881\n",
      "train loss:0.9999832806653302\n",
      "train loss:1.0365835358856517\n",
      "train loss:1.0841890020394838\n",
      "train loss:0.8339377203084681\n",
      "train loss:0.9865746919535744\n",
      "train loss:1.0527978315567794\n",
      "train loss:1.0195882175322033\n",
      "train loss:1.0698813084956573\n",
      "train loss:0.8654004851468338\n",
      "train loss:1.0254664390066675\n",
      "train loss:0.9652415988862315\n",
      "train loss:1.0797907404939113\n",
      "train loss:1.084699556578192\n",
      "train loss:1.0414657648823036\n",
      "train loss:1.002596912990319\n",
      "train loss:0.9893002000718493\n",
      "train loss:0.8840782220911\n",
      "train loss:1.0585816594690358\n",
      "train loss:0.9596401039427785\n",
      "train loss:0.9794054030456241\n",
      "train loss:1.0242939035302807\n",
      "train loss:1.1883706999552541\n",
      "train loss:1.0208185638574219\n",
      "train loss:0.9596947961454044\n",
      "train loss:0.9864237183415234\n",
      "train loss:1.125757375251622\n",
      "train loss:0.9560881702584925\n",
      "train loss:1.0956571474525807\n",
      "train loss:1.08894814652986\n",
      "train loss:1.088288288450309\n",
      "train loss:0.9823294612669238\n",
      "train loss:1.0398946360349035\n",
      "train loss:0.8998820372203636\n",
      "train loss:0.9570059422105065\n",
      "train loss:0.8204426600331602\n",
      "train loss:0.9797236475394215\n",
      "train loss:1.059727317297507\n",
      "train loss:1.0581899622542326\n",
      "train loss:0.8571940685280677\n",
      "train loss:0.873689642018966\n",
      "train loss:0.9762544511724625\n",
      "train loss:1.0065867116479255\n",
      "train loss:0.9850294893298803\n",
      "train loss:0.7133123047062131\n",
      "train loss:0.8391173227748183\n",
      "train loss:1.3052564036102066\n",
      "train loss:1.0209442308552028\n",
      "train loss:0.9468787662449192\n",
      "train loss:1.0012507073256094\n",
      "train loss:1.0316878521000257\n",
      "train loss:0.9663816665074542\n",
      "train loss:0.9873671513436427\n",
      "train loss:0.932220512887829\n",
      "train loss:1.090997097033044\n",
      "train loss:1.026309512623993\n",
      "train loss:0.9085817498596369\n",
      "train loss:1.0588890505213693\n",
      "train loss:0.8623498216124703\n",
      "train loss:0.7652758534498268\n",
      "train loss:0.9278678975596353\n",
      "train loss:0.8269139944462509\n",
      "train loss:0.909019082295127\n",
      "train loss:1.0519355499939762\n",
      "train loss:0.9559012614428245\n",
      "train loss:0.9248924094478409\n",
      "train loss:1.200446225689606\n",
      "train loss:0.8200327823213321\n",
      "train loss:0.9863428814117731\n",
      "train loss:1.1748164496980622\n",
      "train loss:0.905356823813758\n",
      "train loss:0.9683319123525942\n",
      "train loss:0.8430152528628393\n",
      "train loss:0.8921054106734553\n",
      "train loss:1.0589776928678372\n",
      "train loss:0.9640415569765785\n",
      "train loss:1.059151369822232\n",
      "train loss:0.79463126781514\n",
      "train loss:0.8689248751567594\n",
      "train loss:1.050502430867586\n",
      "train loss:0.8856852938726213\n",
      "train loss:1.2114279333526259\n",
      "train loss:0.892740216793189\n",
      "train loss:0.9893540029163526\n",
      "train loss:1.0975668449088523\n",
      "train loss:0.9248576960261506\n",
      "train loss:0.9719125955621721\n",
      "train loss:0.9467925999350174\n",
      "train loss:1.0573641634355853\n",
      "train loss:1.1174721494130986\n",
      "train loss:1.0509095379643427\n",
      "train loss:0.8403146363304647\n",
      "train loss:1.0320544023756397\n",
      "train loss:1.1561513708115216\n",
      "train loss:1.0707790265083068\n",
      "train loss:0.8495226069468508\n",
      "train loss:1.0005853771032138\n",
      "train loss:1.0875058967904874\n",
      "train loss:1.0514386566188862\n",
      "train loss:0.9403466524650372\n",
      "train loss:1.0198071517160925\n",
      "train loss:0.9511495065425302\n",
      "train loss:1.0912261832913766\n",
      "train loss:1.0308168369392652\n",
      "train loss:0.8466847987261\n",
      "train loss:1.0337233409012285\n",
      "train loss:0.9517468595076211\n",
      "train loss:0.6372901930592693\n",
      "train loss:1.0105228020159749\n",
      "train loss:0.9502249388554175\n",
      "train loss:1.110648090142334\n",
      "train loss:0.9106701188121614\n",
      "train loss:0.8264243820094568\n",
      "train loss:0.9109338224426661\n",
      "train loss:1.083725142788185\n",
      "train loss:1.0608505587102723\n",
      "train loss:1.0457661739581112\n",
      "train loss:0.9334881522605074\n",
      "train loss:0.9425482819769033\n",
      "train loss:1.0960023700702728\n",
      "train loss:0.9274328447137458\n",
      "train loss:1.0560050577005806\n",
      "train loss:0.9780791294538546\n",
      "train loss:1.011248399361179\n",
      "train loss:0.8864327946409829\n",
      "train loss:1.0370827488676442\n",
      "train loss:1.1806979229412902\n",
      "train loss:1.0073683260762984\n",
      "train loss:1.0389461403787719\n",
      "train loss:1.0280925850138982\n",
      "train loss:0.9205417942434698\n",
      "train loss:0.9977574567739861\n",
      "train loss:1.048045803544106\n",
      "train loss:0.9960737528775303\n",
      "train loss:0.8700524754060267\n",
      "train loss:1.0010885826538294\n",
      "train loss:1.0987396128977136\n",
      "train loss:0.8398095106376128\n",
      "train loss:0.9605184435907068\n",
      "train loss:0.8886849755923246\n",
      "train loss:0.9378506109964617\n",
      "train loss:0.8796983423501454\n",
      "train loss:1.0680758150168552\n",
      "train loss:1.0441277598186294\n",
      "train loss:0.8597612859937052\n",
      "train loss:1.05714310473384\n",
      "train loss:0.8805925555393312\n",
      "train loss:0.8721549537579679\n",
      "train loss:0.9906265582348094\n",
      "train loss:1.0636981367008964\n",
      "train loss:1.0811031083867944\n",
      "train loss:0.9821389707560327\n",
      "train loss:1.0095247052663625\n",
      "train loss:1.0145295498281228\n",
      "train loss:0.9636066864932734\n",
      "train loss:1.0664802127983348\n",
      "train loss:0.9815864383383068\n",
      "train loss:0.9340067150904896\n",
      "train loss:0.8735776951553395\n",
      "train loss:1.052291628049631\n",
      "train loss:0.7684032724610955\n",
      "train loss:1.1777749676177565\n",
      "train loss:0.8274410670253386\n",
      "train loss:1.040323937823196\n",
      "train loss:0.9249200667624551\n",
      "train loss:0.958052541032719\n",
      "train loss:1.1657386206731593\n",
      "train loss:0.9482176659432652\n",
      "train loss:0.9649735639346705\n",
      "train loss:1.0191378262525896\n",
      "train loss:0.8779843719843634\n",
      "train loss:0.9859302546717877\n",
      "train loss:0.9532302669450903\n",
      "train loss:1.0178682709563014\n",
      "train loss:0.8534306163226243\n",
      "train loss:0.9753935927462463\n",
      "train loss:1.0413605024757444\n",
      "train loss:0.8983010295614782\n",
      "train loss:0.9945612662417064\n",
      "train loss:1.0402417317802701\n",
      "train loss:0.8443309047248597\n",
      "train loss:1.1112087109396043\n",
      "train loss:0.9453362102102874\n",
      "train loss:1.186188068494359\n",
      "train loss:1.0942742299604875\n",
      "train loss:1.051012224998102\n",
      "train loss:0.8806129142090665\n",
      "train loss:1.090185152259654\n",
      "train loss:0.9313691499791783\n",
      "train loss:0.9194828583481428\n",
      "train loss:0.9618897103782232\n",
      "train loss:0.9700080271335584\n",
      "train loss:0.9367645444464024\n",
      "train loss:0.9697367131658368\n",
      "train loss:1.0904068521159003\n",
      "train loss:0.8914873965923634\n",
      "train loss:0.9509867591080678\n",
      "train loss:1.045291763175544\n",
      "train loss:1.2070296895275432\n",
      "train loss:0.9038002995158787\n",
      "train loss:0.9625067550223285\n",
      "train loss:0.8652622832056375\n",
      "train loss:1.0060535327370619\n",
      "train loss:0.867096438686625\n",
      "train loss:0.8536080065947199\n",
      "train loss:0.8628861489494113\n",
      "train loss:0.894746604076861\n",
      "train loss:1.1127341108434599\n",
      "train loss:0.9478660803356767\n",
      "train loss:1.1071405377748298\n",
      "train loss:1.0513207873941979\n",
      "train loss:1.0332136630999487\n",
      "train loss:1.0167394837051693\n",
      "train loss:0.9288334640669403\n",
      "train loss:0.8698658983405113\n",
      "train loss:1.0988109093470633\n",
      "train loss:0.920559160103211\n",
      "train loss:1.1006891852834444\n",
      "train loss:1.1437568238610172\n",
      "train loss:0.9585276583238247\n",
      "train loss:0.9135424165167474\n",
      "train loss:1.095757628517883\n",
      "train loss:0.7573382002125547\n",
      "train loss:1.1024601194777504\n",
      "train loss:0.8426046492584469\n",
      "train loss:0.9146856709212129\n",
      "train loss:1.1377490743000929\n",
      "train loss:0.8792152767119249\n",
      "train loss:1.000568784236127\n",
      "train loss:0.9669794109939285\n",
      "train loss:0.9928170946524882\n",
      "train loss:0.9405073871429254\n",
      "train loss:1.0664327531956281\n",
      "train loss:1.0067121132208505\n",
      "train loss:0.9059282708872299\n",
      "train loss:0.9949476459959076\n",
      "train loss:0.9857059777163891\n",
      "train loss:1.1125083439096022\n",
      "train loss:0.8534648409166279\n",
      "train loss:0.9841321281868715\n",
      "train loss:0.9948523766375865\n",
      "train loss:0.9453667714686536\n",
      "train loss:1.0150448489420925\n",
      "train loss:0.9895777676290862\n",
      "train loss:1.0381116368970078\n",
      "train loss:1.1420278711489706\n",
      "train loss:0.7748672230855624\n",
      "train loss:1.0977838273503895\n",
      "train loss:1.0220450835050017\n",
      "train loss:1.0257327935703302\n",
      "train loss:0.8918616077196151\n",
      "train loss:0.9160190753289299\n",
      "train loss:0.8023059969084636\n",
      "train loss:0.9600299866956175\n",
      "train loss:0.9263285715924141\n",
      "train loss:0.9372350596265542\n",
      "train loss:1.0394277845354667\n",
      "train loss:1.1059790568370873\n",
      "train loss:0.8361920805820512\n",
      "train loss:0.7728529682041619\n",
      "train loss:1.1483763490544603\n",
      "train loss:1.2671228173790734\n",
      "train loss:1.1123466220758689\n",
      "train loss:0.8556799815424456\n",
      "train loss:0.9983294359475395\n",
      "train loss:0.97461990867306\n",
      "train loss:0.8779442382666973\n",
      "train loss:1.0434500058654659\n",
      "train loss:0.961932875859612\n",
      "train loss:1.0247644087468109\n",
      "train loss:0.9381176751957012\n",
      "train loss:0.9823765753038327\n",
      "train loss:0.9581286242078216\n",
      "train loss:0.8903311896301355\n",
      "train loss:1.056431734498846\n",
      "train loss:1.032455721869442\n",
      "train loss:0.8352082003625384\n",
      "train loss:1.0516832272674588\n",
      "train loss:0.7699139121144353\n",
      "train loss:0.9071389385146358\n",
      "train loss:0.8765857953787353\n",
      "train loss:1.0625226524306817\n",
      "train loss:1.0380358684022573\n",
      "train loss:0.8744356149369429\n",
      "train loss:1.016093430065055\n",
      "train loss:0.9343584734052764\n",
      "train loss:0.8908092573244175\n",
      "train loss:0.8603479269829548\n",
      "train loss:0.8057648244011445\n",
      "train loss:0.9854548265185116\n",
      "train loss:0.9772382287083229\n",
      "train loss:1.0252997590535577\n",
      "train loss:0.6261921426901917\n",
      "train loss:0.9892355247599053\n",
      "train loss:0.9342105689246458\n",
      "train loss:0.9499891353274776\n",
      "train loss:0.9441407324843872\n",
      "train loss:1.0100897391695915\n",
      "train loss:1.1875456274916232\n",
      "train loss:1.1154070546655068\n",
      "train loss:1.0003620350781575\n",
      "train loss:0.6988925495420771\n",
      "train loss:0.8944335585819556\n",
      "train loss:0.9434968102466879\n",
      "train loss:0.9313463034745695\n",
      "train loss:1.1379191517901628\n",
      "train loss:1.034115753257631\n",
      "train loss:0.9866627718623974\n",
      "train loss:1.2073046843494943\n",
      "train loss:0.8887608189555517\n",
      "train loss:0.8714422304686122\n",
      "train loss:0.85785915590887\n",
      "train loss:0.9559931264107735\n",
      "train loss:1.0547082879818017\n",
      "train loss:0.8144983515666783\n",
      "train loss:0.9737642781432156\n",
      "train loss:1.0680046103961842\n",
      "train loss:0.8006264858719817\n",
      "train loss:1.0500493822023755\n",
      "train loss:0.8967291821984843\n",
      "train loss:0.9958904315095041\n",
      "train loss:0.8496008021190297\n",
      "train loss:1.0450690027233238\n",
      "train loss:0.9225047056639265\n",
      "train loss:0.8867536365321239\n",
      "train loss:1.0454361129108316\n",
      "train loss:0.8283613999284098\n",
      "train loss:1.0448676487628867\n",
      "train loss:0.8859996872488921\n",
      "train loss:0.7405692879820437\n",
      "train loss:0.9088905363115514\n",
      "train loss:0.977232941507893\n",
      "train loss:0.908193050165107\n",
      "train loss:0.9644288521105784\n",
      "train loss:0.9996313434021736\n",
      "train loss:0.9588040924979505\n",
      "train loss:1.0177889765260506\n",
      "train loss:0.9318848541742482\n",
      "train loss:1.0695050000903776\n",
      "train loss:0.8675002211941467\n",
      "train loss:0.8393909871375307\n",
      "train loss:0.9018448181923624\n",
      "train loss:0.9064088987536314\n",
      "train loss:1.0263996172560461\n",
      "train loss:0.7586798633343078\n",
      "train loss:0.9019490621023212\n",
      "train loss:1.0302224340338415\n",
      "train loss:0.952426126985956\n",
      "train loss:0.9024937408009247\n",
      "train loss:0.999056048162293\n",
      "train loss:1.0442203605458227\n",
      "train loss:0.9430035253661654\n",
      "train loss:1.0130849411430816\n",
      "train loss:0.890072320299044\n",
      "train loss:1.0498950539089884\n",
      "train loss:0.8581095326952155\n",
      "train loss:0.7801978968297599\n",
      "train loss:0.9261086043085305\n",
      "train loss:0.9740601319435234\n",
      "train loss:1.1486036689948416\n",
      "train loss:0.8682634313221473\n",
      "train loss:0.954337746574434\n",
      "train loss:0.9543202394264635\n",
      "train loss:0.9811886155376338\n",
      "train loss:0.9495043655482452\n",
      "train loss:0.9670141263384765\n",
      "train loss:0.9084058204906121\n",
      "train loss:0.8882253414005574\n",
      "train loss:0.9116878479365961\n",
      "train loss:0.8352758834077862\n",
      "train loss:0.8670413268475143\n",
      "train loss:1.065288936020817\n",
      "train loss:1.0529167738989225\n",
      "train loss:0.8909415786946078\n",
      "train loss:1.116032481778642\n",
      "train loss:1.121802949715445\n",
      "train loss:0.9733582136374873\n",
      "train loss:1.0638721378158342\n",
      "train loss:0.8940738761952578\n",
      "train loss:0.8812956508598796\n",
      "train loss:1.0611490073698966\n",
      "train loss:0.9134005034718323\n",
      "train loss:0.929335372435351\n",
      "train loss:0.9189963161808157\n",
      "train loss:0.9776226319612684\n",
      "train loss:0.9373383524177317\n",
      "train loss:1.0063313237327765\n",
      "train loss:1.0998881247757089\n",
      "train loss:0.9513421457993898\n",
      "train loss:0.9694431430821541\n",
      "train loss:1.2355440008204077\n",
      "train loss:1.2054645241466535\n",
      "train loss:0.9446090145228112\n",
      "train loss:0.783813583287808\n",
      "train loss:1.0019041179602575\n",
      "train loss:1.143646270931918\n",
      "train loss:0.9699186710968619\n",
      "train loss:1.0310926746473128\n",
      "train loss:0.9120170646621082\n",
      "train loss:0.9745638104034943\n",
      "train loss:0.917860628559369\n",
      "train loss:0.9766285233941585\n",
      "train loss:1.079451420143646\n",
      "train loss:0.9399527722502303\n",
      "train loss:0.9645337374803441\n",
      "train loss:1.0565126171535868\n",
      "train loss:1.0031091898346385\n",
      "train loss:0.9205552435766051\n",
      "train loss:0.9607170988614725\n",
      "train loss:0.9015565845197112\n",
      "train loss:0.9407230331207921\n",
      "train loss:0.869061051913504\n",
      "train loss:1.0322434022678082\n",
      "train loss:0.9792213178561169\n",
      "train loss:0.9651915120213006\n",
      "train loss:1.0742658896998023\n",
      "train loss:0.9388388430875654\n",
      "train loss:0.9124357449670656\n",
      "train loss:1.0103591409986092\n",
      "train loss:0.9754937105848674\n",
      "train loss:0.8919414390157031\n",
      "train loss:1.0794433958401775\n",
      "train loss:0.9323643674782263\n",
      "train loss:1.0951242705011939\n",
      "train loss:1.1171949869734363\n",
      "train loss:1.0553000379378172\n",
      "train loss:0.8774983827516901\n",
      "train loss:1.1218802377787929\n",
      "train loss:0.9905349256141754\n",
      "train loss:1.0238749605927857\n",
      "train loss:0.9935120027537819\n",
      "train loss:0.9501745931013725\n",
      "train loss:0.8748106487799095\n",
      "train loss:0.9679444478459893\n",
      "train loss:0.8350170364607093\n",
      "train loss:0.9660901690052708\n",
      "train loss:0.967217456154026\n",
      "train loss:0.8456125203991104\n",
      "train loss:0.8454015932432264\n",
      "train loss:1.1667320422365222\n",
      "train loss:1.024692281475878\n",
      "train loss:0.929671322466993\n",
      "train loss:0.9584699207914078\n",
      "train loss:0.7254721380654704\n",
      "train loss:0.9710202273806733\n",
      "train loss:1.0019123817806257\n",
      "train loss:0.9722416572493014\n",
      "train loss:0.9662470912933213\n",
      "train loss:0.9713365949070665\n",
      "train loss:1.0086235003912456\n",
      "train loss:0.9155345166592833\n",
      "train loss:0.8217645109292587\n",
      "train loss:0.919447107837754\n",
      "train loss:0.7912967835013266\n",
      "train loss:0.7360433223730649\n",
      "train loss:0.8137959396445906\n",
      "train loss:0.938260550757843\n",
      "train loss:0.9327992699382364\n",
      "train loss:0.9195258887590768\n",
      "train loss:0.8803641738368796\n",
      "train loss:1.071731606901253\n",
      "train loss:0.8928201828467948\n",
      "train loss:0.9774816931350854\n",
      "train loss:0.9010845693762057\n",
      "train loss:0.9452730094777846\n",
      "train loss:1.059902016849608\n",
      "train loss:0.991551897452312\n",
      "train loss:0.8941287896261347\n",
      "train loss:1.036937613321098\n",
      "train loss:0.8821337047676617\n",
      "train loss:0.9697968941237324\n",
      "train loss:0.9113601492428914\n",
      "train loss:0.8505065973831798\n",
      "train loss:1.0220650137556546\n",
      "train loss:0.9307676978833238\n",
      "train loss:0.9922217762147462\n",
      "train loss:0.8428486893844935\n",
      "train loss:1.0536032993819764\n",
      "train loss:0.9348788838450585\n",
      "train loss:0.7897932069330714\n",
      "train loss:0.9507875602913639\n",
      "train loss:0.8090648724198889\n",
      "train loss:0.7686410415849575\n",
      "train loss:0.9630865209997315\n",
      "train loss:0.9463706005251222\n",
      "train loss:0.9512507825256077\n",
      "train loss:1.0563919200054226\n",
      "train loss:1.053345125863034\n",
      "train loss:0.938391963274594\n",
      "train loss:0.93847443919167\n",
      "train loss:0.7999437950563129\n",
      "train loss:1.0466296103371862\n",
      "train loss:0.8375443533637995\n",
      "train loss:1.131874595881342\n",
      "train loss:0.8628856236626089\n",
      "train loss:0.7576286497692817\n",
      "train loss:0.9408387022165954\n",
      "train loss:0.9778688827317725\n",
      "train loss:0.8712651742196337\n",
      "train loss:0.9337984760822171\n",
      "train loss:1.0773679795969986\n",
      "train loss:1.0020584752718469\n",
      "train loss:0.9150334946270675\n",
      "=== epoch:4, train acc:0.988, test acc:0.988 ===\n",
      "train loss:1.008388540435429\n",
      "train loss:0.9942753384632687\n",
      "train loss:0.9739607704967329\n",
      "train loss:0.6049996708705332\n",
      "train loss:1.0574569511637029\n",
      "train loss:0.9549956080284967\n",
      "train loss:0.8470196020447449\n",
      "train loss:0.8513936117086446\n",
      "train loss:0.9679705642185803\n",
      "train loss:0.8469786856728015\n",
      "train loss:1.028036812287438\n",
      "train loss:0.9715489810217797\n",
      "train loss:1.0756928641766281\n",
      "train loss:0.9076263678180808\n",
      "train loss:0.8156274843990959\n",
      "train loss:1.1563082215187255\n",
      "train loss:0.8852078865189842\n",
      "train loss:0.9319258119277555\n",
      "train loss:1.0129367360616848\n",
      "train loss:1.0157157773495458\n",
      "train loss:0.9792233590040094\n",
      "train loss:0.860979029258651\n",
      "train loss:0.8700892675663018\n",
      "train loss:1.0111884507359283\n",
      "train loss:0.9961899643092836\n",
      "train loss:0.9733191574493949\n",
      "train loss:0.7705402772848933\n",
      "train loss:0.9114781906540123\n",
      "train loss:1.07100063207405\n",
      "train loss:0.864682189453235\n",
      "train loss:0.9429711620189403\n",
      "train loss:1.013887618089837\n",
      "train loss:0.960258408939884\n",
      "train loss:1.0594229185073563\n",
      "train loss:1.0325630466124818\n",
      "train loss:0.9903355788255083\n",
      "train loss:1.0057243820789645\n",
      "train loss:0.9921709503450686\n",
      "train loss:0.7063126789079799\n",
      "train loss:0.8800324044773107\n",
      "train loss:0.9896054421180012\n",
      "train loss:1.0584823967077273\n",
      "train loss:1.1155063486811896\n",
      "train loss:1.2239965225605496\n",
      "train loss:0.899652753906244\n",
      "train loss:0.956045528124023\n",
      "train loss:0.9192793025454574\n",
      "train loss:1.0082618743187395\n",
      "train loss:0.8304821942248871\n",
      "train loss:1.006874984040897\n",
      "train loss:1.0078946913749163\n",
      "train loss:1.0113409577960613\n",
      "train loss:0.9574165236260502\n",
      "train loss:0.8385328679139726\n",
      "train loss:0.9354048794466203\n",
      "train loss:0.9694227749373878\n",
      "train loss:0.9965646333834388\n",
      "train loss:0.8447224978158965\n",
      "train loss:0.9821613588904764\n",
      "train loss:0.9715062418144591\n",
      "train loss:0.8704735331132726\n",
      "train loss:1.0537301849152698\n",
      "train loss:0.8487811450743747\n",
      "train loss:1.0044526737444293\n",
      "train loss:0.946916207422974\n",
      "train loss:1.002048276423741\n",
      "train loss:0.8420930369487436\n",
      "train loss:0.9702614213114665\n",
      "train loss:1.1626014009793046\n",
      "train loss:1.01188104437491\n",
      "train loss:1.0082927802198451\n",
      "train loss:0.9167680380561514\n",
      "train loss:0.9384622149598911\n",
      "train loss:0.9038898383872699\n",
      "train loss:0.9126811346945649\n",
      "train loss:0.996708348539931\n",
      "train loss:1.015227439017648\n",
      "train loss:0.9062030361470034\n",
      "train loss:0.8706662509766978\n",
      "train loss:1.0188677439754614\n",
      "train loss:0.9508486563731412\n",
      "train loss:0.9910527330589826\n",
      "train loss:1.0095720839730473\n",
      "train loss:1.0301578312132178\n",
      "train loss:0.9653592815037293\n",
      "train loss:1.1241617038246738\n",
      "train loss:0.830492327081533\n",
      "train loss:0.90734043623515\n",
      "train loss:1.0074481961119564\n",
      "train loss:0.9547131950218076\n",
      "train loss:1.0508016488992813\n",
      "train loss:0.9410185818356112\n",
      "train loss:0.849201139867934\n",
      "train loss:0.9071746444164487\n",
      "train loss:1.1182565590348756\n",
      "train loss:1.0456069230189278\n",
      "train loss:0.8850377645001365\n",
      "train loss:0.9535575438672602\n",
      "train loss:0.8153872175188944\n",
      "train loss:1.0482202985941993\n",
      "train loss:1.0489132070291582\n",
      "train loss:0.9039061695675248\n",
      "train loss:1.0397502654203665\n",
      "train loss:1.1370570532879294\n",
      "train loss:1.0402103427063658\n",
      "train loss:0.868597318731888\n",
      "train loss:0.8442530226616956\n",
      "train loss:0.8214836976079732\n",
      "train loss:0.8725803020439948\n",
      "train loss:1.1192042856303834\n",
      "train loss:0.9794056876320438\n",
      "train loss:0.8915257005660608\n",
      "train loss:0.9691557292296986\n",
      "train loss:0.8958119451416157\n",
      "train loss:1.0014076407549142\n",
      "train loss:1.0458085948248241\n",
      "train loss:0.8256798144133932\n",
      "train loss:0.9727675441659429\n",
      "train loss:0.7503340369822478\n",
      "train loss:0.9481078309846142\n",
      "train loss:1.053583962717096\n",
      "train loss:1.0400162158086328\n",
      "train loss:1.0051885238872487\n",
      "train loss:0.9240773095395434\n",
      "train loss:0.9658838329676809\n",
      "train loss:0.8997175168688731\n",
      "train loss:0.8582063338078025\n",
      "train loss:0.9939716216887672\n",
      "train loss:0.7816426496707432\n",
      "train loss:0.6867326361556387\n",
      "train loss:1.0685879849317355\n",
      "train loss:0.8250935167817471\n",
      "train loss:0.9199700757178623\n",
      "train loss:0.9323157693087949\n",
      "train loss:0.9960663517801475\n",
      "train loss:1.0770662778564362\n",
      "train loss:0.879396996590855\n",
      "train loss:0.9443499722551831\n",
      "train loss:0.810949793951374\n",
      "train loss:0.9160538143422028\n",
      "train loss:1.0660224747542317\n",
      "train loss:1.1254151658584914\n",
      "train loss:0.9479286533495869\n",
      "train loss:0.9666253233190956\n",
      "train loss:1.0646846630289668\n",
      "train loss:0.9102679042892089\n",
      "train loss:0.8950845833004404\n",
      "train loss:0.9225222678338902\n",
      "train loss:1.0092116446157346\n",
      "train loss:0.9032012878143993\n",
      "train loss:1.0494852490757998\n",
      "train loss:1.0860482652488037\n",
      "train loss:0.9645587786415164\n",
      "train loss:1.1132386624416002\n",
      "train loss:1.085981528441495\n",
      "train loss:1.1294953136823347\n",
      "train loss:0.9204260331812971\n",
      "train loss:0.9874747360461058\n",
      "train loss:0.9718878436977262\n",
      "train loss:0.8822072674519551\n",
      "train loss:0.9555180577868575\n",
      "train loss:0.9014128253844423\n",
      "train loss:0.9294971117980633\n",
      "train loss:0.906803670022572\n",
      "train loss:0.9416334545401922\n",
      "train loss:1.0873895080704137\n",
      "train loss:0.8456890131069463\n",
      "train loss:1.1838874629084772\n",
      "train loss:1.057601754477238\n",
      "train loss:0.8382517823840371\n",
      "train loss:1.005071410571147\n",
      "train loss:0.9374586093574462\n",
      "train loss:0.9556509347565314\n",
      "train loss:0.9365887994828277\n",
      "train loss:0.8839305510347226\n",
      "train loss:0.9965600864085112\n",
      "train loss:1.0365248114971308\n",
      "train loss:0.8112721397636355\n",
      "train loss:0.7669225492035123\n",
      "train loss:1.0135959491047055\n",
      "train loss:0.9059809987568274\n",
      "train loss:0.9206142614451264\n",
      "train loss:0.9856833244403242\n",
      "train loss:0.9391851778436004\n",
      "train loss:0.8997960907729545\n",
      "train loss:0.8016946712162881\n",
      "train loss:0.914602014867793\n",
      "train loss:0.9514308808683656\n",
      "train loss:1.0405399932955195\n",
      "train loss:0.8635393039849588\n",
      "train loss:0.8399776018507048\n",
      "train loss:0.9313767266131735\n",
      "train loss:0.8214702284833423\n",
      "train loss:0.8096842444502275\n",
      "train loss:0.9503740296379735\n",
      "train loss:1.1286584355965887\n",
      "train loss:0.7835322818781083\n",
      "train loss:0.8206561862006738\n",
      "train loss:0.9820596891817794\n",
      "train loss:0.8611073358615556\n",
      "train loss:0.9518104015564994\n",
      "train loss:0.9505261533374124\n",
      "train loss:1.0609387483946717\n",
      "train loss:0.9959108119746578\n",
      "train loss:1.0021953232299152\n",
      "train loss:0.9640842339211556\n",
      "train loss:1.0497943157167084\n",
      "train loss:0.8197768989241304\n",
      "train loss:0.8888573880858547\n",
      "train loss:0.9555927285605139\n",
      "train loss:0.9605093000965778\n",
      "train loss:0.9848076721652513\n",
      "train loss:0.8758669414847351\n",
      "train loss:0.9660883654291712\n",
      "train loss:1.1452196952608604\n",
      "train loss:1.0065044024995573\n",
      "train loss:0.9399401635071226\n",
      "train loss:0.8468943778910558\n",
      "train loss:0.8500115408635094\n",
      "train loss:0.8709830345625724\n",
      "train loss:0.9980890273513852\n",
      "train loss:0.9048848128479823\n",
      "train loss:0.85328936757586\n",
      "train loss:1.1197216717802467\n",
      "train loss:0.906515030121435\n",
      "train loss:0.8320657240146464\n",
      "train loss:0.966330468145426\n",
      "train loss:0.9254767945281331\n",
      "train loss:0.9598370834896427\n",
      "train loss:1.0092507541279183\n",
      "train loss:0.870128436682353\n",
      "train loss:0.8971607488648408\n",
      "train loss:0.8834196024727585\n",
      "train loss:1.0079516329277813\n",
      "train loss:1.0271437020268142\n",
      "train loss:0.9614882874351233\n",
      "train loss:0.8979239990307518\n",
      "train loss:0.9682687576399057\n",
      "train loss:1.0754825917369164\n",
      "train loss:0.6623170077600143\n",
      "train loss:1.0803406504320627\n",
      "train loss:1.1096162499926263\n",
      "train loss:0.8864027011427992\n",
      "train loss:0.9399120474346485\n",
      "train loss:0.9681919447168387\n",
      "train loss:1.0939102779553138\n",
      "train loss:0.8658102156103824\n",
      "train loss:0.9646683475710014\n",
      "train loss:0.9273011015515137\n",
      "train loss:1.0136374182642531\n",
      "train loss:0.9188983902108631\n",
      "train loss:0.973728371921375\n",
      "train loss:1.0349772139890812\n",
      "train loss:1.1195723285503085\n",
      "train loss:0.9891481976004991\n",
      "train loss:0.7981727576261758\n",
      "train loss:0.9299002742386115\n",
      "train loss:0.9334146760503472\n",
      "train loss:0.8632874737816368\n",
      "train loss:1.019347866276159\n",
      "train loss:0.8156445524920989\n",
      "train loss:0.9761286999540896\n",
      "train loss:0.8885063554617498\n",
      "train loss:0.9115915517115293\n",
      "train loss:0.9576078947542703\n",
      "train loss:0.8420910359520367\n",
      "train loss:1.0227301921478158\n",
      "train loss:0.9645129667169967\n",
      "train loss:0.7356165542321829\n",
      "train loss:0.9908255780197429\n",
      "train loss:0.8950376925498529\n",
      "train loss:0.7886361773194079\n",
      "train loss:0.8924840140855026\n",
      "train loss:0.9619574462220648\n",
      "train loss:0.9669859107539132\n",
      "train loss:0.9332313059906586\n",
      "train loss:0.9204312335441484\n",
      "train loss:1.0317691588755997\n",
      "train loss:1.1925149246309048\n",
      "train loss:0.9894201321783467\n",
      "train loss:0.8279194282438971\n",
      "train loss:0.8665295886980218\n",
      "train loss:0.8052202670857433\n",
      "train loss:1.0344223371360268\n",
      "train loss:0.9471205207394492\n",
      "train loss:0.9414235846040859\n",
      "train loss:0.9987787681764493\n",
      "train loss:0.9989484418110202\n",
      "train loss:0.887989578245775\n",
      "train loss:0.8256035441054239\n",
      "train loss:1.0068485049023137\n",
      "train loss:0.8328988088427451\n",
      "train loss:0.8750599876190188\n",
      "train loss:0.9260140024581712\n",
      "train loss:1.0774285451260657\n",
      "train loss:0.968620580723092\n",
      "train loss:0.8590844586458443\n",
      "train loss:0.8515057528742602\n",
      "train loss:0.8869628981798698\n",
      "train loss:0.9564599500501498\n",
      "train loss:0.9957060704729866\n",
      "train loss:1.1038793725819966\n",
      "train loss:1.0337666978499325\n",
      "train loss:0.8733966956930906\n",
      "train loss:0.9490607228195539\n",
      "train loss:0.8217967026367884\n",
      "train loss:0.8714400880507907\n",
      "train loss:0.9090774511491345\n",
      "train loss:0.8905278100830573\n",
      "train loss:1.146811398560763\n",
      "train loss:0.9897152231132148\n",
      "train loss:0.8634020189146726\n",
      "train loss:0.9953270342722217\n",
      "train loss:0.9933255575753683\n",
      "train loss:0.914111465694505\n",
      "train loss:1.0250597487222306\n",
      "train loss:0.8916011213349029\n",
      "train loss:1.0050733923513926\n",
      "train loss:0.876799094498091\n",
      "train loss:0.8711112989352402\n",
      "train loss:0.9616598628416959\n",
      "train loss:0.902210729051355\n",
      "train loss:0.9492757239248748\n",
      "train loss:0.9535157525155005\n",
      "train loss:0.9139817098723393\n",
      "train loss:1.0475764374288576\n",
      "train loss:0.8754508591961785\n",
      "train loss:1.100814716812469\n",
      "train loss:0.841687388654155\n",
      "train loss:0.9866769134866296\n",
      "train loss:0.8733838673487301\n",
      "train loss:1.0399956523384135\n",
      "train loss:0.8486160611479038\n",
      "train loss:0.9456650981075317\n",
      "train loss:0.9281889573699561\n",
      "train loss:0.897788164560777\n",
      "train loss:0.8273740326766093\n",
      "train loss:0.8871370128645935\n",
      "train loss:0.8554469844497464\n",
      "train loss:0.9371386141045829\n",
      "train loss:1.0154916837635435\n",
      "train loss:1.195410285684003\n",
      "train loss:0.8072771741967594\n",
      "train loss:0.7683066977939699\n",
      "train loss:0.9559989118410438\n",
      "train loss:1.2200023726641382\n",
      "train loss:0.8264355681533158\n",
      "train loss:0.8921031650410853\n",
      "train loss:1.1356614793907722\n",
      "train loss:0.9232269715615896\n",
      "train loss:0.9469629338415855\n",
      "train loss:1.0431113973930473\n",
      "train loss:0.8724250138311548\n",
      "train loss:0.8459090333071865\n",
      "train loss:1.0147161172710613\n",
      "train loss:0.9185927701051775\n",
      "train loss:1.1330102103861188\n",
      "train loss:0.9671189569148715\n",
      "train loss:0.8064269266810236\n",
      "train loss:0.9949428439471898\n",
      "train loss:0.9769668344568884\n",
      "train loss:0.890836404857889\n",
      "train loss:0.9840557017886045\n",
      "train loss:0.8449018183532669\n",
      "train loss:0.9913397076652845\n",
      "train loss:1.0079027164419956\n",
      "train loss:0.9798718283893494\n",
      "train loss:0.7562141072223437\n",
      "train loss:0.9447188188544524\n",
      "train loss:0.8304726850734424\n",
      "train loss:0.955556626318063\n",
      "train loss:0.83495012494533\n",
      "train loss:1.1265902597897535\n",
      "train loss:1.0160161947974098\n",
      "train loss:0.8494667646547094\n",
      "train loss:1.0546833496835297\n",
      "train loss:0.9782420912340833\n",
      "train loss:1.00822072902271\n",
      "train loss:0.9804825163566042\n",
      "train loss:0.9088089226805885\n",
      "train loss:1.027918225461025\n",
      "train loss:0.8175124789447389\n",
      "train loss:0.9303075672221693\n",
      "train loss:0.9372582620999945\n",
      "train loss:0.8823406874936829\n",
      "train loss:0.9446493978954836\n",
      "train loss:0.9270596662990197\n",
      "train loss:1.0064953824431278\n",
      "train loss:0.9272330985701823\n",
      "train loss:0.9950234808557562\n",
      "train loss:1.0019550684411844\n",
      "train loss:1.1565235207232\n",
      "train loss:0.8142107777847758\n",
      "train loss:0.7652075569090626\n",
      "train loss:0.8666809029606156\n",
      "train loss:0.8661413122509104\n",
      "train loss:0.8554435321926469\n",
      "train loss:0.8523662749165123\n",
      "train loss:0.7922576807090816\n",
      "train loss:0.9293109234492615\n",
      "train loss:0.8868037488848821\n",
      "train loss:0.9387749933506072\n",
      "train loss:0.9985965403094891\n",
      "train loss:0.9729537254591146\n",
      "train loss:0.9630905436789956\n",
      "train loss:0.9900467658390607\n",
      "train loss:0.8537312363131763\n",
      "train loss:0.9872347078268446\n",
      "train loss:1.0146663652606396\n",
      "train loss:0.8929298787137384\n",
      "train loss:0.8644694749943812\n",
      "train loss:0.9074835182283898\n",
      "train loss:0.8267401314670252\n",
      "train loss:0.8011797284459522\n",
      "train loss:0.8451093520211593\n",
      "train loss:0.9286961241185009\n",
      "train loss:1.0125162823223688\n",
      "train loss:0.8420510380929109\n",
      "train loss:0.8781767116114203\n",
      "train loss:1.0539629902025553\n",
      "train loss:0.8695299405557264\n",
      "train loss:0.952766501881653\n",
      "train loss:1.0427734080609945\n",
      "train loss:1.1079891547389513\n",
      "train loss:1.023700242789124\n",
      "train loss:0.8445893576461251\n",
      "train loss:0.9161755844799135\n",
      "train loss:0.9820550503010935\n",
      "train loss:0.8677236922003047\n",
      "train loss:0.9671869566514224\n",
      "train loss:0.9384038400956614\n",
      "train loss:0.9453423801289017\n",
      "train loss:0.9097225106041685\n",
      "train loss:0.9448117845473463\n",
      "train loss:1.0228764280604858\n",
      "train loss:0.8047183696181706\n",
      "train loss:0.7440263222591823\n",
      "train loss:0.858662472004448\n",
      "train loss:0.9254043957250078\n",
      "train loss:0.8792965144960344\n",
      "train loss:0.8919501873376986\n",
      "train loss:0.957797369208466\n",
      "train loss:0.9852397340762231\n",
      "train loss:0.9338893036515986\n",
      "train loss:0.8852239066668508\n",
      "train loss:0.8534314418589601\n",
      "train loss:0.9376623796829583\n",
      "train loss:0.9331019481629407\n",
      "train loss:1.0626514499037034\n",
      "train loss:1.070672363494524\n",
      "train loss:0.8902445621165217\n",
      "train loss:1.1160813811077064\n",
      "train loss:0.7995688210665824\n",
      "train loss:0.9447869579106721\n",
      "train loss:0.9330792164397288\n",
      "train loss:1.0398119781218835\n",
      "train loss:1.1061251942042887\n",
      "train loss:0.9835460906709868\n",
      "train loss:0.8166441943288032\n",
      "train loss:0.8693729609755205\n",
      "train loss:0.8185541540355825\n",
      "train loss:0.9627668113345317\n",
      "train loss:1.0621579209094276\n",
      "train loss:0.827253774089789\n",
      "train loss:0.8028114624586833\n",
      "train loss:0.9065711691840797\n",
      "train loss:0.9496976396271425\n",
      "train loss:0.8757576648046012\n",
      "train loss:1.0756658552136467\n",
      "train loss:1.043522458938967\n",
      "train loss:1.0819465970601343\n",
      "train loss:1.0784563746311262\n",
      "train loss:0.7590041696128468\n",
      "train loss:1.046355822995795\n",
      "train loss:1.131276074884112\n",
      "train loss:0.9232577229899038\n",
      "train loss:1.0279560163095156\n",
      "train loss:0.9247307190632335\n",
      "train loss:0.9547531265940014\n",
      "train loss:0.8203624398948083\n",
      "train loss:0.9621231846627255\n",
      "train loss:1.0493684263157044\n",
      "train loss:0.9863506356826715\n",
      "train loss:0.8111875956235486\n",
      "train loss:0.9236487725852889\n",
      "train loss:1.0364241780395376\n",
      "train loss:0.8504419106947951\n",
      "train loss:0.8633293167542866\n",
      "train loss:0.9332629536346417\n",
      "train loss:0.8256428509939243\n",
      "train loss:1.012346511644095\n",
      "train loss:1.0362139246048705\n",
      "train loss:0.9384737820968738\n",
      "train loss:0.9376628782680224\n",
      "train loss:0.9642535688112589\n",
      "train loss:0.9291184621409969\n",
      "train loss:0.7504707351529296\n",
      "train loss:1.0245245549979516\n",
      "train loss:0.853643409764098\n",
      "train loss:0.8817788622879192\n",
      "train loss:0.8801356270571854\n",
      "train loss:0.9119585913574302\n",
      "train loss:0.8994836748425727\n",
      "train loss:0.8422413058788366\n",
      "train loss:1.12086632932697\n",
      "train loss:0.9358335681205822\n",
      "train loss:0.7835423924096439\n",
      "train loss:0.8547405171502244\n",
      "train loss:0.8558281867749966\n",
      "train loss:0.9264454541279646\n",
      "train loss:0.9623160996993485\n",
      "train loss:1.0524477226883429\n",
      "train loss:0.8732152885236779\n",
      "train loss:0.828335711510928\n",
      "train loss:0.9895669136025611\n",
      "train loss:0.8201729299461049\n",
      "train loss:0.894984727787149\n",
      "train loss:0.9641108164485158\n",
      "train loss:1.0414601609066974\n",
      "train loss:1.011693650493514\n",
      "train loss:1.0516820303757204\n",
      "train loss:0.9039703406830712\n",
      "train loss:1.023510666421478\n",
      "train loss:0.8175512084636922\n",
      "train loss:1.011540959018501\n",
      "train loss:0.8118831153205085\n",
      "train loss:0.9055792598030705\n",
      "train loss:1.0192315852037028\n",
      "train loss:1.1026753009699184\n",
      "train loss:0.7633762734814902\n",
      "train loss:0.8573682375972108\n",
      "train loss:0.9877458169817339\n",
      "train loss:0.8981528716797474\n",
      "train loss:1.1136825899750478\n",
      "train loss:0.8921697393131902\n",
      "train loss:1.0730609014336754\n",
      "train loss:0.9943591986884147\n",
      "train loss:0.7043638391417671\n",
      "train loss:0.98402589442165\n",
      "train loss:0.9533957194904004\n",
      "train loss:0.8182066097061845\n",
      "train loss:1.0751776925117342\n",
      "train loss:0.9283090951523539\n",
      "train loss:0.9951236167631613\n",
      "train loss:0.8531900494882737\n",
      "train loss:0.7071546548019156\n",
      "train loss:0.9390765923420904\n",
      "train loss:0.8268832771683405\n",
      "train loss:0.8407022928656426\n",
      "train loss:1.0131248679901996\n",
      "train loss:0.9573339444606473\n",
      "train loss:0.9295253540655395\n",
      "train loss:0.7512619600638445\n",
      "train loss:0.8580674028758444\n",
      "train loss:0.8427529827243005\n",
      "train loss:0.9734914066514397\n",
      "train loss:0.8804857939559203\n",
      "train loss:0.8221042056167068\n",
      "train loss:0.7532581213982865\n",
      "train loss:0.78111124439122\n",
      "train loss:0.929903112865931\n",
      "train loss:1.1289799079367742\n",
      "train loss:0.8025110305713128\n",
      "train loss:0.9318119933979666\n",
      "train loss:1.0137267892182886\n",
      "train loss:0.9668146158106771\n",
      "train loss:0.8579521874982251\n",
      "train loss:0.7831222169562905\n",
      "train loss:0.9050502083239873\n",
      "train loss:0.9944616014716626\n",
      "train loss:1.0825264537506776\n",
      "train loss:0.8577292397257508\n",
      "train loss:0.8688898398383852\n",
      "train loss:1.0519823276668487\n",
      "train loss:0.9588394687269427\n",
      "train loss:0.8381336836915051\n",
      "train loss:0.8397488854176943\n",
      "train loss:0.8035977684725502\n",
      "train loss:0.7889038966735652\n",
      "train loss:0.8908655798591276\n",
      "train loss:1.0648419275827126\n",
      "train loss:0.9096688066352334\n",
      "train loss:1.0693845415217966\n",
      "train loss:0.9069884976959561\n",
      "train loss:0.8588237455635763\n",
      "train loss:0.9315435555051246\n",
      "train loss:0.8432932158029532\n",
      "train loss:0.8808581834305245\n",
      "train loss:0.9439687416563258\n",
      "train loss:0.9045037452612763\n",
      "train loss:0.7980048397644215\n",
      "train loss:0.791876608946207\n",
      "train loss:0.99321224019387\n",
      "train loss:0.7200671915979145\n",
      "train loss:0.8508041401303731\n",
      "train loss:1.006389320280912\n",
      "train loss:1.153376132793953\n",
      "train loss:0.909146290948318\n",
      "train loss:1.1400907982060544\n",
      "train loss:1.0705874903428474\n",
      "=== epoch:5, train acc:0.99, test acc:0.984 ===\n",
      "train loss:0.9956436127886451\n",
      "train loss:1.1766951577296902\n",
      "train loss:0.9218328581797137\n",
      "train loss:1.1409266726634066\n",
      "train loss:0.9084829167729765\n",
      "train loss:1.0434590756876867\n",
      "train loss:0.80304493450735\n",
      "train loss:0.8748973231449702\n",
      "train loss:0.9174630364044198\n",
      "train loss:1.0150078364133106\n",
      "train loss:0.8480559054456671\n",
      "train loss:0.9356929858487529\n",
      "train loss:1.104341179635194\n",
      "train loss:0.9803277446766845\n",
      "train loss:0.9263690366705398\n",
      "train loss:0.8176358781316116\n",
      "train loss:1.0750682352240364\n",
      "train loss:0.9999062534123091\n",
      "train loss:1.017468780267872\n",
      "train loss:1.1760799420526835\n",
      "train loss:0.9268506996667913\n",
      "train loss:0.8959032394432442\n",
      "train loss:0.9597981498383737\n",
      "train loss:0.9438257898663841\n",
      "train loss:0.8939429269321153\n",
      "train loss:0.8005868720041956\n",
      "train loss:0.9938080640025352\n",
      "train loss:0.9521698221319294\n",
      "train loss:1.088884660655426\n",
      "train loss:1.0045999363639815\n",
      "train loss:0.928383271165379\n",
      "train loss:0.851254951896689\n",
      "train loss:0.9200532846471744\n",
      "train loss:0.8899936642105711\n",
      "train loss:1.0592776193040896\n",
      "train loss:1.0204937069437598\n",
      "train loss:1.0633648499023227\n",
      "train loss:0.9444758959125561\n",
      "train loss:1.105649685065017\n",
      "train loss:1.1083521796753286\n",
      "train loss:0.8589673760803909\n",
      "train loss:0.8322185357397095\n",
      "train loss:0.8269608332891791\n",
      "train loss:0.9345722930541512\n",
      "train loss:0.9427925164135064\n",
      "train loss:0.8544423313356749\n",
      "train loss:1.0382573167128621\n",
      "train loss:1.0062013042682751\n",
      "train loss:0.9007671420449819\n",
      "train loss:0.9680517629142977\n",
      "train loss:1.0321696195515437\n",
      "train loss:0.9365748090166893\n",
      "train loss:0.8800413558213773\n",
      "train loss:0.9418642101141481\n",
      "train loss:1.0601874724033755\n",
      "train loss:1.0381830468273419\n",
      "train loss:0.9627605002633378\n",
      "train loss:0.9437932987702462\n",
      "train loss:0.8044363242164492\n",
      "train loss:1.1115242394901417\n",
      "train loss:0.9666709883186191\n",
      "train loss:0.8690053442437218\n",
      "train loss:0.9884737412460991\n",
      "train loss:0.9177055858540976\n",
      "train loss:0.9849262112478426\n",
      "train loss:0.9529513762783219\n",
      "train loss:0.9609270789734126\n",
      "train loss:0.8834615264956972\n",
      "train loss:0.805704496734622\n",
      "train loss:0.864716413039265\n",
      "train loss:0.935684495354113\n",
      "train loss:0.8701140379863368\n",
      "train loss:0.9046925733289979\n",
      "train loss:1.133009295344238\n",
      "train loss:0.9884835909811257\n",
      "train loss:0.9721869284236989\n",
      "train loss:0.9500524907280367\n",
      "train loss:0.8164113836113457\n",
      "train loss:0.7337875269437599\n",
      "train loss:1.1439723440310756\n",
      "train loss:0.778542833571665\n",
      "train loss:0.931048929505163\n",
      "train loss:0.8650218174318693\n",
      "train loss:0.9676493760431358\n",
      "train loss:0.9661837575702548\n",
      "train loss:0.8859046537416249\n",
      "train loss:0.9678060277322524\n",
      "train loss:0.8829627744174104\n",
      "train loss:0.8012470057449864\n",
      "train loss:0.9141757513556592\n",
      "train loss:1.0308874914681043\n",
      "train loss:1.0246268022234353\n",
      "train loss:1.079956736217447\n",
      "train loss:0.9227472256133753\n",
      "train loss:1.0015325301449192\n",
      "train loss:1.0843387824965074\n",
      "train loss:0.8960906860571314\n",
      "train loss:0.9603556873851106\n",
      "train loss:0.9622176002495927\n",
      "train loss:0.8871858636694806\n",
      "train loss:0.8569850899433717\n",
      "train loss:0.8767813002119703\n",
      "train loss:1.0586945038582682\n",
      "train loss:0.9146571727865771\n",
      "train loss:0.8776619023866961\n",
      "train loss:0.895661180177662\n",
      "train loss:0.9191700468029042\n",
      "train loss:1.0483431677907222\n",
      "train loss:0.8850626239914087\n",
      "train loss:0.8015446327287689\n",
      "train loss:0.9828552330054068\n",
      "train loss:0.9531193135144876\n",
      "train loss:0.8911421395442115\n",
      "train loss:0.9830824190235304\n",
      "train loss:0.913296320036261\n",
      "train loss:0.8446908562135511\n",
      "train loss:1.0457499457014547\n",
      "train loss:0.8869282965667549\n",
      "train loss:1.0408116586601066\n",
      "train loss:0.8805652119306963\n",
      "train loss:0.9040727729045013\n",
      "train loss:0.9911750801190561\n",
      "train loss:0.9469187394801559\n",
      "train loss:0.9378059025684904\n",
      "train loss:0.937520395917939\n",
      "train loss:1.0009562180703795\n",
      "train loss:1.0401791284631956\n",
      "train loss:0.5821998972707084\n",
      "train loss:0.8561053286850708\n",
      "train loss:1.140519178398126\n",
      "train loss:1.066298086013886\n",
      "train loss:0.9618686028511861\n",
      "train loss:0.7533630065401407\n",
      "train loss:0.8871058083843768\n",
      "train loss:0.8964074906614108\n",
      "train loss:1.0664574887874292\n",
      "train loss:0.8988393195062615\n",
      "train loss:0.9097401841345057\n",
      "train loss:0.8560501886302317\n",
      "train loss:1.0006026992603698\n",
      "train loss:0.9930861386155614\n",
      "train loss:0.9006211965656157\n",
      "train loss:0.8974074835356808\n",
      "train loss:0.899603527358151\n",
      "train loss:1.0412835528069169\n",
      "train loss:0.8977064794874561\n",
      "train loss:0.8557398691831481\n",
      "train loss:0.7978425390310205\n",
      "train loss:0.9413078109245528\n",
      "train loss:1.1033861585669626\n",
      "train loss:0.9696277519269753\n",
      "train loss:0.9090635194333242\n",
      "train loss:1.0010117487943924\n",
      "train loss:1.1292672457869712\n",
      "train loss:0.9739181071473313\n",
      "train loss:1.1145295809184201\n",
      "train loss:0.8312277423998311\n",
      "train loss:0.9000878040396303\n",
      "train loss:0.9752891460372761\n",
      "train loss:1.0201598889834895\n",
      "train loss:0.7942653767224231\n",
      "train loss:0.9857129425584692\n",
      "train loss:0.9491041392803045\n",
      "train loss:0.9478661377033475\n",
      "train loss:0.7514844936804557\n",
      "train loss:0.9319708376118265\n",
      "train loss:1.0329519620475975\n",
      "train loss:1.0176187230991907\n",
      "train loss:0.8643892239486101\n",
      "train loss:0.8872631661657117\n",
      "train loss:0.9136702352969595\n",
      "train loss:0.9109181748905845\n",
      "train loss:0.8320347635873575\n",
      "train loss:0.7351694587135468\n",
      "train loss:1.072166168676035\n",
      "train loss:0.9098623074127583\n",
      "train loss:1.05810882829533\n",
      "train loss:0.8339860259974848\n",
      "train loss:0.9933034393164667\n",
      "train loss:1.108147765594547\n",
      "train loss:1.0050006463485572\n",
      "train loss:0.8255621419228995\n",
      "train loss:1.0081890126120083\n",
      "train loss:0.9278680569007804\n",
      "train loss:0.8633342074597513\n",
      "train loss:0.9680973827414887\n",
      "train loss:0.8917542554664754\n",
      "train loss:0.886934287420146\n",
      "train loss:1.0745415475477176\n",
      "train loss:0.8950633458791895\n",
      "train loss:0.9216490808486633\n",
      "train loss:0.9277945308655071\n",
      "train loss:1.106515260198659\n",
      "train loss:0.8569673395048736\n",
      "train loss:0.8065447506565852\n",
      "train loss:0.8561144284603688\n",
      "train loss:1.0260252329698358\n",
      "train loss:0.9660459831113424\n",
      "train loss:1.0066813498624354\n",
      "train loss:1.0520543853452884\n",
      "train loss:0.8272241832888555\n",
      "train loss:0.7643355069544562\n",
      "train loss:0.7887775820102668\n",
      "train loss:1.0178405515607802\n",
      "train loss:0.9669000633339612\n",
      "train loss:0.8831110709645499\n",
      "train loss:1.2306036087884977\n",
      "train loss:0.9476570823991131\n",
      "train loss:0.8243598549098621\n",
      "train loss:1.029502352847773\n",
      "train loss:0.8847308278246504\n",
      "train loss:0.9661639471547173\n",
      "train loss:0.8808181622136958\n",
      "train loss:1.0180009143557542\n",
      "train loss:0.9378431470126014\n",
      "train loss:0.9428242408627056\n",
      "train loss:0.8573141677152287\n",
      "train loss:0.8223903918511\n",
      "train loss:0.8504569283385546\n",
      "train loss:0.9260485200253977\n",
      "train loss:1.0829257904410698\n",
      "train loss:0.9511898620178805\n",
      "train loss:0.8631153281045643\n",
      "train loss:1.165425865347906\n",
      "train loss:1.0012015659553015\n",
      "train loss:0.9884730730289293\n",
      "train loss:0.9332547439488478\n",
      "train loss:1.0454409715423254\n",
      "train loss:0.8913348454351343\n",
      "train loss:0.9039253973053598\n",
      "train loss:0.896694544713291\n",
      "train loss:0.9160253260521779\n",
      "train loss:1.0088163389120994\n",
      "train loss:0.9198224204034584\n",
      "train loss:0.9330368268295542\n",
      "train loss:0.8861982547284741\n",
      "train loss:0.9937595310518746\n",
      "train loss:0.8216558254017129\n",
      "train loss:0.8928029478760422\n",
      "train loss:0.8995075640444868\n",
      "train loss:0.8965828178425713\n",
      "train loss:0.7428148340476883\n",
      "train loss:0.7499326808036263\n",
      "train loss:0.9165929069430089\n",
      "train loss:0.9666076894000711\n",
      "train loss:0.8550060128365656\n",
      "train loss:0.9941084931459976\n",
      "train loss:0.980001972043154\n",
      "train loss:0.8618950986225635\n",
      "train loss:0.9414440913833954\n",
      "train loss:0.9686558047090115\n",
      "train loss:0.9060062979208493\n",
      "train loss:0.9944308477396526\n",
      "train loss:1.0089034828640957\n",
      "train loss:0.9040018443247416\n",
      "train loss:1.0424598644034986\n",
      "train loss:0.8666714326820579\n",
      "train loss:0.800916614498457\n",
      "train loss:0.9078307127321915\n",
      "train loss:0.7619910639217182\n",
      "train loss:0.9211232524524665\n",
      "train loss:1.1405023198338602\n",
      "train loss:0.8843354427614684\n",
      "train loss:0.6823117927055372\n",
      "train loss:1.0059143502279217\n",
      "train loss:0.8833653042505751\n",
      "train loss:0.9322459944322248\n",
      "train loss:1.0451839807516372\n",
      "train loss:0.9244591130634459\n",
      "train loss:0.9973798973642127\n",
      "train loss:0.9958887185637865\n",
      "train loss:1.1318906397021025\n",
      "train loss:0.9359691859748961\n",
      "train loss:0.9318673865895549\n",
      "train loss:0.9465963327359055\n",
      "train loss:0.8808636724472989\n",
      "train loss:1.0132122809032715\n",
      "train loss:1.022754684583835\n",
      "train loss:0.9453068720850599\n",
      "train loss:0.9670382151741876\n",
      "train loss:1.1202161759882272\n",
      "train loss:0.9053099006839311\n",
      "train loss:0.9137885824019791\n",
      "train loss:0.757583486048059\n",
      "train loss:1.0554928257289917\n",
      "train loss:0.9769163612186316\n",
      "train loss:0.8141966970715289\n",
      "train loss:1.0498419962701553\n",
      "train loss:0.9515448731049493\n",
      "train loss:0.8442495419429799\n",
      "train loss:0.9382072820513904\n",
      "train loss:0.9872839265378011\n",
      "train loss:0.9458656431494213\n",
      "train loss:0.8983974355656237\n",
      "train loss:0.9258866579018359\n",
      "train loss:1.0243281254033851\n",
      "train loss:0.8916638819675239\n",
      "train loss:1.10815117727724\n",
      "train loss:1.006803463682264\n",
      "train loss:0.9106571871134261\n",
      "train loss:0.888070793866395\n",
      "train loss:1.1477098238230896\n",
      "train loss:0.9151070012115332\n",
      "train loss:0.9685954799402482\n",
      "train loss:0.8829670685359914\n",
      "train loss:0.8082157235793449\n",
      "train loss:0.8144099224306565\n",
      "train loss:0.7630088263020331\n",
      "train loss:1.1011108514513546\n",
      "train loss:0.9397757402858659\n",
      "train loss:0.9169591261896418\n",
      "train loss:0.8532037825002046\n",
      "train loss:0.8962425806309905\n",
      "train loss:0.9368779015917553\n",
      "train loss:0.8887564493408888\n",
      "train loss:1.176281146264981\n",
      "train loss:0.8141964652337628\n",
      "train loss:0.9278375763465327\n",
      "train loss:1.0006947022669486\n",
      "train loss:0.9413377109112555\n",
      "train loss:1.0112031750314951\n",
      "train loss:1.0419325364734957\n",
      "train loss:0.9743848986238774\n",
      "train loss:0.9812501977944001\n",
      "train loss:0.8273009865185317\n",
      "train loss:0.9473406884568516\n",
      "train loss:0.9367345026611763\n",
      "train loss:0.8841394676614737\n",
      "train loss:0.9278873451368973\n",
      "train loss:0.9271035602045464\n",
      "train loss:0.9302035381623278\n",
      "train loss:0.7562118766497279\n",
      "train loss:1.1379504683834556\n",
      "train loss:0.8584842384394126\n",
      "train loss:1.0495526103170514\n",
      "train loss:0.9909487059966422\n",
      "train loss:0.7970989350080899\n",
      "train loss:0.9792849754073346\n",
      "train loss:0.9566456576097846\n",
      "train loss:0.9194159815584654\n",
      "train loss:0.9500997992823287\n",
      "train loss:1.0049864688347367\n",
      "train loss:1.0014650772564824\n",
      "train loss:0.8743647090139135\n",
      "train loss:0.7737532937417896\n",
      "train loss:0.9047386245861699\n",
      "train loss:1.071635406049223\n",
      "train loss:0.9434989978880952\n",
      "train loss:0.7462403916826728\n",
      "train loss:0.7514225191247569\n",
      "train loss:1.0326965132525017\n",
      "train loss:0.9717676925984553\n",
      "train loss:0.8504009676074101\n",
      "train loss:0.7850889352107534\n",
      "train loss:0.9127480782670645\n",
      "train loss:0.8680211365851243\n",
      "train loss:0.8429593206056586\n",
      "train loss:1.0604154680369147\n",
      "train loss:0.979729779644951\n",
      "train loss:0.9868515917703471\n",
      "train loss:0.9615839984824328\n",
      "train loss:0.9538028422620879\n",
      "train loss:0.9114174654240526\n",
      "train loss:0.8408281553714069\n",
      "train loss:1.0069589724006682\n",
      "train loss:0.8499306373203243\n",
      "train loss:0.9970315910747374\n",
      "train loss:0.994958285094011\n",
      "train loss:0.835876876746127\n",
      "train loss:1.0594260411141794\n",
      "train loss:1.0047136324200685\n",
      "train loss:0.9024680525228795\n",
      "train loss:0.7001276039178085\n",
      "train loss:0.98987851201963\n",
      "train loss:0.9789933179175678\n",
      "train loss:0.880611227732333\n",
      "train loss:1.0059683839164042\n",
      "train loss:1.0286056551603342\n",
      "train loss:0.8562762160774006\n",
      "train loss:0.9553358155523283\n",
      "train loss:0.9321180976016837\n",
      "train loss:0.6770820324672506\n",
      "train loss:0.7979460773581772\n",
      "train loss:0.8197532203718341\n",
      "train loss:1.018090020075977\n",
      "train loss:0.9196748910218169\n",
      "train loss:1.0059824278343537\n",
      "train loss:0.8915577024027083\n",
      "train loss:0.8711024474296888\n",
      "train loss:0.9454348992621697\n",
      "train loss:0.7697393365770097\n",
      "train loss:0.9137588927460777\n",
      "train loss:0.7088686075103544\n",
      "train loss:0.8077554972400113\n",
      "train loss:0.8017862954726863\n",
      "train loss:0.9469659232761717\n",
      "train loss:1.0503301126400566\n",
      "train loss:0.8646454097322611\n",
      "train loss:1.2428186093689915\n",
      "train loss:1.0049285844077782\n",
      "train loss:0.9797857695063652\n",
      "train loss:1.0445717994367163\n",
      "train loss:0.9706626469328534\n",
      "train loss:0.9289950119277876\n",
      "train loss:0.9978230402208542\n",
      "train loss:1.0711518789048973\n",
      "train loss:0.9570491433702445\n",
      "train loss:1.0331748859735712\n",
      "train loss:0.939546355020629\n",
      "train loss:1.1666898830687267\n",
      "train loss:1.0353663222935525\n",
      "train loss:0.923733752648958\n",
      "train loss:0.8440219453967168\n",
      "train loss:0.8769023754051034\n",
      "train loss:0.9170664568606994\n",
      "train loss:1.0294929833061606\n",
      "train loss:0.7520331453241575\n",
      "train loss:0.9357819543470905\n",
      "train loss:0.9213845482404173\n",
      "train loss:0.8622754981266904\n",
      "train loss:1.0485123605265219\n",
      "train loss:0.8277219810361189\n",
      "train loss:0.7830122262413856\n",
      "train loss:1.0071446542535103\n",
      "train loss:0.8129430122626685\n",
      "train loss:0.8486839608459635\n",
      "train loss:1.077926436765946\n",
      "train loss:0.99249245467988\n",
      "train loss:0.8572455850904744\n",
      "train loss:0.8228546843381611\n",
      "train loss:0.8251482981803038\n",
      "train loss:0.9858144737507122\n",
      "train loss:0.9579991430360845\n",
      "train loss:0.870959015050898\n",
      "train loss:0.8056489287343415\n",
      "train loss:0.9938308180437795\n",
      "train loss:0.8130830558779262\n",
      "train loss:0.8652795186941347\n",
      "train loss:0.9969091094069238\n",
      "train loss:0.8912123262221298\n",
      "train loss:1.0372936214783517\n",
      "train loss:0.9555123089109737\n",
      "train loss:0.8294064335253652\n",
      "train loss:0.9673310389814541\n",
      "train loss:0.9913470507617226\n",
      "train loss:1.0378235678454855\n",
      "train loss:0.9763261202662512\n",
      "train loss:0.8231789082183483\n",
      "train loss:0.8891449102478093\n",
      "train loss:0.8280670134339525\n",
      "train loss:0.93882963565452\n",
      "train loss:0.850077620223155\n",
      "train loss:0.9661086963177613\n",
      "train loss:1.0115968187462527\n",
      "train loss:0.9684603323914174\n",
      "train loss:0.7889363826331229\n",
      "train loss:0.8983537543709674\n",
      "train loss:0.7681446716187595\n",
      "train loss:1.0150424825611717\n",
      "train loss:0.9834515823537413\n",
      "train loss:1.0563511750596792\n",
      "train loss:0.9576981655406819\n",
      "train loss:0.9366767211324691\n",
      "train loss:1.0299955114375485\n",
      "train loss:0.9291493948159555\n",
      "train loss:0.9137643328941523\n",
      "train loss:0.9076885276606199\n",
      "train loss:1.02250963879951\n",
      "train loss:1.0590871683601377\n",
      "train loss:0.9111155699921427\n",
      "train loss:0.9672681519674313\n",
      "train loss:0.9387162849071474\n",
      "train loss:1.0114944668989716\n",
      "train loss:1.0094224485179044\n",
      "train loss:0.8762191544922581\n",
      "train loss:1.014452055990563\n",
      "train loss:1.0487334401694124\n",
      "train loss:0.845890400749392\n",
      "train loss:0.8888773323194137\n",
      "train loss:0.9362314187161587\n",
      "train loss:0.8730050691955178\n",
      "train loss:0.9443377471906901\n",
      "train loss:0.9399724601257016\n",
      "train loss:0.9114302317444652\n",
      "train loss:0.9246173217781302\n",
      "train loss:0.9696623864733804\n",
      "train loss:0.9265799153757674\n",
      "train loss:1.0322637255341427\n",
      "train loss:0.9276533846683158\n",
      "train loss:0.9841534155281764\n",
      "train loss:0.8085037259929923\n",
      "train loss:1.0201704538791636\n",
      "train loss:0.8440517752190712\n",
      "train loss:0.840357365049679\n",
      "train loss:1.0490270166694657\n",
      "train loss:1.014045852019278\n",
      "train loss:0.8488016234454996\n",
      "train loss:0.9817363899838845\n",
      "train loss:1.0478358521579307\n",
      "train loss:0.9606521778130939\n",
      "train loss:0.8469874705636699\n",
      "train loss:0.9167237705171144\n",
      "train loss:0.9458783875543508\n",
      "train loss:0.8723475168402817\n",
      "train loss:0.975224514672568\n",
      "train loss:0.8672036531227078\n",
      "train loss:0.9546472791669578\n",
      "train loss:0.9693773916140784\n",
      "train loss:0.8326958080341885\n",
      "train loss:0.8988124074470928\n",
      "train loss:0.9743123388356106\n",
      "train loss:0.9210866699065399\n",
      "train loss:0.819676852197516\n",
      "train loss:0.8381731693605801\n",
      "train loss:0.865629934071865\n",
      "train loss:0.9282548992309042\n",
      "train loss:0.960372545710213\n",
      "train loss:0.6392964591947905\n",
      "train loss:1.0426587715288502\n",
      "train loss:0.8777970313098084\n",
      "train loss:0.8094529611704906\n",
      "train loss:0.7111097436940053\n",
      "train loss:1.024846889035774\n",
      "train loss:0.9124558825155786\n",
      "train loss:0.9628971620134423\n",
      "train loss:0.9747213513939061\n",
      "train loss:0.8486948890230852\n",
      "train loss:0.946939114571008\n",
      "train loss:0.8895514350472119\n",
      "train loss:0.7936938035976867\n",
      "train loss:1.0023079774120902\n",
      "train loss:0.9823907217352559\n",
      "train loss:0.9713844063723523\n",
      "train loss:0.7635175634168156\n",
      "train loss:1.0342082597554734\n",
      "train loss:1.0823169395958852\n",
      "train loss:0.9683119378809272\n",
      "train loss:0.9625414444316435\n",
      "train loss:0.9757369557850916\n",
      "train loss:0.8270135922624295\n",
      "train loss:0.9416804483245119\n",
      "train loss:0.9057330152714513\n",
      "train loss:1.01040975678484\n",
      "train loss:0.8970743439463108\n",
      "train loss:0.9594916755204973\n",
      "train loss:1.08793412492855\n",
      "train loss:1.129266407394138\n",
      "train loss:0.9187134832024954\n",
      "train loss:1.0159532260545479\n",
      "train loss:1.0898130379119295\n",
      "train loss:0.6642559351328365\n",
      "train loss:0.9744213687963531\n",
      "train loss:0.751083156794903\n",
      "train loss:0.9910776685484025\n",
      "train loss:0.9589074572509685\n",
      "train loss:1.0289753694438042\n",
      "train loss:0.9178186556782915\n",
      "train loss:0.8540055537337188\n",
      "train loss:1.102242019415217\n",
      "train loss:0.8239643212167634\n",
      "train loss:0.9127976504520285\n",
      "train loss:0.7345284335696802\n",
      "train loss:0.8707293288966214\n",
      "train loss:0.8538509513764619\n",
      "train loss:0.9891734308871034\n",
      "train loss:0.9665936603085379\n",
      "train loss:1.206664793425257\n",
      "train loss:1.1388643661670919\n",
      "train loss:0.8735289585217396\n",
      "train loss:1.024836953866884\n",
      "train loss:1.0205184066498167\n",
      "train loss:0.9250533667874347\n",
      "train loss:0.8314425364845516\n",
      "train loss:0.9409795297233623\n",
      "train loss:0.9115194369492062\n",
      "train loss:0.8462979501635874\n",
      "train loss:0.7816115906955668\n",
      "train loss:0.9274293502005125\n",
      "train loss:0.869272218796105\n",
      "train loss:0.9291258007503979\n",
      "train loss:0.9975214563957472\n",
      "train loss:0.7427390141824555\n",
      "train loss:0.9001431441297087\n",
      "train loss:1.0951320640674307\n",
      "train loss:0.9290588723004737\n",
      "train loss:0.8066641225585031\n",
      "train loss:1.0451946051019367\n",
      "train loss:0.8301423883540494\n",
      "train loss:0.952539421587411\n",
      "train loss:1.0811447859384358\n",
      "train loss:0.9788818972416483\n",
      "train loss:0.8808211568823288\n",
      "train loss:1.0437789747721993\n",
      "train loss:1.066857257311493\n",
      "train loss:1.035565044194002\n",
      "train loss:0.9479486538591152\n",
      "train loss:1.0393844789230018\n",
      "train loss:1.0204657812380606\n",
      "train loss:0.8953051233047211\n",
      "train loss:1.0503127733140307\n",
      "=== epoch:6, train acc:0.992, test acc:0.988 ===\n",
      "train loss:0.934747288958669\n",
      "train loss:0.8562258802134168\n",
      "train loss:0.949014556284684\n",
      "train loss:0.8643441581530155\n",
      "train loss:1.0035154919206335\n",
      "train loss:0.8221675108087476\n",
      "train loss:0.9827287555397843\n",
      "train loss:1.086519950575533\n",
      "train loss:0.7152414154598509\n",
      "train loss:0.8571918442253756\n",
      "train loss:0.8837835558090086\n",
      "train loss:0.970722064197936\n",
      "train loss:0.8972063469043947\n",
      "train loss:0.871488867686731\n",
      "train loss:0.8823718844854024\n",
      "train loss:0.8424220421060578\n",
      "train loss:0.8146870999149606\n",
      "train loss:0.9666112689084059\n",
      "train loss:0.9961155454116464\n",
      "train loss:0.9784930174984264\n",
      "train loss:0.8197121516688546\n",
      "train loss:0.9422660573416305\n",
      "train loss:0.7234385986467933\n",
      "train loss:1.1109237025411358\n",
      "train loss:0.7345162660862455\n",
      "train loss:0.8814950683391881\n",
      "train loss:0.9127225160183697\n",
      "train loss:1.0165947221831282\n",
      "train loss:0.8549789460950293\n",
      "train loss:0.8809199135796054\n",
      "train loss:0.8776172367547437\n",
      "train loss:0.9873766656814678\n",
      "train loss:0.8770323658922983\n",
      "train loss:0.9506350357170937\n",
      "train loss:0.9621259085599192\n",
      "train loss:1.1483096835767594\n",
      "train loss:0.9058831806626454\n",
      "train loss:0.8621417520229526\n",
      "train loss:1.0944572382920217\n",
      "train loss:0.9075116351921069\n",
      "train loss:0.8496920376285846\n",
      "train loss:1.0478694662121553\n",
      "train loss:0.8600810206017355\n",
      "train loss:0.8401297948237573\n",
      "train loss:1.0040419681706862\n",
      "train loss:0.9219708336389565\n",
      "train loss:0.9441822134571751\n",
      "train loss:0.986820011781288\n",
      "train loss:0.8901245029961022\n",
      "train loss:1.0235318570218266\n",
      "train loss:0.8688742981868467\n",
      "train loss:0.799563642944726\n",
      "train loss:0.8283976409210241\n",
      "train loss:0.8614503297545781\n",
      "train loss:0.7437227425312617\n",
      "train loss:0.8946038960523383\n",
      "train loss:0.9799731395151938\n",
      "train loss:0.7540085743148366\n",
      "train loss:0.8534179581938965\n",
      "train loss:0.9102591032566955\n",
      "train loss:0.9160275228217905\n",
      "train loss:0.7927864940414066\n",
      "train loss:0.8555671853646065\n",
      "train loss:0.9679261030879723\n",
      "train loss:0.9140996253272639\n",
      "train loss:0.8226905875534775\n",
      "train loss:0.8592997132705048\n",
      "train loss:0.8629238067545428\n",
      "train loss:0.9087957002746014\n",
      "train loss:0.9757375542391052\n",
      "train loss:1.023610822100602\n",
      "train loss:0.868993618455944\n",
      "train loss:0.7962971471830241\n",
      "train loss:1.0031264269493194\n",
      "train loss:1.120338422475181\n",
      "train loss:0.9411651927161284\n",
      "train loss:0.9171360165920568\n",
      "train loss:0.9251132565739859\n",
      "train loss:0.9594454484417709\n",
      "train loss:1.1174441649333813\n",
      "train loss:0.8455316569178123\n",
      "train loss:1.0166958197291052\n",
      "train loss:0.8651358238985196\n",
      "train loss:0.7821622596419625\n",
      "train loss:1.1663900344716682\n",
      "train loss:0.7801832295533354\n",
      "train loss:0.8575884886793073\n",
      "train loss:0.949551161753243\n",
      "train loss:0.9484788506905172\n",
      "train loss:0.8878009221421072\n",
      "train loss:0.9475364023506466\n",
      "train loss:0.893563595582141\n",
      "train loss:0.8798985074621326\n",
      "train loss:0.9194121037371212\n",
      "train loss:0.8759501814047916\n",
      "train loss:0.8844855401965279\n",
      "train loss:0.8122186107352519\n",
      "train loss:0.8427491854494812\n",
      "train loss:0.9190212623094811\n",
      "train loss:0.8953825810619429\n",
      "train loss:0.8070217195655255\n",
      "train loss:1.026857901802802\n",
      "train loss:1.0654919350853032\n",
      "train loss:0.8885237161632138\n",
      "train loss:0.9910786529313418\n",
      "train loss:0.9761123853700281\n",
      "train loss:1.0537129716740257\n",
      "train loss:0.9270506309036226\n",
      "train loss:0.9381509512187275\n",
      "train loss:0.8685872006753043\n",
      "train loss:0.9110250622601863\n",
      "train loss:0.8516576253817856\n",
      "train loss:0.8387722537086368\n",
      "train loss:0.8530917346087442\n",
      "train loss:0.8218974894633565\n",
      "train loss:0.8290146786341711\n",
      "train loss:0.9481064089797436\n",
      "train loss:1.1831056746762123\n",
      "train loss:0.9360008906663236\n",
      "train loss:0.8225273941046154\n",
      "train loss:0.8225077993909277\n",
      "train loss:0.9440763527700433\n",
      "train loss:0.9898681997908315\n",
      "train loss:0.9103487622165446\n",
      "train loss:0.9635446627462468\n",
      "train loss:0.9139997044671883\n",
      "train loss:0.9350804178719492\n",
      "train loss:0.6535977057956031\n",
      "train loss:0.9218209172110048\n",
      "train loss:0.8842821434390742\n",
      "train loss:1.0457901699449688\n",
      "train loss:0.8998686209512763\n",
      "train loss:0.8544084186277462\n",
      "train loss:0.9409221584805201\n",
      "train loss:1.3346333025387094\n",
      "train loss:0.9281402602177049\n",
      "train loss:0.7702765212654793\n",
      "train loss:0.8779908625540495\n",
      "train loss:0.9332664585018846\n",
      "train loss:1.0112366761345621\n",
      "train loss:0.864465999328318\n",
      "train loss:0.9225207541678618\n",
      "train loss:1.122017263035354\n",
      "train loss:0.8610219563591376\n",
      "train loss:1.0932125335677967\n",
      "train loss:1.086177275035509\n",
      "train loss:0.9867723483466604\n",
      "train loss:0.9071998467193442\n",
      "train loss:1.040461280883639\n",
      "train loss:0.9182870602434764\n",
      "train loss:0.9771708868039539\n",
      "train loss:0.8293006662440854\n",
      "train loss:0.9392923938323449\n",
      "train loss:0.8452627697118299\n",
      "train loss:0.7371323806444233\n",
      "train loss:0.8731088814099418\n",
      "train loss:0.634966428445193\n",
      "train loss:0.9518588495659934\n",
      "train loss:1.022035473612111\n",
      "train loss:0.9625794695514547\n",
      "train loss:0.773022203054108\n",
      "train loss:0.9347030935813626\n",
      "train loss:1.106820078256745\n",
      "train loss:1.004479258128707\n",
      "train loss:0.9182246884779562\n",
      "train loss:1.1615411636331334\n",
      "train loss:0.9046871221447513\n",
      "train loss:1.0299633130081702\n",
      "train loss:0.7888377616726119\n",
      "train loss:0.8576505714632021\n",
      "train loss:1.0301481710755522\n",
      "train loss:0.8559511963301304\n",
      "train loss:0.9490675656458306\n",
      "train loss:1.0195421268092895\n",
      "train loss:0.9280600451391883\n",
      "train loss:0.9833473617690432\n",
      "train loss:0.9216619257819536\n",
      "train loss:0.9448905539374002\n",
      "train loss:0.8443394190416881\n",
      "train loss:1.0488155628176457\n",
      "train loss:1.0455716443103222\n",
      "train loss:0.8590812926660442\n",
      "train loss:0.9639676439451936\n",
      "train loss:0.973690488647505\n",
      "train loss:0.8531230890868672\n",
      "train loss:0.9439384019227742\n",
      "train loss:1.0525431391992002\n",
      "train loss:0.7487444397164391\n",
      "train loss:1.0798161106393638\n",
      "train loss:1.0874754969159053\n",
      "train loss:0.9396297916464671\n",
      "train loss:1.000367637934759\n",
      "train loss:0.8892212916387873\n",
      "train loss:0.7749601454194125\n",
      "train loss:0.8283993983610318\n",
      "train loss:0.7884375070826999\n",
      "train loss:0.8285075848028598\n",
      "train loss:1.0223629934731775\n",
      "train loss:0.9039606709811778\n",
      "train loss:0.9519819060142473\n",
      "train loss:1.1892695246056755\n",
      "train loss:0.8038724114598207\n",
      "train loss:1.1119216676457562\n",
      "train loss:0.8274977276241863\n",
      "train loss:0.9960419379182034\n",
      "train loss:0.8860222579870467\n",
      "train loss:0.9722462943251116\n",
      "train loss:0.8361689972206905\n",
      "train loss:0.9575900542948292\n",
      "train loss:0.9315036617576347\n",
      "train loss:0.7955586739401224\n",
      "train loss:0.9440342490105972\n",
      "train loss:0.8409265133475813\n",
      "train loss:0.9439617652510875\n",
      "train loss:1.003970805887412\n",
      "train loss:0.9144815720709039\n",
      "train loss:0.8370758754809892\n",
      "train loss:0.9585006928973508\n",
      "train loss:0.9176810345577319\n",
      "train loss:1.0134364725050353\n",
      "train loss:1.0182972782274207\n",
      "train loss:0.8350345827924905\n",
      "train loss:0.6868538656304461\n",
      "train loss:0.8384164807996362\n",
      "train loss:0.790134934384787\n",
      "train loss:0.8449671361559926\n",
      "train loss:0.876264963726911\n",
      "train loss:0.9667141228486023\n",
      "train loss:0.9532096961119183\n",
      "train loss:0.88031024485833\n",
      "train loss:0.8059634180003191\n",
      "train loss:0.9528344773111865\n",
      "train loss:0.8871152715256697\n",
      "train loss:0.8873805866069888\n",
      "train loss:1.003253886807683\n",
      "train loss:0.9428377104474057\n",
      "train loss:0.9533706341392105\n",
      "train loss:0.972996813972268\n",
      "train loss:0.7811342470405106\n",
      "train loss:1.062453815442787\n",
      "train loss:0.8013007875517042\n",
      "train loss:0.9419442522339222\n",
      "train loss:0.8987859399916797\n",
      "train loss:0.8851474723591073\n",
      "train loss:1.0385584844481945\n",
      "train loss:0.9853277848586718\n",
      "train loss:0.8579739527451118\n",
      "train loss:0.9191778230687132\n",
      "train loss:0.9718345303218159\n",
      "train loss:0.9069414178131764\n",
      "train loss:0.9041609868533489\n",
      "train loss:1.0998598582325974\n",
      "train loss:0.9267268089139609\n",
      "train loss:1.0620153718195013\n",
      "train loss:0.7584378801771551\n",
      "train loss:1.0084047465456414\n",
      "train loss:1.0666812079697445\n",
      "train loss:0.9520272712243838\n",
      "train loss:0.9102516866400319\n",
      "train loss:0.8903203815776473\n",
      "train loss:0.9380099440985659\n",
      "train loss:0.92761451602598\n",
      "train loss:0.8189152010269806\n",
      "train loss:1.0040633238500676\n",
      "train loss:0.9812118098537654\n",
      "train loss:1.1688470486116587\n",
      "train loss:1.000819388157345\n",
      "train loss:0.9285747084172468\n",
      "train loss:0.8280728998082758\n",
      "train loss:0.8842637879054486\n",
      "train loss:0.9295892319625702\n",
      "train loss:0.9016628277527792\n",
      "train loss:0.8633859584866229\n",
      "train loss:0.8536907792586632\n",
      "train loss:0.7975036288958506\n",
      "train loss:1.0395784700078547\n",
      "train loss:0.9809992910318925\n",
      "train loss:1.0021134789878652\n",
      "train loss:0.9439077362593564\n",
      "train loss:0.8496568651020036\n",
      "train loss:0.9518604169567004\n",
      "train loss:1.1825486352686267\n",
      "train loss:0.8508439164456862\n",
      "train loss:0.9331409715515594\n",
      "train loss:1.083179720046478\n",
      "train loss:0.9741737727912592\n",
      "train loss:0.8396086541699657\n",
      "train loss:0.8918890296365302\n",
      "train loss:0.8601807228618203\n",
      "train loss:0.8573183574558211\n",
      "train loss:0.8005503660169693\n",
      "train loss:1.1966989526894276\n",
      "train loss:0.903648566105596\n",
      "train loss:1.0124483970518428\n",
      "train loss:0.8894318891472173\n",
      "train loss:0.7789818563861111\n",
      "train loss:0.9708355577932655\n",
      "train loss:0.8712719199587045\n",
      "train loss:0.9531501567067647\n",
      "train loss:0.9720086211084913\n",
      "train loss:0.9981933508743781\n",
      "train loss:0.884691737222895\n",
      "train loss:0.8303670077907686\n",
      "train loss:0.9600185641976279\n",
      "train loss:0.9947048468123241\n",
      "train loss:1.0052430543062694\n",
      "train loss:0.9680540813589706\n",
      "train loss:0.9679775065330475\n",
      "train loss:0.868869294963329\n",
      "train loss:0.9773194207196254\n",
      "train loss:1.0222455080592443\n",
      "train loss:0.7675148032490915\n",
      "train loss:0.8111423136252991\n",
      "train loss:0.9636373640079233\n",
      "train loss:0.9279322428971356\n",
      "train loss:1.0332459815158253\n",
      "train loss:0.887512478274785\n",
      "train loss:0.79397413461481\n",
      "train loss:0.9650094033001771\n",
      "train loss:0.9788061436063623\n",
      "train loss:1.0253794381951866\n",
      "train loss:0.7631050255088324\n",
      "train loss:1.089533519436305\n",
      "train loss:0.7842532847035889\n",
      "train loss:0.8777675848280526\n",
      "train loss:1.0231575868287537\n",
      "train loss:0.9140262470597271\n",
      "train loss:0.8868782049858922\n",
      "train loss:0.9737022922501543\n",
      "train loss:0.8127376339672199\n",
      "train loss:0.9083415318016835\n",
      "train loss:1.0891157245202985\n",
      "train loss:0.8738945060680566\n",
      "train loss:0.7620203481251042\n",
      "train loss:0.7346799042849267\n",
      "train loss:0.8996424453220895\n",
      "train loss:1.1585093098934813\n",
      "train loss:0.8666779802971434\n",
      "train loss:0.7677641991378837\n",
      "train loss:0.9313206495061351\n",
      "train loss:0.8263903336401947\n",
      "train loss:1.0676010888393643\n",
      "train loss:1.039633676320522\n",
      "train loss:0.934470701722764\n",
      "train loss:0.8068892440415696\n",
      "train loss:0.9340045164644506\n",
      "train loss:0.9602834795857252\n",
      "train loss:0.9722988455124173\n",
      "train loss:0.8838889458605095\n",
      "train loss:0.9269716726962914\n",
      "train loss:0.8374469080041748\n",
      "train loss:0.948209818971138\n",
      "train loss:0.9686922338587277\n",
      "train loss:0.9519827757292625\n",
      "train loss:0.8844935502108366\n",
      "train loss:0.926306278485502\n",
      "train loss:0.942754746264696\n",
      "train loss:0.9121748547523343\n",
      "train loss:1.0757819551717085\n",
      "train loss:0.7492071051157929\n",
      "train loss:0.9091956079092932\n",
      "train loss:1.0786247901460948\n",
      "train loss:0.9495545613364901\n",
      "train loss:0.8668032123153016\n",
      "train loss:0.9546115566854286\n",
      "train loss:0.9106814638568698\n",
      "train loss:0.987637735554206\n",
      "train loss:0.7722262528920355\n",
      "train loss:1.0402120482131152\n",
      "train loss:0.7056221970349315\n",
      "train loss:0.9467219965906406\n",
      "train loss:0.8038439816301788\n",
      "train loss:0.8515631514265558\n",
      "train loss:0.8532900041243859\n",
      "train loss:0.7675499525126667\n",
      "train loss:0.973363609181559\n",
      "train loss:0.8729770968263708\n",
      "train loss:1.0113990103079213\n",
      "train loss:0.9370851454672716\n",
      "train loss:0.7284699593466781\n",
      "train loss:0.879793675483875\n",
      "train loss:0.8696609039634116\n",
      "train loss:0.9076026349961672\n",
      "train loss:0.9583991849539164\n",
      "train loss:0.8456317153313097\n",
      "train loss:0.8838635184472777\n",
      "train loss:0.8677613447184735\n",
      "train loss:0.9397236976851818\n",
      "train loss:0.9296355847745046\n",
      "train loss:0.9821482797264116\n",
      "train loss:0.8764471453321869\n",
      "train loss:0.9773133116074868\n",
      "train loss:0.8252196239699071\n",
      "train loss:1.0226541269423874\n",
      "train loss:0.9450943171668122\n",
      "train loss:1.0119071526833756\n",
      "train loss:0.9372092161174136\n",
      "train loss:0.8791547980527433\n",
      "train loss:0.8421383887743166\n",
      "train loss:0.8385022328023878\n",
      "train loss:1.0250937354645915\n",
      "train loss:0.955321996159868\n",
      "train loss:0.9738277025404996\n",
      "train loss:0.9355421805196862\n",
      "train loss:0.803206891753002\n",
      "train loss:0.9660993159689741\n",
      "train loss:0.842245497882911\n",
      "train loss:0.9159925512518372\n",
      "train loss:0.762765576805006\n",
      "train loss:0.9856288769949022\n",
      "train loss:0.9459808950690102\n",
      "train loss:0.9923126665292006\n",
      "train loss:1.004336669487549\n",
      "train loss:1.0219007358629115\n",
      "train loss:0.924547652666453\n",
      "train loss:0.8205787230201278\n",
      "train loss:0.9817106424660389\n",
      "train loss:0.8919915495658519\n",
      "train loss:0.9899196269205439\n",
      "train loss:0.9219323515212235\n",
      "train loss:0.9885002336384736\n",
      "train loss:0.9474832215212345\n",
      "train loss:0.9939298624885418\n",
      "train loss:0.9758404896065503\n",
      "train loss:0.9145823324990694\n",
      "train loss:0.9549243719017484\n",
      "train loss:0.9696031443534869\n",
      "train loss:0.9341492965268615\n",
      "train loss:0.7234505568216104\n",
      "train loss:0.8275234551837634\n",
      "train loss:1.0287964697733103\n",
      "train loss:0.9546629364450051\n",
      "train loss:1.0520430468077138\n",
      "train loss:0.774260784920484\n",
      "train loss:0.7642100751336559\n",
      "train loss:1.0702897870896864\n",
      "train loss:0.8892969054288697\n",
      "train loss:1.2178488723896446\n",
      "train loss:0.9964484918932243\n",
      "train loss:1.0030878107338226\n",
      "train loss:0.8577518730081679\n",
      "train loss:0.9962590966907138\n",
      "train loss:0.8898616458859843\n",
      "train loss:0.8632954116672965\n",
      "train loss:1.0045574573791247\n",
      "train loss:1.0105667163106271\n",
      "train loss:1.0551342433394348\n",
      "train loss:0.8581462385954406\n",
      "train loss:0.7310640939967723\n",
      "train loss:0.9817177586462639\n",
      "train loss:0.8525765492708293\n",
      "train loss:1.007894654988359\n",
      "train loss:0.8651964360438279\n",
      "train loss:0.7316389977831623\n",
      "train loss:1.010380998921579\n",
      "train loss:0.9128953645452085\n",
      "train loss:0.9017223418178283\n",
      "train loss:0.8947520926966014\n",
      "train loss:0.9600380541973698\n",
      "train loss:1.0389897470799072\n",
      "train loss:0.7773446901744783\n",
      "train loss:1.031233506366026\n",
      "train loss:0.9770235800608005\n",
      "train loss:0.8407708327432065\n",
      "train loss:0.9804417542716236\n",
      "train loss:0.9767169896789129\n",
      "train loss:0.995971388343786\n",
      "train loss:0.9930337566906018\n",
      "train loss:0.6983194185169741\n",
      "train loss:1.0383033702840891\n",
      "train loss:0.8860812471379105\n",
      "train loss:0.7463153836912682\n",
      "train loss:0.9626379004538198\n",
      "train loss:0.8623757519729334\n",
      "train loss:0.9132722467152599\n",
      "train loss:1.1650457101245653\n",
      "train loss:0.9629337359764761\n",
      "train loss:0.9337273748114215\n",
      "train loss:0.87444503437384\n",
      "train loss:0.7898516758628701\n",
      "train loss:0.8095178878361281\n",
      "train loss:0.9697648788337196\n",
      "train loss:1.027590848598015\n",
      "train loss:0.8292903959333053\n",
      "train loss:0.7840237360788287\n",
      "train loss:0.7261000079630807\n",
      "train loss:0.8756002147576267\n",
      "train loss:0.8668741637382993\n",
      "train loss:0.8845455301446691\n",
      "train loss:0.671134812246831\n",
      "train loss:0.9128836199602803\n",
      "train loss:0.9868649336640203\n",
      "train loss:0.76414640705065\n",
      "train loss:1.0439604723257434\n",
      "train loss:1.120524023617839\n",
      "train loss:1.041906691069623\n",
      "train loss:1.001672002086999\n",
      "train loss:1.0122689868046086\n",
      "train loss:0.8776316555867748\n",
      "train loss:1.0086760242318822\n",
      "train loss:0.9636242966599531\n",
      "train loss:0.8947122985337453\n",
      "train loss:0.9273460722117904\n",
      "train loss:1.0346147375994288\n",
      "train loss:0.8266500154587574\n",
      "train loss:0.8000883146884368\n",
      "train loss:0.906220098627829\n",
      "train loss:0.9567685916411246\n",
      "train loss:0.9266969601931072\n",
      "train loss:0.9661937155832452\n",
      "train loss:0.9538753455895731\n",
      "train loss:0.8448406019016734\n",
      "train loss:0.8661411047839301\n",
      "train loss:0.9088883776022585\n",
      "train loss:0.9724468445625563\n",
      "train loss:0.8419636434995647\n",
      "train loss:0.7657105940060401\n",
      "train loss:0.841525575628196\n",
      "train loss:0.8107943631911992\n",
      "train loss:0.9403994428814063\n",
      "train loss:0.8651772901034895\n",
      "train loss:0.8381699179654237\n",
      "train loss:0.8756902854187867\n",
      "train loss:0.8453907377048315\n",
      "train loss:0.936169870336484\n",
      "train loss:0.7569720625602203\n",
      "train loss:0.9028453616679343\n",
      "train loss:0.8506474395532916\n",
      "train loss:0.9715685264871184\n",
      "train loss:0.937198948900263\n",
      "train loss:1.0247725551211764\n",
      "train loss:0.8097802815029975\n",
      "train loss:0.9435932508945308\n",
      "train loss:0.9982069820860103\n",
      "train loss:0.9588665205243788\n",
      "train loss:1.0315551925459672\n",
      "train loss:0.9691580938701815\n",
      "train loss:0.8861899671421863\n",
      "train loss:0.8120209262267967\n",
      "train loss:0.811199438659094\n",
      "train loss:0.8854802218728373\n",
      "train loss:0.9753788008669884\n",
      "train loss:0.9724009104702332\n",
      "train loss:0.8155058759459608\n",
      "train loss:0.9088556462914942\n",
      "train loss:0.8965282376902434\n",
      "train loss:0.9456684421317196\n",
      "train loss:0.930795082214831\n",
      "train loss:0.9122045828812463\n",
      "train loss:1.0704446692827743\n",
      "train loss:0.7646306853372427\n",
      "train loss:0.9162742069210732\n",
      "train loss:1.0072920134919834\n",
      "train loss:0.8559805414473236\n",
      "train loss:0.9541152946311933\n",
      "train loss:0.8275434472425267\n",
      "train loss:0.9498006513939733\n",
      "train loss:1.0547397359478083\n",
      "train loss:0.9339291044040317\n",
      "train loss:0.9854446663508806\n",
      "train loss:0.9433053743092092\n",
      "train loss:0.9573287733164874\n",
      "train loss:0.9573118319236792\n",
      "train loss:0.9366027008502777\n",
      "train loss:0.9422447677485438\n",
      "train loss:0.9943700456607815\n",
      "train loss:0.977951422590634\n",
      "train loss:0.8139074733334827\n",
      "train loss:0.9234160305944618\n",
      "train loss:0.8792708611863467\n",
      "train loss:0.9810747838593814\n",
      "train loss:0.7865824423792531\n",
      "train loss:1.011399646649718\n",
      "train loss:0.9447598382782029\n",
      "train loss:0.9179380703293419\n",
      "train loss:0.9794325508807546\n",
      "train loss:1.0713897009914874\n",
      "train loss:0.8514610377488033\n",
      "train loss:0.9145265894819703\n",
      "train loss:0.9599365395557351\n",
      "train loss:1.1203839672934601\n",
      "train loss:0.9129194029963744\n",
      "train loss:0.8825633068137922\n",
      "train loss:0.8750847128584934\n",
      "train loss:0.9895982688451322\n",
      "train loss:0.8888906736877884\n",
      "train loss:0.9628524511148443\n",
      "train loss:0.7407612887343149\n",
      "train loss:0.9519560098712547\n",
      "train loss:1.0365455450166416\n",
      "train loss:0.9012289446644587\n",
      "train loss:0.9658417783100142\n",
      "train loss:1.0422403890785035\n",
      "train loss:0.9019149207168438\n",
      "train loss:0.928918350772197\n",
      "train loss:0.8332020972067308\n",
      "train loss:0.9811314592135817\n",
      "train loss:1.157702993219966\n",
      "train loss:0.6902964122594455\n",
      "train loss:0.9399696456498704\n",
      "=== epoch:7, train acc:0.992, test acc:0.986 ===\n",
      "train loss:0.9921542325873841\n",
      "train loss:0.7723836384490652\n",
      "train loss:0.997253913196624\n",
      "train loss:0.8869034737419824\n",
      "train loss:0.9931932786019411\n",
      "train loss:0.9372036527793584\n",
      "train loss:1.0613922998245677\n",
      "train loss:1.0001319050663644\n",
      "train loss:0.8860996993701662\n",
      "train loss:0.9944757001210677\n",
      "train loss:0.9188565411667219\n",
      "train loss:0.9177063262832627\n",
      "train loss:0.8355109310857364\n",
      "train loss:0.779653086362811\n",
      "train loss:0.9008831472219001\n",
      "train loss:0.951607115580239\n",
      "train loss:0.9539433686108183\n",
      "train loss:1.0787435074642886\n",
      "train loss:0.961225945278579\n",
      "train loss:0.9592380904259847\n",
      "train loss:0.9006939759986987\n",
      "train loss:1.02081465422938\n",
      "train loss:0.7801158835262736\n",
      "train loss:0.8646684254540721\n",
      "train loss:0.8759028672687096\n",
      "train loss:0.8564800513007333\n",
      "train loss:1.033158720530286\n",
      "train loss:0.9020280655011306\n",
      "train loss:0.7826961775379405\n",
      "train loss:1.0622813502094286\n",
      "train loss:0.953418063906512\n",
      "train loss:0.8495368397613902\n",
      "train loss:0.8783177794546673\n",
      "train loss:0.8277379316836537\n",
      "train loss:0.9174545116937659\n",
      "train loss:0.8524672088678726\n",
      "train loss:0.9507063030035094\n",
      "train loss:0.8578700388680103\n",
      "train loss:0.9393710434592323\n",
      "train loss:0.94617607695245\n",
      "train loss:0.8470561620974777\n",
      "train loss:0.8598445645067574\n",
      "train loss:1.046398082512289\n",
      "train loss:0.7612929704402812\n",
      "train loss:0.6640207886178502\n",
      "train loss:0.9106752838869344\n",
      "train loss:0.9810994910771901\n",
      "train loss:1.0312298314887216\n",
      "train loss:1.0866754660842448\n",
      "train loss:1.0037087638650077\n",
      "train loss:0.8398753931106785\n",
      "train loss:0.8639735192711525\n",
      "train loss:0.7737395302966206\n",
      "train loss:1.0404483154759026\n",
      "train loss:0.870115484061147\n",
      "train loss:1.0569063202391964\n",
      "train loss:0.9853567528616228\n",
      "train loss:1.0420869659338117\n",
      "train loss:0.8481464946148799\n",
      "train loss:0.6630923857795752\n",
      "train loss:0.9519362340513631\n",
      "train loss:0.9821850462174467\n",
      "train loss:1.015702149812441\n",
      "train loss:1.1659137281352394\n",
      "train loss:0.9033661677766297\n",
      "train loss:0.9687623720218893\n",
      "train loss:0.8897752216882623\n",
      "train loss:0.8001376562033435\n",
      "train loss:0.7799370563627726\n",
      "train loss:1.0404143316265215\n",
      "train loss:0.847381667756295\n",
      "train loss:0.935802799197345\n",
      "train loss:0.8099300227194327\n",
      "train loss:0.8302049246717287\n",
      "train loss:0.9819983839771027\n",
      "train loss:0.7852754689299343\n",
      "train loss:0.9080987855200624\n",
      "train loss:0.8120222560423689\n",
      "train loss:1.1708650230823308\n",
      "train loss:0.9594228766262332\n",
      "train loss:0.7429817583178594\n",
      "train loss:0.9811395726989234\n",
      "train loss:0.8370910955319973\n",
      "train loss:0.8990987372266788\n",
      "train loss:0.9418172877394198\n",
      "train loss:0.8313784510856886\n",
      "train loss:0.9230003843585761\n",
      "train loss:1.0066250478351406\n",
      "train loss:0.9310774750987919\n",
      "train loss:1.1285731369437746\n",
      "train loss:0.8135713431986258\n",
      "train loss:0.9286781719839956\n",
      "train loss:0.8319766053698561\n",
      "train loss:0.8462721750167007\n",
      "train loss:0.9797323659933658\n",
      "train loss:0.936614648745659\n",
      "train loss:0.9032613938394577\n",
      "train loss:0.872092173234012\n",
      "train loss:0.8485092446189465\n",
      "train loss:0.8862874468818263\n",
      "train loss:0.8900027449046221\n",
      "train loss:0.7902066244052334\n",
      "train loss:0.9787763843971892\n",
      "train loss:1.0451118297727195\n",
      "train loss:1.0710609124004928\n",
      "train loss:0.9062577791845341\n",
      "train loss:0.9150206362820709\n",
      "train loss:0.8803060195032684\n",
      "train loss:0.9494841585568541\n",
      "train loss:0.8541449412185034\n",
      "train loss:0.806663732055494\n",
      "train loss:0.8488114827934283\n",
      "train loss:1.0555274722338202\n",
      "train loss:0.8735253134690488\n",
      "train loss:0.8799553589040546\n",
      "train loss:0.9957781841928862\n",
      "train loss:0.9614317524461515\n",
      "train loss:0.8988204474102267\n",
      "train loss:0.8461245270649367\n",
      "train loss:0.973070719919468\n",
      "train loss:0.9910208773968522\n",
      "train loss:0.8352714837938735\n",
      "train loss:0.944781154538013\n",
      "train loss:0.8580944011516162\n",
      "train loss:0.878589044787418\n",
      "train loss:0.8024115856062478\n",
      "train loss:0.8849499995403906\n",
      "train loss:0.8223198754068219\n",
      "train loss:0.8507813674572523\n",
      "train loss:1.102057929417499\n",
      "train loss:0.8538997688157807\n",
      "train loss:1.0170701971594154\n",
      "train loss:0.8832215541810007\n",
      "train loss:1.0054710473614694\n",
      "train loss:0.9008469538983674\n",
      "train loss:0.9908856402196428\n",
      "train loss:0.8778583401910821\n",
      "train loss:0.8671332961831285\n",
      "train loss:0.9865231705264302\n",
      "train loss:0.9886277130769328\n",
      "train loss:0.8690638353333588\n",
      "train loss:0.9410827682288613\n",
      "train loss:1.0133584433426999\n",
      "train loss:0.9508650535631135\n",
      "train loss:0.7752284598365751\n",
      "train loss:0.8110524303008466\n",
      "train loss:0.8462553348158914\n",
      "train loss:1.0300936804887195\n",
      "train loss:0.934166265120568\n",
      "train loss:0.7881796243921438\n",
      "train loss:0.9285699271891497\n",
      "train loss:0.9317423838088846\n",
      "train loss:0.9667385598868603\n",
      "train loss:0.8359434084699906\n",
      "train loss:0.8536192244244301\n",
      "train loss:1.0232349327057981\n",
      "train loss:0.8068752095594859\n",
      "train loss:0.8913262007589106\n",
      "train loss:1.0066416957637023\n",
      "train loss:0.870764368080809\n",
      "train loss:0.9311741050461054\n",
      "train loss:0.8400685012840525\n",
      "train loss:0.8712050264671253\n",
      "train loss:0.8042110690345232\n",
      "train loss:1.0696435255883703\n",
      "train loss:0.98516580426182\n",
      "train loss:1.0340082747127533\n",
      "train loss:0.9407760468339751\n",
      "train loss:0.8763931747309037\n",
      "train loss:0.9695861743134525\n",
      "train loss:0.8305607443840384\n",
      "train loss:0.7779729862666876\n",
      "train loss:1.0365221652243972\n",
      "train loss:0.9903206403752847\n",
      "train loss:0.9819152306985087\n",
      "train loss:0.9705737306643956\n",
      "train loss:1.0044465876948316\n",
      "train loss:0.8838008050303546\n",
      "train loss:0.9683640301816792\n",
      "train loss:0.8943747318736905\n",
      "train loss:0.8386280869527721\n",
      "train loss:0.8963484617137152\n",
      "train loss:0.9548495265874679\n",
      "train loss:0.9049940782089362\n",
      "train loss:0.7998163256718134\n",
      "train loss:0.9302314618792706\n",
      "train loss:0.9611960644742088\n",
      "train loss:0.9730391674832704\n",
      "train loss:0.8802505015940558\n",
      "train loss:1.0721433295502454\n",
      "train loss:1.098794232770215\n",
      "train loss:0.8570468720595139\n",
      "train loss:0.7939040873138984\n",
      "train loss:0.8751142750358001\n",
      "train loss:0.8381251759059434\n",
      "train loss:0.8954755673529821\n",
      "train loss:0.8837917209721042\n",
      "train loss:0.9909091893099936\n",
      "train loss:0.9281452740535614\n",
      "train loss:0.8477057969697317\n",
      "train loss:0.9710067271033664\n",
      "train loss:0.9957337983411633\n",
      "train loss:0.8787604257194656\n",
      "train loss:0.9535588270209056\n",
      "train loss:0.8481512942912031\n",
      "train loss:0.7970660581907986\n",
      "train loss:1.129968524194756\n",
      "train loss:0.8870556993303115\n",
      "train loss:0.9433586281309158\n",
      "train loss:0.8592264220820592\n",
      "train loss:0.9513171123226267\n",
      "train loss:0.8869734153907375\n",
      "train loss:0.9603398822259795\n",
      "train loss:0.8538511079184322\n",
      "train loss:0.8906949233640628\n",
      "train loss:1.0123189812017173\n",
      "train loss:0.9175986536939296\n",
      "train loss:1.0767636607625994\n",
      "train loss:0.9421981365379701\n",
      "train loss:1.0691039163400646\n",
      "train loss:1.0332025620453593\n",
      "train loss:0.9558233132282794\n",
      "train loss:0.8886509878501337\n",
      "train loss:0.8674834839297944\n",
      "train loss:0.9260952886851658\n",
      "train loss:1.011060015035341\n",
      "train loss:0.9225912314998713\n",
      "train loss:0.8072939366539424\n",
      "train loss:0.861982436601304\n",
      "train loss:0.834041104311588\n",
      "train loss:0.8882157113645656\n",
      "train loss:0.969261151113344\n",
      "train loss:0.9184193162188677\n",
      "train loss:0.7885753290990237\n",
      "train loss:0.9083363096347716\n",
      "train loss:0.8974645499686303\n",
      "train loss:0.9287734406262242\n",
      "train loss:0.8270916383456387\n",
      "train loss:0.8945658204460025\n",
      "train loss:0.9666361423037446\n",
      "train loss:0.8837885159368546\n",
      "train loss:0.9804072911812405\n",
      "train loss:1.017602648887013\n",
      "train loss:0.8551044891096086\n",
      "train loss:0.9108098888553521\n",
      "train loss:0.9357419042257673\n",
      "train loss:1.0355453904108807\n",
      "train loss:0.8108667615006153\n",
      "train loss:0.9750825015544171\n",
      "train loss:0.8583730631755989\n",
      "train loss:0.9074270738614\n",
      "train loss:0.7757021496324331\n",
      "train loss:0.7402580998955\n",
      "train loss:0.9554283720912738\n",
      "train loss:0.8216242523283301\n",
      "train loss:0.8767692297201648\n",
      "train loss:1.074651097958591\n",
      "train loss:0.7860925682056107\n",
      "train loss:0.8417819209884173\n",
      "train loss:0.9353664586933117\n",
      "train loss:0.9613242086299246\n",
      "train loss:0.8695191771206457\n",
      "train loss:0.8601252474247549\n",
      "train loss:0.9033618193511492\n",
      "train loss:0.8073041479036701\n",
      "train loss:0.8587170611208187\n",
      "train loss:0.9364392620305004\n",
      "train loss:0.948226070269812\n",
      "train loss:1.0222021495510567\n",
      "train loss:0.7953552881572543\n",
      "train loss:0.8989162011894123\n",
      "train loss:0.9734995313538576\n",
      "train loss:1.049206897548843\n",
      "train loss:1.0123618601664366\n",
      "train loss:0.9499801661824863\n",
      "train loss:1.0221371166348063\n",
      "train loss:1.0323487557296944\n",
      "train loss:0.8466876057714452\n",
      "train loss:0.8490383166406862\n",
      "train loss:0.9327976809476387\n",
      "train loss:0.9688345997274308\n",
      "train loss:0.9956291123881229\n",
      "train loss:0.9431141370480735\n",
      "train loss:0.9634280388011086\n",
      "train loss:1.0107748091912485\n",
      "train loss:0.9083682543624388\n",
      "train loss:0.9909247461458688\n",
      "train loss:0.8337322253244455\n",
      "train loss:0.9443230198261368\n",
      "train loss:0.9300316443561725\n",
      "train loss:1.0488577000779111\n",
      "train loss:0.93694028784524\n",
      "train loss:0.9900780249128525\n",
      "train loss:0.9323626029511753\n",
      "train loss:0.850700264615368\n",
      "train loss:0.9680210855966197\n",
      "train loss:0.8773487451958145\n",
      "train loss:0.8809335915752133\n",
      "train loss:0.8499856984665806\n",
      "train loss:1.0342968918467281\n",
      "train loss:0.918819196605919\n",
      "train loss:0.8283695977443715\n",
      "train loss:0.7924194334652986\n",
      "train loss:0.9986266728635868\n",
      "train loss:0.996491868803984\n",
      "train loss:0.8627836096106003\n",
      "train loss:0.9188991975272113\n",
      "train loss:0.9554936513307881\n",
      "train loss:1.0359859124670212\n",
      "train loss:0.893674831367765\n",
      "train loss:0.7881923674305125\n",
      "train loss:0.9287921792082398\n",
      "train loss:0.8795866789201827\n",
      "train loss:0.9664871553432981\n",
      "train loss:0.7447215943081986\n",
      "train loss:0.908425882733513\n",
      "train loss:0.8405374437739966\n",
      "train loss:0.8468280531237781\n",
      "train loss:0.8721361474844739\n",
      "train loss:0.9394554613070295\n",
      "train loss:0.8679736704677904\n",
      "train loss:0.9579313315235608\n",
      "train loss:0.7646929958607385\n",
      "train loss:0.9974842256281949\n",
      "train loss:0.940977652069573\n",
      "train loss:0.9075996397869801\n",
      "train loss:1.0638803497459035\n",
      "train loss:1.132741936612056\n",
      "train loss:0.9310947760814277\n",
      "train loss:1.0207895567389944\n",
      "train loss:0.6733242214883428\n",
      "train loss:0.8068076010314189\n",
      "train loss:0.8329073577554541\n",
      "train loss:0.8958016197617364\n",
      "train loss:1.0194466303552476\n",
      "train loss:0.7626729528272346\n",
      "train loss:1.0135333580758545\n",
      "train loss:1.039858482855496\n",
      "train loss:0.9813592909956741\n",
      "train loss:0.8764443435707595\n",
      "train loss:0.9271472107052479\n",
      "train loss:0.9233340779712934\n",
      "train loss:0.9193957335504025\n",
      "train loss:0.8603458281750739\n",
      "train loss:1.0477738346270578\n",
      "train loss:0.9196696493843395\n",
      "train loss:0.9942150162272023\n",
      "train loss:0.8996956856548479\n",
      "train loss:0.9104441440657509\n",
      "train loss:0.8312221061208372\n",
      "train loss:0.891061211172361\n",
      "train loss:0.6779307674059564\n",
      "train loss:1.0089883017907837\n",
      "train loss:0.9195804544135479\n",
      "train loss:0.9671954002962541\n",
      "train loss:0.8771661636757565\n",
      "train loss:0.8064328252711745\n",
      "train loss:0.8703229283366731\n",
      "train loss:0.6913946574165053\n",
      "train loss:0.907670192613891\n",
      "train loss:1.000284277756645\n",
      "train loss:0.7675024586445579\n",
      "train loss:0.9401384712863268\n",
      "train loss:0.9528052027184759\n",
      "train loss:0.9591899045823609\n",
      "train loss:0.678446182294879\n",
      "train loss:0.9807903304635704\n",
      "train loss:0.806991741271111\n",
      "train loss:0.8998080987711389\n",
      "train loss:0.8765979541553587\n",
      "train loss:0.9102391191333633\n",
      "train loss:0.9640139233163013\n",
      "train loss:0.9307577450198786\n",
      "train loss:0.9218776834420077\n",
      "train loss:1.0135897720729514\n",
      "train loss:0.800159379432313\n",
      "train loss:0.9169712177363998\n",
      "train loss:0.8235749213577455\n",
      "train loss:0.78906069506837\n",
      "train loss:0.8644440665246073\n",
      "train loss:0.9200841946233764\n",
      "train loss:0.8376180815758988\n",
      "train loss:0.872896233434288\n",
      "train loss:1.062534054493436\n",
      "train loss:0.8758515324493144\n",
      "train loss:0.7278310555443764\n",
      "train loss:0.9532853215886667\n",
      "train loss:0.8361338540378993\n",
      "train loss:0.7095012849432036\n",
      "train loss:0.846737794293892\n",
      "train loss:1.0398665158521987\n",
      "train loss:0.9418802055697651\n",
      "train loss:0.8785381796281964\n",
      "train loss:1.031384420310379\n",
      "train loss:0.9312102998757531\n",
      "train loss:0.8745825122426087\n",
      "train loss:0.880348699333708\n",
      "train loss:0.8420256647948652\n",
      "train loss:0.9644071377813965\n",
      "train loss:1.0305299108022463\n",
      "train loss:0.8745430500385342\n",
      "train loss:0.8096775644409142\n",
      "train loss:0.945066896382609\n",
      "train loss:0.9318129464277846\n",
      "train loss:0.8523030107123541\n",
      "train loss:0.8245244820692477\n",
      "train loss:0.8822428059956949\n",
      "train loss:1.0984185675135276\n",
      "train loss:0.8884480672871115\n",
      "train loss:0.7915599219166516\n",
      "train loss:0.995567979679966\n",
      "train loss:0.9492048325264161\n",
      "train loss:0.9843194436689565\n",
      "train loss:0.9708129366404019\n",
      "train loss:0.8448466096920619\n",
      "train loss:0.9279358796684413\n",
      "train loss:1.0448532513714417\n",
      "train loss:1.0410720661439585\n",
      "train loss:0.8538980006802447\n",
      "train loss:0.9331491598150258\n",
      "train loss:0.9779586009017431\n",
      "train loss:1.0724816534893196\n",
      "train loss:0.9860712236252469\n",
      "train loss:0.7173954827717103\n",
      "train loss:0.8937668146188403\n",
      "train loss:0.9523896531369977\n",
      "train loss:0.9180716517526164\n",
      "train loss:0.9483194938337656\n",
      "train loss:1.114948792734006\n",
      "train loss:0.8519884470416297\n",
      "train loss:1.0069404371669817\n",
      "train loss:0.9253026916094731\n",
      "train loss:0.8509312835572392\n",
      "train loss:0.8222797741462209\n",
      "train loss:0.9714046505583341\n",
      "train loss:0.6874261836141554\n",
      "train loss:0.9371369720030626\n",
      "train loss:0.8937806215478701\n",
      "train loss:0.985717051509955\n",
      "train loss:0.7837410317276159\n",
      "train loss:0.9745375383984016\n",
      "train loss:0.8985880138152545\n",
      "train loss:0.7678441790501297\n",
      "train loss:0.8328172554792296\n",
      "train loss:0.7255885171420582\n",
      "train loss:0.7090629472263514\n",
      "train loss:0.9794586864741143\n",
      "train loss:1.1084595015528003\n",
      "train loss:0.7982649033986796\n",
      "train loss:0.8164689255063664\n",
      "train loss:0.9367749845758955\n",
      "train loss:0.7500687997821331\n",
      "train loss:0.9938853801520598\n",
      "train loss:0.8799261180640008\n",
      "train loss:1.0893482896413693\n",
      "train loss:0.9847021478404165\n",
      "train loss:0.8296212583206153\n",
      "train loss:1.0013492047620214\n",
      "train loss:0.7458579251472037\n",
      "train loss:0.7923207405145153\n",
      "train loss:0.9559863477652709\n",
      "train loss:0.906515841877131\n",
      "train loss:0.8561759543847685\n",
      "train loss:0.8876233869522651\n",
      "train loss:0.781635172192138\n",
      "train loss:1.0326323650210911\n",
      "train loss:0.8127416665590493\n",
      "train loss:0.83191186305075\n",
      "train loss:0.8580615597159477\n",
      "train loss:0.8598000588734347\n",
      "train loss:0.8094672806074567\n",
      "train loss:0.8986039379714803\n",
      "train loss:0.80097853665763\n",
      "train loss:0.9244813359496361\n",
      "train loss:0.8940809758383327\n",
      "train loss:0.9128891790954838\n",
      "train loss:0.9192708811675105\n",
      "train loss:0.9113073267752302\n",
      "train loss:0.8652319368703264\n",
      "train loss:1.0957939513620822\n",
      "train loss:1.0321666220582524\n",
      "train loss:0.9545567248939083\n",
      "train loss:0.9071001861891539\n",
      "train loss:0.9681859421085768\n",
      "train loss:0.9160297622486065\n",
      "train loss:0.8924943863385981\n",
      "train loss:0.8402051192089806\n",
      "train loss:0.9239520152689904\n",
      "train loss:0.9245625520945709\n",
      "train loss:0.7583729801924923\n",
      "train loss:0.8697369501824417\n",
      "train loss:0.8808031028194216\n",
      "train loss:1.0777621515500075\n",
      "train loss:0.8137505275412975\n",
      "train loss:0.8692456588343613\n",
      "train loss:0.9959642196556708\n",
      "train loss:0.9261744896431189\n",
      "train loss:1.05585540160769\n",
      "train loss:0.816482043165117\n",
      "train loss:0.7634948659851986\n",
      "train loss:0.9127417141547042\n",
      "train loss:0.9812302456365916\n",
      "train loss:0.8837024066001951\n",
      "train loss:0.9450334585522452\n",
      "train loss:0.8138219043676784\n",
      "train loss:0.7853193950970835\n",
      "train loss:0.7989692881959145\n",
      "train loss:0.7677702678704061\n",
      "train loss:0.9435093497899607\n",
      "train loss:1.0381546810562607\n",
      "train loss:0.8419462262109646\n",
      "train loss:0.8588702628014677\n",
      "train loss:0.8529362402120619\n",
      "train loss:0.7283791209017061\n",
      "train loss:0.8078644804557824\n",
      "train loss:0.8507938958727835\n",
      "train loss:0.8631020455532139\n",
      "train loss:0.7281637559335141\n",
      "train loss:0.9108793109207955\n",
      "train loss:0.8256295103082423\n",
      "train loss:0.9502484399641321\n",
      "train loss:0.7839868381185573\n",
      "train loss:0.9052283834725272\n",
      "train loss:0.882817843156263\n",
      "train loss:0.8345169305375033\n",
      "train loss:0.9961318648205988\n",
      "train loss:0.8621936890219518\n",
      "train loss:0.8105078346605584\n",
      "train loss:0.7546789420248762\n",
      "train loss:0.9374811393482801\n",
      "train loss:0.9119055585143393\n",
      "train loss:0.9671016958337213\n",
      "train loss:1.0007311689533642\n",
      "train loss:1.1105895751142874\n",
      "train loss:0.751438984858956\n",
      "train loss:0.7649971185878087\n",
      "train loss:0.9831921096790249\n",
      "train loss:0.8616585269531737\n",
      "train loss:0.8594026969516045\n",
      "train loss:0.7874397894960502\n",
      "train loss:0.8704966001343055\n",
      "train loss:0.929103064777752\n",
      "train loss:0.8547855748810966\n",
      "train loss:0.7518562441123205\n",
      "train loss:0.9123594159977105\n",
      "train loss:1.1119228820274705\n",
      "train loss:0.9359832168151502\n",
      "train loss:0.8372404443193162\n",
      "train loss:0.7796329619981117\n",
      "train loss:0.7786111444834218\n",
      "train loss:0.8789091289541744\n",
      "train loss:0.9124628245146198\n",
      "train loss:0.8878416660197612\n",
      "train loss:0.8061385120314273\n",
      "train loss:0.895117467506426\n",
      "train loss:0.8500311756090784\n",
      "train loss:0.9797502937110784\n",
      "train loss:0.992856545305753\n",
      "train loss:1.0855567732813551\n",
      "train loss:0.9387662194104227\n",
      "train loss:0.8463884000046918\n",
      "train loss:0.9879144214987758\n",
      "train loss:0.8790398960701866\n",
      "train loss:0.9071157718543956\n",
      "train loss:0.9483246885537089\n",
      "train loss:0.8147192923218818\n",
      "train loss:1.0045447865922748\n",
      "train loss:0.7352950765506308\n",
      "train loss:0.8720585197539189\n",
      "train loss:0.9421921626631068\n",
      "train loss:0.8584376010923315\n",
      "train loss:0.9115228088915517\n",
      "train loss:0.883684214422741\n",
      "train loss:0.8867255843363886\n",
      "train loss:1.0460741142228516\n",
      "train loss:0.7887163389470261\n",
      "train loss:0.9730431978056429\n",
      "train loss:1.048133253847318\n",
      "train loss:0.9216040349042082\n",
      "train loss:0.9286768753489875\n",
      "train loss:0.9343196248582025\n",
      "train loss:1.0032495549419096\n",
      "train loss:0.9568412216772313\n",
      "train loss:0.9200234574646537\n",
      "train loss:1.0042025384013191\n",
      "train loss:0.8794193205805133\n",
      "train loss:0.9140699710361245\n",
      "train loss:1.0030510501297514\n",
      "train loss:0.9254096563925451\n",
      "train loss:0.9085876768430038\n",
      "train loss:0.8191785342891714\n",
      "train loss:0.6799803271105361\n",
      "train loss:1.0227908218118906\n",
      "train loss:0.959644785100979\n",
      "train loss:0.8649705661204673\n",
      "train loss:0.7166045507232132\n",
      "train loss:1.0672805821753801\n",
      "train loss:0.9863444976999658\n",
      "train loss:0.8820331462387822\n",
      "train loss:1.0638224202449467\n",
      "=== epoch:8, train acc:0.996, test acc:0.992 ===\n",
      "train loss:0.847284433178951\n",
      "train loss:0.6632028516051647\n",
      "train loss:0.8588797010401248\n",
      "train loss:1.010609484135985\n",
      "train loss:0.8562809663606801\n",
      "train loss:0.9270457142879129\n",
      "train loss:0.9788266675226542\n",
      "train loss:0.8874880003507232\n",
      "train loss:0.8940148099380463\n",
      "train loss:0.822774876675118\n",
      "train loss:0.9421202331114219\n",
      "train loss:1.0234058752448223\n",
      "train loss:0.7771466477781118\n",
      "train loss:0.9008044228955786\n",
      "train loss:0.941421903170089\n",
      "train loss:0.8903660866571803\n",
      "train loss:0.9222457010306985\n",
      "train loss:0.9981476537749667\n",
      "train loss:0.9249652556665233\n",
      "train loss:0.9403718896206341\n",
      "train loss:1.0116684446993047\n",
      "train loss:0.9544599745443396\n",
      "train loss:0.9003295734794016\n",
      "train loss:0.9589074118876444\n",
      "train loss:0.8318255712608308\n",
      "train loss:1.0153240678029414\n",
      "train loss:0.9537355888227197\n",
      "train loss:0.9008448275228723\n",
      "train loss:1.1001620289328213\n",
      "train loss:1.033012241320603\n",
      "train loss:0.8576203717384306\n",
      "train loss:0.8712854980269826\n",
      "train loss:0.9360966502992946\n",
      "train loss:0.955189781600685\n",
      "train loss:0.8959251375681041\n",
      "train loss:0.8952346820277962\n",
      "train loss:0.9531894867441123\n",
      "train loss:0.889990422732821\n",
      "train loss:0.6886746803224106\n",
      "train loss:0.7383115978988621\n",
      "train loss:0.9123148682910143\n",
      "train loss:0.8093409387613422\n",
      "train loss:0.9120851342726952\n",
      "train loss:0.8571041966582044\n",
      "train loss:0.8700337659967243\n",
      "train loss:0.7921648979644982\n",
      "train loss:0.8743615034938963\n",
      "train loss:0.8841321098724124\n",
      "train loss:0.8964391167719711\n",
      "train loss:1.154906197635964\n",
      "train loss:0.861877264011023\n",
      "train loss:1.0972325717220344\n",
      "train loss:0.7962138790194802\n",
      "train loss:0.9740109166638762\n",
      "train loss:0.9760543048800235\n",
      "train loss:0.9383206109118774\n",
      "train loss:0.8401446159747905\n",
      "train loss:0.9779397492472042\n",
      "train loss:0.9528579309916347\n",
      "train loss:0.7418632995662194\n",
      "train loss:0.9306607253460103\n",
      "train loss:0.8551071393858343\n",
      "train loss:0.8979941501899178\n",
      "train loss:0.8018792396581057\n",
      "train loss:0.937257355798902\n",
      "train loss:0.8669146210690477\n",
      "train loss:0.8759660568481397\n",
      "train loss:0.9647380793885916\n",
      "train loss:0.9591959837160399\n",
      "train loss:0.9110520824885758\n",
      "train loss:0.8365762089360825\n",
      "train loss:0.9484341950689296\n",
      "train loss:0.7565479595715046\n",
      "train loss:0.7043961263653422\n",
      "train loss:0.996135310584069\n",
      "train loss:0.8828953990204208\n",
      "train loss:1.0109777736097412\n",
      "train loss:0.8549481087949161\n",
      "train loss:0.9424387057669067\n",
      "train loss:0.8635794751777917\n",
      "train loss:0.9292001674642245\n",
      "train loss:0.8561931104699909\n",
      "train loss:0.9583023277976664\n",
      "train loss:0.9736080331238065\n",
      "train loss:0.9855823828190389\n",
      "train loss:0.8157439963004272\n",
      "train loss:0.9136348699495181\n",
      "train loss:0.9216446542128489\n",
      "train loss:0.9616903247054637\n",
      "train loss:0.9529383903792088\n",
      "train loss:0.9605909737502745\n",
      "train loss:1.0678157607617778\n",
      "train loss:1.1741887741480244\n",
      "train loss:0.9004030868499285\n",
      "train loss:0.9694481595693786\n",
      "train loss:1.0109238819448803\n",
      "train loss:0.9721996741723123\n",
      "train loss:0.7969115390422213\n",
      "train loss:0.9459775859527254\n",
      "train loss:0.9679947740321966\n",
      "train loss:0.8599146790955716\n",
      "train loss:1.0888577085277598\n",
      "train loss:0.9322678289169711\n",
      "train loss:0.6600477200918036\n",
      "train loss:0.846154765805472\n",
      "train loss:1.0568226028667511\n",
      "train loss:0.8668442356731192\n",
      "train loss:0.9032385313169116\n",
      "train loss:1.0165707854438448\n",
      "train loss:0.9485813362717532\n",
      "train loss:0.9020095985056089\n",
      "train loss:0.8277991100540301\n",
      "train loss:0.9119221231658284\n",
      "train loss:0.8498461446647273\n",
      "train loss:0.8440673540624494\n",
      "train loss:0.9012538315927743\n",
      "train loss:0.9346554635805902\n",
      "train loss:0.8526222108539265\n",
      "train loss:0.9287250255440282\n",
      "train loss:0.8657683352177689\n",
      "train loss:0.8607820235158471\n",
      "train loss:0.9820263373836043\n",
      "train loss:1.0020872291214418\n",
      "train loss:0.8525091311514383\n",
      "train loss:0.9295090706608778\n",
      "train loss:0.729530450828063\n",
      "train loss:0.8662147668751851\n",
      "train loss:0.8411283905415695\n",
      "train loss:0.7307168075470533\n",
      "train loss:1.0800304924198039\n",
      "train loss:1.0837930308132824\n",
      "train loss:0.984681072419313\n",
      "train loss:0.883932517069271\n",
      "train loss:0.9134924573496128\n",
      "train loss:0.9138425062374488\n",
      "train loss:0.903712576138641\n",
      "train loss:0.8545872000051965\n",
      "train loss:0.9088162833219368\n",
      "train loss:0.8597906296587533\n",
      "train loss:0.7971096672655017\n",
      "train loss:0.9341606333076586\n",
      "train loss:1.0114221927639837\n",
      "train loss:0.8372855203124042\n",
      "train loss:0.872089912755889\n",
      "train loss:0.8671835306010308\n",
      "train loss:0.7600561059408464\n",
      "train loss:0.8964973741324787\n",
      "train loss:0.8630181973281104\n",
      "train loss:0.975219493639033\n",
      "train loss:0.8621761515914165\n",
      "train loss:0.9208801329245537\n",
      "train loss:0.9654198769846283\n",
      "train loss:0.931423516645133\n",
      "train loss:0.8790794909890022\n",
      "train loss:0.9750139209311994\n",
      "train loss:1.139049366287269\n",
      "train loss:0.8186403352517893\n",
      "train loss:0.9601820421365995\n",
      "train loss:0.8803465421569012\n",
      "train loss:0.7584712698850901\n",
      "train loss:0.9367935545301443\n",
      "train loss:0.8784045197066926\n",
      "train loss:0.9608130638631716\n",
      "train loss:1.0611778076268943\n",
      "train loss:0.782302946016239\n",
      "train loss:0.8238234880271397\n",
      "train loss:1.0075351484722226\n",
      "train loss:0.9674174391375385\n",
      "train loss:0.9566863316033816\n",
      "train loss:0.8322425185909522\n",
      "train loss:0.8219903363907451\n",
      "train loss:0.9227835712677914\n",
      "train loss:1.0879919941196048\n",
      "train loss:0.733995808323461\n",
      "train loss:0.944345375486435\n",
      "train loss:0.8837687688261078\n",
      "train loss:1.1542744862516676\n",
      "train loss:0.8760896478626296\n",
      "train loss:0.8898385302710036\n",
      "train loss:0.9276614429025561\n",
      "train loss:0.9001322881604054\n",
      "train loss:0.9233213964939094\n",
      "train loss:0.72290275886823\n",
      "train loss:0.9430794494932271\n",
      "train loss:0.9433917226745381\n",
      "train loss:0.9426315071032347\n",
      "train loss:0.9207527352449923\n",
      "train loss:0.7117702973192052\n",
      "train loss:1.1170621380259553\n",
      "train loss:0.8359368737800973\n",
      "train loss:0.8827670399165839\n",
      "train loss:0.8757086976196355\n",
      "train loss:1.0088777620064695\n",
      "train loss:0.8702230261527256\n",
      "train loss:0.8284200474634931\n",
      "train loss:0.9630121394864207\n",
      "train loss:1.128746022412825\n",
      "train loss:0.7441933221891527\n",
      "train loss:1.064369298274399\n",
      "train loss:0.8925492495812588\n",
      "train loss:0.8746544703136988\n",
      "train loss:0.8013154015268783\n",
      "train loss:0.8378660498165645\n",
      "train loss:0.9348689509380117\n",
      "train loss:1.053440996820529\n",
      "train loss:0.7834251898607502\n",
      "train loss:0.8738295215257995\n",
      "train loss:0.9610848367868008\n",
      "train loss:0.7741274419088385\n",
      "train loss:0.7391012503168973\n",
      "train loss:0.779958419161769\n",
      "train loss:1.034050462800549\n",
      "train loss:0.8097235459777692\n",
      "train loss:0.955958512158962\n",
      "train loss:0.8356830588680532\n",
      "train loss:0.8656582633694125\n",
      "train loss:0.8738735945390615\n",
      "train loss:0.9470240212232447\n",
      "train loss:0.9278689133569682\n",
      "train loss:0.9464837198209622\n",
      "train loss:0.8137370359035984\n",
      "train loss:0.8172803363111872\n",
      "train loss:0.896208849562217\n",
      "train loss:0.9711492652792271\n",
      "train loss:1.005377184008526\n",
      "train loss:0.948702717774634\n",
      "train loss:0.9322657801733962\n",
      "train loss:0.8874042707198815\n",
      "train loss:0.752546962254526\n",
      "train loss:0.9767419446249519\n",
      "train loss:0.8909974948104648\n",
      "train loss:0.7822311850638353\n",
      "train loss:0.858876221707921\n",
      "train loss:0.7262859430934405\n",
      "train loss:0.7867171603110847\n",
      "train loss:0.8478792330685775\n",
      "train loss:0.7476779050606814\n",
      "train loss:0.8633940263772385\n",
      "train loss:1.0485814505132056\n",
      "train loss:0.9057709005641524\n",
      "train loss:0.8292754594219224\n",
      "train loss:0.8815937972678322\n",
      "train loss:0.8999786718319441\n",
      "train loss:0.8952969980574507\n",
      "train loss:0.8174796844536069\n",
      "train loss:0.8481199022096743\n",
      "train loss:0.853684667161926\n",
      "train loss:0.9360371416612356\n",
      "train loss:0.873349486437523\n",
      "train loss:0.866096945421462\n",
      "train loss:0.8070232552969229\n",
      "train loss:0.8320376789885269\n",
      "train loss:0.8777040639026328\n",
      "train loss:0.9609240858882263\n",
      "train loss:0.9903290122915158\n",
      "train loss:0.9548297022451727\n",
      "train loss:0.8424699844966332\n",
      "train loss:0.8006988375807965\n",
      "train loss:0.8010242758360832\n",
      "train loss:0.8467334159413734\n",
      "train loss:0.992670727940668\n",
      "train loss:0.8936681025514072\n",
      "train loss:0.9425048445164177\n",
      "train loss:0.9336887952021706\n",
      "train loss:0.8789600706332018\n",
      "train loss:0.81860356270099\n",
      "train loss:0.8680550538322742\n",
      "train loss:0.8542887555657583\n",
      "train loss:0.947263925875311\n",
      "train loss:0.9338030996279651\n",
      "train loss:0.9500536088476025\n",
      "train loss:1.0972313197975427\n",
      "train loss:1.0931866279690647\n",
      "train loss:0.801762103349488\n",
      "train loss:0.877796506324905\n",
      "train loss:0.8869710160137125\n",
      "train loss:0.8198755658806242\n",
      "train loss:0.791661218586545\n",
      "train loss:0.8112284190107764\n",
      "train loss:0.9821044950057609\n",
      "train loss:1.0096953051139617\n",
      "train loss:0.967678203109085\n",
      "train loss:0.8302169278413984\n",
      "train loss:0.9001583021082893\n",
      "train loss:0.9439483255788771\n",
      "train loss:0.9350011264476553\n",
      "train loss:0.9641574619958279\n",
      "train loss:0.9524321728875185\n",
      "train loss:1.0756492175787364\n",
      "train loss:0.9954545696762261\n",
      "train loss:0.8858847150597653\n",
      "train loss:0.9294358915569417\n",
      "train loss:0.7928625084254233\n",
      "train loss:0.8955993916751578\n",
      "train loss:0.8171754098218836\n",
      "train loss:1.0074969166081833\n",
      "train loss:0.9144936856830888\n",
      "train loss:0.9307199526120056\n",
      "train loss:1.0055269825036965\n",
      "train loss:1.0306666462428924\n",
      "train loss:0.9378668666782406\n",
      "train loss:0.8561589668821254\n",
      "train loss:0.9469486497905667\n",
      "train loss:0.7397436409557179\n",
      "train loss:1.0181701981421778\n",
      "train loss:0.7886031063913541\n",
      "train loss:0.8446715051493107\n",
      "train loss:0.8715543460818184\n",
      "train loss:0.8495166200823385\n",
      "train loss:0.8308154809295304\n",
      "train loss:1.0146420131772735\n",
      "train loss:0.7780388993752481\n",
      "train loss:0.9443670930666128\n",
      "train loss:1.1204891799812906\n",
      "train loss:0.8135566198326317\n",
      "train loss:1.0480536537721683\n",
      "train loss:1.0616055356942957\n",
      "train loss:0.8809801212258396\n",
      "train loss:0.8230306025469903\n",
      "train loss:0.9066757802355754\n",
      "train loss:0.7794338389562541\n",
      "train loss:1.0448587461262269\n",
      "train loss:0.9007687785491588\n",
      "train loss:0.8562204372730136\n",
      "train loss:0.9131155393581192\n",
      "train loss:0.947995103726082\n",
      "train loss:0.9700627446013077\n",
      "train loss:0.8539174719056968\n",
      "train loss:0.79177718038977\n",
      "train loss:0.8803871103800521\n",
      "train loss:0.8687174904775314\n",
      "train loss:1.031880919640567\n",
      "train loss:0.7245069934216953\n",
      "train loss:0.966747847479144\n",
      "train loss:0.9255415942827132\n",
      "train loss:0.9044978026912821\n",
      "train loss:0.9987939710627132\n",
      "train loss:0.909070425415901\n",
      "train loss:0.9274736018260288\n",
      "train loss:1.063285281988769\n",
      "train loss:0.9400823448210338\n",
      "train loss:0.9317883310472361\n",
      "train loss:0.8413114555443947\n",
      "train loss:1.1530152348773757\n",
      "train loss:0.8550470919421389\n",
      "train loss:0.6812729604525594\n",
      "train loss:1.0888451246010595\n",
      "train loss:0.8055071481645144\n",
      "train loss:1.0310083916933859\n",
      "train loss:0.9571037000813969\n",
      "train loss:0.9698083390796043\n",
      "train loss:0.5986038904512478\n",
      "train loss:1.009456943378397\n",
      "train loss:0.9176505639717043\n",
      "train loss:0.932236819751753\n",
      "train loss:0.7739377484234339\n",
      "train loss:0.8579302046988755\n",
      "train loss:0.9464668342265814\n",
      "train loss:0.871993633474008\n",
      "train loss:0.9350758166817673\n",
      "train loss:0.6372900260302206\n",
      "train loss:0.9277933786847933\n",
      "train loss:0.8957566691144083\n",
      "train loss:1.0243587714996827\n",
      "train loss:0.9440948217628622\n",
      "train loss:0.7195281870642911\n",
      "train loss:0.8229873154681248\n",
      "train loss:0.7912629725890915\n",
      "train loss:0.9495971306266636\n",
      "train loss:0.7318549462327091\n",
      "train loss:0.82204325492489\n",
      "train loss:1.0318589183045903\n",
      "train loss:0.8716297265905691\n",
      "train loss:0.8830170289244841\n",
      "train loss:0.8458812322095792\n",
      "train loss:1.0176911330518892\n",
      "train loss:0.8908108351690861\n",
      "train loss:1.036777076100442\n",
      "train loss:0.9368047679045735\n",
      "train loss:0.9317940910170157\n",
      "train loss:1.111552830553188\n",
      "train loss:0.7901108114649469\n",
      "train loss:0.9839946404372532\n",
      "train loss:0.9405067202616515\n",
      "train loss:1.0784567161152347\n",
      "train loss:0.9092862489524376\n",
      "train loss:0.9940251799475504\n",
      "train loss:0.8356693818856529\n",
      "train loss:0.8359677853861914\n",
      "train loss:0.9102002124784022\n",
      "train loss:0.9411571726032952\n",
      "train loss:0.8728077410452818\n",
      "train loss:0.852188683397975\n",
      "train loss:0.884468865244462\n",
      "train loss:0.9845782009690518\n",
      "train loss:0.8766539752428867\n",
      "train loss:1.0348263010852528\n",
      "train loss:0.865578778386988\n",
      "train loss:0.9211716896618009\n",
      "train loss:1.032039142071627\n",
      "train loss:0.951442340334132\n",
      "train loss:1.0639294578350689\n",
      "train loss:0.8573411588643078\n",
      "train loss:0.8914286185699531\n",
      "train loss:0.7573362017597393\n",
      "train loss:1.029511357797917\n",
      "train loss:0.950102196262239\n",
      "train loss:0.7900069430605926\n",
      "train loss:0.8188185485488969\n",
      "train loss:0.8311437073890011\n",
      "train loss:1.0045936521622756\n",
      "train loss:0.8958534659559346\n",
      "train loss:0.8886145922014584\n",
      "train loss:1.0033920324635373\n",
      "train loss:1.0578988611301208\n",
      "train loss:0.8349950637069845\n",
      "train loss:0.8628626964226537\n",
      "train loss:0.8373607997381527\n",
      "train loss:0.7425055541560559\n",
      "train loss:0.8424003135642636\n",
      "train loss:0.9055836597039781\n",
      "train loss:0.828992468259671\n",
      "train loss:0.9250904658111192\n",
      "train loss:0.9817791829644237\n",
      "train loss:0.9744091300989094\n",
      "train loss:0.8194176572273362\n",
      "train loss:1.0753580327922037\n",
      "train loss:0.7118891747770442\n",
      "train loss:1.0389553910090021\n",
      "train loss:0.7675341509496714\n",
      "train loss:0.8211957627821042\n",
      "train loss:0.7833764546440019\n",
      "train loss:0.9434454318466723\n",
      "train loss:0.9236427110351478\n",
      "train loss:0.8569922596973862\n",
      "train loss:0.9585112702684131\n",
      "train loss:0.9718000278293653\n",
      "train loss:1.064586304011042\n",
      "train loss:0.9247765995202141\n",
      "train loss:0.970422933986708\n",
      "train loss:0.8545991415898313\n",
      "train loss:1.084256827190594\n",
      "train loss:0.9847326549104171\n",
      "train loss:1.0281560318969982\n",
      "train loss:0.9127018765214161\n",
      "train loss:0.9278315427711825\n",
      "train loss:0.9281816791818244\n",
      "train loss:0.8969801397784681\n",
      "train loss:0.8180074868122112\n",
      "train loss:0.9255078825266412\n",
      "train loss:0.775838675046722\n",
      "train loss:1.094958414274516\n",
      "train loss:1.0270738652141984\n",
      "train loss:0.872697738941164\n",
      "train loss:0.832416271774285\n",
      "train loss:0.9139519777697174\n",
      "train loss:0.9247280705062186\n",
      "train loss:0.9032098207155457\n",
      "train loss:0.9493745699590167\n",
      "train loss:0.905421388259398\n",
      "train loss:0.8034109651084081\n",
      "train loss:0.9032794516711209\n",
      "train loss:0.9989021554752735\n",
      "train loss:0.9353111610773293\n",
      "train loss:0.9648447410810357\n",
      "train loss:0.9490798200001173\n",
      "train loss:0.8632648501182798\n",
      "train loss:0.8417187936370432\n",
      "train loss:0.9536685022593097\n",
      "train loss:0.9521215813493855\n",
      "train loss:0.9438611652175378\n",
      "train loss:0.8721691941634812\n",
      "train loss:0.7971251090540538\n",
      "train loss:0.9501776637412562\n",
      "train loss:0.9112027876679762\n",
      "train loss:0.9667644223439996\n",
      "train loss:0.9570706008543197\n",
      "train loss:1.0611175618612683\n",
      "train loss:1.0127560600517191\n",
      "train loss:0.925174881198239\n",
      "train loss:0.7329817292469799\n",
      "train loss:1.0640597171850268\n",
      "train loss:0.8993682535661506\n",
      "train loss:0.8376740333407001\n",
      "train loss:0.757634392456004\n",
      "train loss:0.8737012916841589\n",
      "train loss:1.0304705853269522\n",
      "train loss:0.8685148920968424\n",
      "train loss:0.8518328861390961\n",
      "train loss:0.80207680056733\n",
      "train loss:0.8002526204742224\n",
      "train loss:0.8219570534921484\n",
      "train loss:0.8978130811196181\n",
      "train loss:1.098610323108873\n",
      "train loss:0.8021625484328642\n",
      "train loss:0.8727147875634632\n",
      "train loss:0.8755990237294337\n",
      "train loss:0.8315140180775196\n",
      "train loss:0.9775192883548408\n",
      "train loss:1.0042879336868615\n",
      "train loss:0.7686781600808961\n",
      "train loss:0.8407256732306934\n",
      "train loss:0.8909618938347963\n",
      "train loss:0.8388984661853116\n",
      "train loss:0.8030826662107813\n",
      "train loss:1.0164910429884804\n",
      "train loss:0.9392166410563825\n",
      "train loss:0.8810050174768266\n",
      "train loss:0.8695225294342727\n",
      "train loss:0.8139676997778936\n",
      "train loss:0.9688754622925647\n",
      "train loss:0.8234041013794591\n",
      "train loss:0.9710798825533424\n",
      "train loss:0.8761624887402587\n",
      "train loss:1.0427315878751466\n",
      "train loss:0.9351514171206093\n",
      "train loss:1.0245417314849743\n",
      "train loss:0.8280914991164634\n",
      "train loss:0.8428911907811372\n",
      "train loss:1.0382575393768005\n",
      "train loss:0.7576591985458383\n",
      "train loss:0.8691025783066822\n",
      "train loss:0.905614333401642\n",
      "train loss:0.8657990393555143\n",
      "train loss:0.962413935867959\n",
      "train loss:0.9102623017984743\n",
      "train loss:0.8333926690569473\n",
      "train loss:0.9914173760126029\n",
      "train loss:0.9286012746593312\n",
      "train loss:0.8467435655026015\n",
      "train loss:0.9818697825793414\n",
      "train loss:0.8869569896260239\n",
      "train loss:0.9946484841462457\n",
      "train loss:0.9659609273239554\n",
      "train loss:1.0463078930328458\n",
      "train loss:1.0150345283574638\n",
      "train loss:1.0912165787737085\n",
      "train loss:0.9983406034329491\n",
      "train loss:0.8097200575427981\n",
      "train loss:0.807194013153461\n",
      "train loss:0.8016841713856157\n",
      "train loss:1.151794916949737\n",
      "train loss:1.1505454075633386\n",
      "train loss:0.9253925553801863\n",
      "train loss:0.9406895227808855\n",
      "train loss:0.7776738046652938\n",
      "train loss:0.9613521799192972\n",
      "train loss:0.8095404003362212\n",
      "train loss:0.8785277584943952\n",
      "train loss:0.8544721241419689\n",
      "train loss:1.0019907822511116\n",
      "train loss:0.870573292111546\n",
      "train loss:0.8037112841606887\n",
      "train loss:0.8854744953490489\n",
      "train loss:0.9760556869671284\n",
      "train loss:0.94338289278372\n",
      "train loss:1.057920599697129\n",
      "train loss:0.9431630913468166\n",
      "train loss:0.9334128759013974\n",
      "train loss:0.9673295730832525\n",
      "train loss:0.8341568073636461\n",
      "train loss:1.0215972344816597\n",
      "train loss:0.7850824409638196\n",
      "train loss:0.9889496926692135\n",
      "train loss:0.7915176621968465\n",
      "train loss:0.9100836444332357\n",
      "train loss:0.8065115827625193\n",
      "train loss:1.040817785631285\n",
      "train loss:0.8641868904794161\n",
      "train loss:1.1307110480204343\n",
      "train loss:0.9625863667092205\n",
      "train loss:1.0193270175462257\n",
      "train loss:0.9845960252043281\n",
      "train loss:1.0140932570089434\n",
      "train loss:0.8812720118087529\n",
      "train loss:0.9228496459954855\n",
      "train loss:0.876917431903311\n",
      "train loss:0.8002631984545253\n",
      "train loss:0.8737302413466295\n",
      "train loss:0.8251239773635122\n",
      "train loss:0.7916768297912542\n",
      "train loss:1.0224693752307414\n",
      "train loss:0.8500603987217924\n",
      "train loss:0.8884841989560348\n",
      "train loss:0.927692784316232\n",
      "train loss:0.8754149007081142\n",
      "train loss:0.9287463314560774\n",
      "train loss:0.8228293793257884\n",
      "train loss:0.7618295806485889\n",
      "train loss:0.973569671443451\n",
      "train loss:0.9246751228585807\n",
      "train loss:0.8604281381391611\n",
      "train loss:0.8669862829086205\n",
      "train loss:0.9426197573007984\n",
      "train loss:0.8985361873068424\n",
      "train loss:0.8843817338659022\n",
      "train loss:0.884889733985199\n",
      "train loss:0.9413031763357467\n",
      "train loss:0.7679060401822002\n",
      "train loss:0.8110763271655727\n",
      "=== epoch:9, train acc:0.997, test acc:0.992 ===\n",
      "train loss:0.8437788753101007\n",
      "train loss:0.9564522635238814\n",
      "train loss:0.9257499747283695\n",
      "train loss:0.8764968135154634\n",
      "train loss:1.1884684888661263\n",
      "train loss:0.9839141106973751\n",
      "train loss:0.8262133318225869\n",
      "train loss:0.9131524176098794\n",
      "train loss:0.8930516707165999\n",
      "train loss:0.7929748729304603\n",
      "train loss:0.9413482665587825\n",
      "train loss:0.8201393147588854\n",
      "train loss:0.7487369167957837\n",
      "train loss:1.0751730376042838\n",
      "train loss:0.7937132030024021\n",
      "train loss:0.8597679948139186\n",
      "train loss:0.9503918133470575\n",
      "train loss:0.7443086368585823\n",
      "train loss:0.9624029089879251\n",
      "train loss:1.0279098099078956\n",
      "train loss:1.1883149328737077\n",
      "train loss:0.9093718479098126\n",
      "train loss:1.0095050773004035\n",
      "train loss:0.8377646550684307\n",
      "train loss:0.9176789869671148\n",
      "train loss:0.9633105185420183\n",
      "train loss:0.8342243510843784\n",
      "train loss:0.7961649189174425\n",
      "train loss:0.8481969215395171\n",
      "train loss:0.9379619276256486\n",
      "train loss:1.0150592509579277\n",
      "train loss:0.845350844262436\n",
      "train loss:0.8013330876735034\n",
      "train loss:0.9973692228453701\n",
      "train loss:0.8381156504951335\n",
      "train loss:0.8595287247449377\n",
      "train loss:0.815242070047963\n",
      "train loss:0.7794512159087265\n",
      "train loss:0.8280125769209324\n",
      "train loss:0.9221035752263552\n",
      "train loss:0.9169317201969365\n",
      "train loss:0.8972374642304248\n",
      "train loss:0.9643390994616194\n",
      "train loss:0.946843297814323\n",
      "train loss:0.8841555275211364\n",
      "train loss:0.8366218478171913\n",
      "train loss:0.8541561601861939\n",
      "train loss:0.8691473933421813\n",
      "train loss:0.963719604250992\n",
      "train loss:0.9258651403468234\n",
      "train loss:0.9056876846203165\n",
      "train loss:0.7207453045613792\n",
      "train loss:0.9549682898269102\n",
      "train loss:0.8985278534115146\n",
      "train loss:0.7943518922381493\n",
      "train loss:0.906376531672622\n",
      "train loss:0.8871723473021136\n",
      "train loss:0.9845736685716555\n",
      "train loss:0.6800906389432229\n",
      "train loss:1.0304064178563892\n",
      "train loss:1.0987864267924534\n",
      "train loss:0.9562674484956252\n",
      "train loss:1.041089211629487\n",
      "train loss:0.9128574497544656\n",
      "train loss:1.1812067179088306\n",
      "train loss:0.9115704488543162\n",
      "train loss:0.9413992564932389\n",
      "train loss:0.9755448785595472\n",
      "train loss:0.9188058800531596\n",
      "train loss:0.8264770450370663\n",
      "train loss:0.7968474506510329\n",
      "train loss:0.9062946830004001\n",
      "train loss:1.0387787615177244\n",
      "train loss:0.8097151602327238\n",
      "train loss:0.959187306219623\n",
      "train loss:0.969136913065894\n",
      "train loss:0.8282172805949491\n",
      "train loss:1.0533645293613254\n",
      "train loss:0.9499945034340631\n",
      "train loss:0.7804386304134096\n",
      "train loss:0.9118135830283588\n",
      "train loss:0.9797851961943376\n",
      "train loss:0.7686193395597172\n",
      "train loss:0.9080567241859783\n",
      "train loss:0.8118331131395093\n",
      "train loss:0.8377516865054863\n",
      "train loss:0.8524463255301314\n",
      "train loss:0.8591155133042508\n",
      "train loss:0.9437259106878302\n",
      "train loss:0.7153942570403027\n",
      "train loss:0.8768003794151333\n",
      "train loss:0.9819168936474573\n",
      "train loss:0.9473061201335942\n",
      "train loss:0.9246353221793542\n",
      "train loss:0.8434693905151631\n",
      "train loss:0.7490453413896481\n",
      "train loss:0.8969769115137504\n",
      "train loss:0.9677684329127615\n",
      "train loss:0.9168853107827792\n",
      "train loss:0.9692566006366987\n",
      "train loss:0.9444365068809312\n",
      "train loss:0.8488405819721169\n",
      "train loss:0.8174175848442573\n",
      "train loss:1.0208376203346345\n",
      "train loss:0.8376266005333367\n",
      "train loss:0.7712852731875712\n",
      "train loss:0.8628412284683713\n",
      "train loss:0.6905274118339423\n",
      "train loss:0.8360466726623911\n",
      "train loss:1.0192460839901551\n",
      "train loss:0.9018867987712543\n",
      "train loss:0.8215215861491165\n",
      "train loss:1.0401201907760704\n",
      "train loss:0.8376476638078779\n",
      "train loss:0.9494978150786185\n",
      "train loss:0.9358959276364606\n",
      "train loss:0.801582342811747\n",
      "train loss:0.8872992619862862\n",
      "train loss:0.8218675505910678\n",
      "train loss:0.8553450162317362\n",
      "train loss:1.094388798273483\n",
      "train loss:0.8649212267702331\n",
      "train loss:0.8664682851769383\n",
      "train loss:0.9644673745484228\n",
      "train loss:0.8575885460075127\n",
      "train loss:0.9192238488334987\n",
      "train loss:0.9865194724017117\n",
      "train loss:0.892276545692139\n",
      "train loss:0.981006526015351\n",
      "train loss:0.9514374850803249\n",
      "train loss:1.0046291669352763\n",
      "train loss:0.9989307952460503\n",
      "train loss:0.8351048100356327\n",
      "train loss:0.7592371783820085\n",
      "train loss:0.8979465438212383\n",
      "train loss:0.8002039878184039\n",
      "train loss:0.8637318775476501\n",
      "train loss:0.8502191644253899\n",
      "train loss:1.0439242072819057\n",
      "train loss:0.8071675588296324\n",
      "train loss:0.9519943595346494\n",
      "train loss:0.7124086521528216\n",
      "train loss:0.9198992173662931\n",
      "train loss:0.8809070847661343\n",
      "train loss:0.8982367079930333\n",
      "train loss:1.0331464315478627\n",
      "train loss:0.8487604009018004\n",
      "train loss:0.924458407261733\n",
      "train loss:1.1716811098412576\n",
      "train loss:0.9905803098399282\n",
      "train loss:0.9112405210814214\n",
      "train loss:0.8424973598141843\n",
      "train loss:0.9782025160944725\n",
      "train loss:0.9628039333764637\n",
      "train loss:1.0841120689741999\n",
      "train loss:1.0162478559442543\n",
      "train loss:0.8954824942745188\n",
      "train loss:0.9495862197232701\n",
      "train loss:0.827567072689702\n",
      "train loss:1.0841614749791972\n",
      "train loss:0.6904311966303507\n",
      "train loss:0.7964294087381216\n",
      "train loss:0.8794447859882575\n",
      "train loss:0.9825220165378148\n",
      "train loss:0.9165687489530085\n",
      "train loss:0.9511081710686322\n",
      "train loss:0.7007926643756274\n",
      "train loss:0.8817200906688961\n",
      "train loss:0.9001099009808445\n",
      "train loss:1.0351461672035032\n",
      "train loss:0.8679171584067137\n",
      "train loss:0.9719790703991574\n",
      "train loss:0.9795474743677807\n",
      "train loss:0.9618074459133271\n",
      "train loss:0.8699025900255951\n",
      "train loss:0.8826937292408229\n",
      "train loss:1.0341873165746156\n",
      "train loss:0.8836081836226899\n",
      "train loss:0.636348380447747\n",
      "train loss:1.0417778142707346\n",
      "train loss:0.9658575167440826\n",
      "train loss:1.106615107588318\n",
      "train loss:0.9111545113391664\n",
      "train loss:0.874339464841108\n",
      "train loss:1.0015083160316327\n",
      "train loss:1.0695363661170458\n",
      "train loss:0.8738390433262531\n",
      "train loss:0.9677489374340823\n",
      "train loss:0.7499146099228023\n",
      "train loss:0.9652692550991192\n",
      "train loss:0.9497447162905699\n",
      "train loss:0.8256757732605577\n",
      "train loss:0.8866436127155574\n",
      "train loss:0.9106504500322093\n",
      "train loss:0.7290522038563441\n",
      "train loss:1.0119110953381714\n",
      "train loss:1.0636780686990088\n",
      "train loss:0.8539531607990706\n",
      "train loss:0.8657579578740777\n",
      "train loss:0.8157877625339353\n",
      "train loss:0.8321834965623762\n",
      "train loss:0.9085801171759443\n",
      "train loss:0.9982801088455825\n",
      "train loss:0.7912943680161993\n",
      "train loss:0.745785144797957\n",
      "train loss:0.9662027831536639\n",
      "train loss:0.8347160806625928\n",
      "train loss:1.0246760364111402\n",
      "train loss:0.9909409983539015\n",
      "train loss:0.6728045555049644\n",
      "train loss:0.8369641060319091\n",
      "train loss:0.9364754452987473\n",
      "train loss:0.7285310375248685\n",
      "train loss:0.8195588298808224\n",
      "train loss:0.9361372161867538\n",
      "train loss:0.8478127942762751\n",
      "train loss:0.759928097178972\n",
      "train loss:0.9987909215881677\n",
      "train loss:0.8353572669286367\n",
      "train loss:1.148690778611532\n",
      "train loss:0.8471031398408807\n",
      "train loss:0.8134478519788335\n",
      "train loss:0.8741545485716972\n",
      "train loss:0.9688966088573976\n",
      "train loss:0.8422758603177983\n",
      "train loss:0.727553478717445\n",
      "train loss:0.9666240389600002\n",
      "train loss:0.7008856112614008\n",
      "train loss:0.7607402397907667\n",
      "train loss:1.0994014910209284\n",
      "train loss:0.9874206217221836\n",
      "train loss:0.9928111649796044\n",
      "train loss:0.9378406359846622\n",
      "train loss:1.002046994168957\n",
      "train loss:0.6727634596479062\n",
      "train loss:0.855158478756168\n",
      "train loss:0.9208493239005388\n",
      "train loss:0.8524520551581704\n",
      "train loss:1.0215430417797449\n",
      "train loss:0.975161414147988\n",
      "train loss:0.9716608160487218\n",
      "train loss:0.8724856842698732\n",
      "train loss:0.6788226282833508\n",
      "train loss:0.8320410170115331\n",
      "train loss:0.8472533621648789\n",
      "train loss:0.8985035399925657\n",
      "train loss:0.851527366729936\n",
      "train loss:1.057533769935949\n",
      "train loss:0.8828096139289534\n",
      "train loss:0.9969918618117667\n",
      "train loss:0.9315609911897433\n",
      "train loss:0.9851750356945242\n",
      "train loss:0.8952977337153363\n",
      "train loss:1.0813302915687282\n",
      "train loss:0.7276942012116315\n",
      "train loss:0.8500142913671089\n",
      "train loss:0.8844227212648811\n",
      "train loss:0.8604648463030585\n",
      "train loss:0.8353774627661978\n",
      "train loss:0.9087646543957854\n",
      "train loss:0.925321773279804\n",
      "train loss:0.83821825005542\n",
      "train loss:0.9572537349690087\n",
      "train loss:0.9008009031596126\n",
      "train loss:0.8273853056057717\n",
      "train loss:0.982272821953953\n",
      "train loss:1.0193225533468857\n",
      "train loss:0.8223423678922824\n",
      "train loss:0.9548600885419396\n",
      "train loss:0.8917286563961875\n",
      "train loss:0.8320911783133966\n",
      "train loss:0.8762049474132405\n",
      "train loss:0.8948676711739445\n",
      "train loss:0.8738191002041841\n",
      "train loss:0.9919536459759211\n",
      "train loss:0.9226163431338044\n",
      "train loss:0.9903366552035962\n",
      "train loss:0.8489909847083551\n",
      "train loss:0.9190107260668071\n",
      "train loss:0.8734202965769775\n",
      "train loss:0.7855696814293546\n",
      "train loss:0.7845467092965839\n",
      "train loss:0.8744930163391521\n",
      "train loss:0.9137129111432507\n",
      "train loss:0.6610698517140845\n",
      "train loss:0.8740383783686195\n",
      "train loss:0.902973332652314\n",
      "train loss:1.074076436624709\n",
      "train loss:0.8708964584777661\n",
      "train loss:0.7873157755463631\n",
      "train loss:0.9093576195541881\n",
      "train loss:0.8326754004897193\n",
      "train loss:0.8984638170173089\n",
      "train loss:0.9930568278139208\n",
      "train loss:0.7727008753134276\n",
      "train loss:0.758731791696504\n",
      "train loss:0.8807431865175266\n",
      "train loss:0.7268996822108421\n",
      "train loss:0.9704518502002073\n",
      "train loss:0.8309200131825789\n",
      "train loss:0.6996081041759679\n",
      "train loss:0.984885086294218\n",
      "train loss:1.0142539195075209\n",
      "train loss:0.8919193740205067\n",
      "train loss:0.7433198088629276\n",
      "train loss:0.9228109441844109\n",
      "train loss:0.7036273958194437\n",
      "train loss:0.8964537613580882\n",
      "train loss:1.046547942326625\n",
      "train loss:0.8921417257760301\n",
      "train loss:0.9202237973832905\n",
      "train loss:0.8162657377804227\n",
      "train loss:0.864568781082288\n",
      "train loss:0.8787107841233868\n",
      "train loss:0.7480761508390521\n",
      "train loss:0.9083651348748534\n",
      "train loss:0.9667758456242095\n",
      "train loss:0.8847258469497642\n",
      "train loss:0.9980055174602842\n",
      "train loss:0.8760014748024625\n",
      "train loss:0.9056926763852471\n",
      "train loss:0.6724617732162683\n",
      "train loss:1.0021367234768275\n",
      "train loss:0.8635272235438797\n",
      "train loss:0.7969875059852529\n",
      "train loss:0.8296892961514135\n",
      "train loss:0.9756589686178746\n",
      "train loss:0.85090289811798\n",
      "train loss:0.9923409566224328\n",
      "train loss:0.8532633028160724\n",
      "train loss:0.8311270197662541\n",
      "train loss:0.8480733729876185\n",
      "train loss:0.9045837474785491\n",
      "train loss:0.8940169511524891\n",
      "train loss:0.8418333124010993\n",
      "train loss:0.8447773698526894\n",
      "train loss:0.9649980307328608\n",
      "train loss:0.8374086784884851\n",
      "train loss:0.918466091757638\n",
      "train loss:0.9996436201413452\n",
      "train loss:0.9061722084748163\n",
      "train loss:0.8928170133989632\n",
      "train loss:0.8064706808248157\n",
      "train loss:0.8811317980354936\n",
      "train loss:0.7321995656270346\n",
      "train loss:0.8182581654418988\n",
      "train loss:0.8799497073888988\n",
      "train loss:0.9809538542071798\n",
      "train loss:0.8602057099302481\n",
      "train loss:0.940816413783504\n",
      "train loss:0.8964293303914471\n",
      "train loss:1.0542942797995458\n",
      "train loss:0.8825204819241372\n",
      "train loss:0.8776210722298591\n",
      "train loss:0.8237936708690957\n",
      "train loss:0.916964624281665\n",
      "train loss:0.9020079671189373\n",
      "train loss:0.8389116562078768\n",
      "train loss:0.8595105850924466\n",
      "train loss:0.9527691016585568\n",
      "train loss:0.8764930029002126\n",
      "train loss:0.9158134215162961\n",
      "train loss:0.9037235324483417\n",
      "train loss:0.7416482317697153\n",
      "train loss:0.8946919040069966\n",
      "train loss:1.0212129961393899\n",
      "train loss:1.0382054508179819\n",
      "train loss:0.7731344037069077\n",
      "train loss:0.96915957517697\n",
      "train loss:0.8868502133473577\n",
      "train loss:0.8575391351241964\n",
      "train loss:0.823599901172075\n",
      "train loss:0.9806815922089194\n",
      "train loss:0.940235396704908\n",
      "train loss:0.9304136866346013\n",
      "train loss:1.12462398734395\n",
      "train loss:0.9563402948219022\n",
      "train loss:0.8034681295358879\n",
      "train loss:0.6419465266748899\n",
      "train loss:0.9008436309405845\n",
      "train loss:0.8150722978473578\n",
      "train loss:0.8551312123247172\n",
      "train loss:0.8783998438326995\n",
      "train loss:0.8432542649915914\n",
      "train loss:0.8539650543614711\n",
      "train loss:1.0227837773211268\n",
      "train loss:0.767451189363495\n",
      "train loss:0.82440823450932\n",
      "train loss:0.9980116820240371\n",
      "train loss:0.9831169188669223\n",
      "train loss:0.8431890276299191\n",
      "train loss:1.0057255541926482\n",
      "train loss:0.8421042640165848\n",
      "train loss:0.8398560056872187\n",
      "train loss:0.7855155343006404\n",
      "train loss:0.9544538991786184\n",
      "train loss:0.8593858635093041\n",
      "train loss:0.8075268035305957\n",
      "train loss:0.9852185755187037\n",
      "train loss:0.9285202283125478\n",
      "train loss:0.8710967591796178\n",
      "train loss:1.080713523321402\n",
      "train loss:1.0358149064687026\n",
      "train loss:0.80062752492496\n",
      "train loss:0.8621870130940016\n",
      "train loss:0.7838789154293195\n",
      "train loss:0.8199134137727596\n",
      "train loss:0.9220413407626752\n",
      "train loss:1.0005328892223866\n",
      "train loss:0.8119643834490629\n",
      "train loss:0.8601768171910112\n",
      "train loss:1.0398900575629966\n",
      "train loss:0.8956360570125877\n",
      "train loss:0.9033129581108094\n",
      "train loss:0.8736584303837757\n",
      "train loss:0.9967423324604349\n",
      "train loss:0.7127785689974445\n",
      "train loss:0.7940325802086232\n",
      "train loss:0.8700293628903796\n",
      "train loss:0.6931511228181482\n",
      "train loss:0.8436038558101424\n",
      "train loss:0.800007834292218\n",
      "train loss:1.035526333890791\n",
      "train loss:0.889284486545901\n",
      "train loss:0.8930664716177655\n",
      "train loss:0.8531736386769592\n",
      "train loss:0.9074716746664149\n",
      "train loss:0.8751013704075938\n",
      "train loss:0.8444602253785114\n",
      "train loss:0.9229310912823196\n",
      "train loss:0.9330031428156578\n",
      "train loss:0.8755069784799511\n",
      "train loss:0.9176250205439138\n",
      "train loss:0.8594677241223656\n",
      "train loss:0.991289591637376\n",
      "train loss:0.7682465648298751\n",
      "train loss:0.9553915969207363\n",
      "train loss:0.7803421492895461\n",
      "train loss:0.7215274430653038\n",
      "train loss:0.9852411634285663\n",
      "train loss:0.8094705609860671\n",
      "train loss:1.0212261206793183\n",
      "train loss:0.9511621038286282\n",
      "train loss:1.180995150897089\n",
      "train loss:0.8706145685406378\n",
      "train loss:0.7618366869018446\n",
      "train loss:0.9891377000093058\n",
      "train loss:0.9193989176635107\n",
      "train loss:0.984884413381021\n",
      "train loss:0.9343074556487236\n",
      "train loss:0.8655730569688186\n",
      "train loss:0.8753653103297804\n",
      "train loss:0.9509494358583975\n",
      "train loss:0.8010917812197376\n",
      "train loss:1.0632411437174134\n",
      "train loss:0.8846604560566778\n",
      "train loss:0.8519582725453941\n",
      "train loss:0.8838585801668701\n",
      "train loss:0.9465363821923909\n",
      "train loss:0.9074084711544619\n",
      "train loss:0.9468170410010707\n",
      "train loss:0.6957050864399865\n",
      "train loss:0.7670129413679624\n",
      "train loss:0.9904670922836785\n",
      "train loss:1.1758959943211673\n",
      "train loss:0.924630302472103\n",
      "train loss:1.0461288943154667\n",
      "train loss:1.007042758981662\n",
      "train loss:0.7554481400244079\n",
      "train loss:0.9364148952584308\n",
      "train loss:0.9207217349716585\n",
      "train loss:0.8921452567045633\n",
      "train loss:0.8250457512220224\n",
      "train loss:0.7194341370125409\n",
      "train loss:0.9725112990961557\n",
      "train loss:0.9193671338867893\n",
      "train loss:1.0873738408912497\n",
      "train loss:0.9985980558474418\n",
      "train loss:0.7758359495134047\n",
      "train loss:0.9660915145729497\n",
      "train loss:0.8420588126800513\n",
      "train loss:0.8867127406138757\n",
      "train loss:0.9227434343954712\n",
      "train loss:1.086944462534415\n",
      "train loss:1.0286928182757558\n",
      "train loss:0.7743592244290498\n",
      "train loss:0.9791927093367531\n",
      "train loss:0.986921782281932\n",
      "train loss:1.0243864776188152\n",
      "train loss:0.9108771917980981\n",
      "train loss:0.7540830029666356\n",
      "train loss:0.8982479717437731\n",
      "train loss:0.8378135097249979\n",
      "train loss:0.9131328163876361\n",
      "train loss:0.8584397851715673\n",
      "train loss:0.8294352039404447\n",
      "train loss:0.8660883142404723\n",
      "train loss:0.8884413737268666\n",
      "train loss:1.0984574978278268\n",
      "train loss:0.8120974544470306\n",
      "train loss:0.9713216745623824\n",
      "train loss:0.99352842562915\n",
      "train loss:1.0057876744042764\n",
      "train loss:0.907620617559245\n",
      "train loss:0.9431702440146114\n",
      "train loss:0.9912199316840681\n",
      "train loss:0.8969630962413905\n",
      "train loss:0.7908336804616601\n",
      "train loss:0.9342920069362631\n",
      "train loss:0.8776318118243087\n",
      "train loss:0.942170933219291\n",
      "train loss:0.8689775417467956\n",
      "train loss:0.8906673299496793\n",
      "train loss:0.9178400706245857\n",
      "train loss:0.9064306909386619\n",
      "train loss:0.943092200181202\n",
      "train loss:1.0244433734623128\n",
      "train loss:0.9388206237611614\n",
      "train loss:0.8815480352451863\n",
      "train loss:0.8893675548012869\n",
      "train loss:1.1061623913531375\n",
      "train loss:0.859259046904115\n",
      "train loss:0.8271328452574339\n",
      "train loss:0.8761760088055685\n",
      "train loss:0.8128191685822427\n",
      "train loss:0.8776244195236771\n",
      "train loss:0.8386829511833538\n",
      "train loss:0.8841813754491141\n",
      "train loss:0.7406549320319309\n",
      "train loss:0.9242336710750257\n",
      "train loss:0.8341888145234694\n",
      "train loss:0.7920848638346576\n",
      "train loss:0.9521275283181382\n",
      "train loss:0.9605022794722058\n",
      "train loss:1.0113838680318243\n",
      "train loss:0.7855242824000525\n",
      "train loss:0.8644169144899649\n",
      "train loss:0.8117849975591583\n",
      "train loss:0.8815062015255549\n",
      "train loss:1.0325437227411014\n",
      "train loss:0.8531203592253448\n",
      "train loss:0.9461824451048446\n",
      "train loss:1.0444216651484437\n",
      "train loss:1.0383570795182777\n",
      "train loss:1.0027883665868438\n",
      "train loss:0.9882647576409584\n",
      "train loss:0.732313428131431\n",
      "train loss:0.8784161074370713\n",
      "train loss:0.6959070826670355\n",
      "train loss:0.8935096917620173\n",
      "train loss:0.9579754166743732\n",
      "train loss:0.8196761809203218\n",
      "train loss:0.860310145510712\n",
      "train loss:0.9720741102084243\n",
      "train loss:1.070349741618576\n",
      "train loss:0.8758286276796682\n",
      "train loss:0.7616927476459316\n",
      "train loss:0.7530902004138295\n",
      "train loss:0.8647756059276956\n",
      "train loss:0.9889088852972017\n",
      "train loss:1.1167860316284337\n",
      "train loss:0.8223835629145326\n",
      "train loss:0.9730679694999199\n",
      "train loss:0.9767128263555546\n",
      "train loss:0.7936346843306484\n",
      "train loss:0.9103037756172838\n",
      "train loss:0.9143922751879577\n",
      "train loss:0.9024555964290432\n",
      "train loss:0.8308555086653473\n",
      "train loss:1.0028344431789693\n",
      "train loss:0.8722000440814935\n",
      "train loss:0.9342867187031662\n",
      "train loss:0.9369347488222942\n",
      "train loss:0.9447852179223589\n",
      "train loss:0.9243836316322442\n",
      "train loss:0.8292916189856544\n",
      "train loss:0.7413535420500544\n",
      "train loss:0.9557699495114216\n",
      "train loss:0.9493832245214723\n",
      "train loss:0.8517909343942782\n",
      "train loss:0.9465771831022377\n",
      "train loss:0.8876941363221499\n",
      "train loss:0.8310341221203802\n",
      "train loss:0.8727071270872742\n",
      "train loss:0.8683072213200531\n",
      "train loss:0.9278553881611786\n",
      "train loss:0.7131411462564133\n",
      "train loss:0.9970017446348965\n",
      "train loss:0.817801956035142\n",
      "train loss:1.1326644531982253\n",
      "train loss:0.8769019438568105\n",
      "train loss:0.8545948754534213\n",
      "train loss:0.9852751563062231\n",
      "train loss:0.967362438991962\n",
      "train loss:0.921676286488278\n",
      "train loss:0.8788645596788922\n",
      "train loss:0.9871728479792524\n",
      "train loss:0.9398399058421574\n",
      "train loss:1.1088318347682218\n",
      "train loss:1.01758750392966\n",
      "=== epoch:10, train acc:0.996, test acc:0.992 ===\n",
      "train loss:0.9108120309620854\n",
      "train loss:0.8845190802597344\n",
      "train loss:0.9032521863873797\n",
      "train loss:0.8741382394262132\n",
      "train loss:0.8735691192069557\n",
      "train loss:0.9763050000185443\n",
      "train loss:0.9682409902441148\n",
      "train loss:0.9052471040169859\n",
      "train loss:0.9061653312298987\n",
      "train loss:0.9228486486733253\n",
      "train loss:0.8539514967017188\n",
      "train loss:0.9307016574870386\n",
      "train loss:0.9776970271372807\n",
      "train loss:0.8367552169217319\n",
      "train loss:0.9862562898032643\n",
      "train loss:1.0606139438539604\n",
      "train loss:0.7050912404317903\n",
      "train loss:0.8874538608614353\n",
      "train loss:0.8771165503285296\n",
      "train loss:0.8615392995157979\n",
      "train loss:0.8642185377141867\n",
      "train loss:0.9207508687792604\n",
      "train loss:0.8903962073467365\n",
      "train loss:0.9203130738040902\n",
      "train loss:0.9436395961369448\n",
      "train loss:0.8837975162246365\n",
      "train loss:1.0408775839624702\n",
      "train loss:0.9607255760292768\n",
      "train loss:0.8086805990091207\n",
      "train loss:0.8830824399375403\n",
      "train loss:1.1298619218688701\n",
      "train loss:1.0610688850711487\n",
      "train loss:0.9636385842071608\n",
      "train loss:0.8360319652404017\n",
      "train loss:0.8874650369096817\n",
      "train loss:0.8831220568694056\n",
      "train loss:0.997240590286825\n",
      "train loss:0.8580851417155272\n",
      "train loss:0.854671410134612\n",
      "train loss:0.9830527230684296\n",
      "train loss:0.9263386480266033\n",
      "train loss:0.8816384894751166\n",
      "train loss:0.8799933817353522\n",
      "train loss:0.9003661719548086\n",
      "train loss:0.7968438602234742\n",
      "train loss:0.8464546632903875\n",
      "train loss:0.8712029547934725\n",
      "train loss:1.0014285138779015\n",
      "train loss:0.8761256021675795\n",
      "train loss:0.9209069484865177\n",
      "train loss:0.9050837979362372\n",
      "train loss:0.8801690141719994\n",
      "train loss:0.7708229353272326\n",
      "train loss:0.8956315302505742\n",
      "train loss:0.9062524983855091\n",
      "train loss:0.8688474926473887\n",
      "train loss:0.899541356153924\n",
      "train loss:0.8527515309387594\n",
      "train loss:1.0149278344640755\n",
      "train loss:0.7140470461987459\n",
      "train loss:0.8532486555997226\n",
      "train loss:0.8807435026314212\n",
      "train loss:0.8509538477257453\n",
      "train loss:0.8402057428815126\n",
      "train loss:0.7409428635121412\n",
      "train loss:0.9265773762930428\n",
      "train loss:0.8459409061873913\n",
      "train loss:0.810627471910205\n",
      "train loss:0.897235941333049\n",
      "train loss:0.757461086238892\n",
      "train loss:0.8101340549763124\n",
      "train loss:0.9257584555729887\n",
      "train loss:0.9192213054583823\n",
      "train loss:0.8977190834162319\n",
      "train loss:0.8456814118767952\n",
      "train loss:0.9194588160990379\n",
      "train loss:0.9588580560563444\n",
      "train loss:1.0968357380688765\n",
      "train loss:0.8852827330728172\n",
      "train loss:0.7718455060959528\n",
      "train loss:1.0062135374789027\n",
      "train loss:0.8712425072947049\n",
      "train loss:0.9141135448750204\n",
      "train loss:0.9564815994135849\n",
      "train loss:0.9134728670811539\n",
      "train loss:0.7993891665870554\n",
      "train loss:0.7139794192075213\n",
      "train loss:0.7481650877211993\n",
      "train loss:0.7068778624525492\n",
      "train loss:0.9142130219768121\n",
      "train loss:0.8425429059208918\n",
      "train loss:0.7579134607164528\n",
      "train loss:0.7713774693426549\n",
      "train loss:0.9068213223742454\n",
      "train loss:0.9753628541096596\n",
      "train loss:0.904398893308041\n",
      "train loss:0.8601504908830762\n",
      "train loss:0.7914273564279748\n",
      "train loss:0.9134522402651437\n",
      "train loss:0.927098471587742\n",
      "train loss:0.8261369340478847\n",
      "train loss:1.0274222602612841\n",
      "train loss:0.820989862805014\n",
      "train loss:0.8844937584665666\n",
      "train loss:0.99831117826783\n",
      "train loss:0.8405744930922374\n",
      "train loss:0.8432644763239874\n",
      "train loss:0.8464314972449347\n",
      "train loss:0.8837108431769087\n",
      "train loss:0.9192289594279082\n",
      "train loss:0.8910872981981606\n",
      "train loss:0.8085962019053388\n",
      "train loss:0.9601093802817406\n",
      "train loss:0.9214369233605693\n",
      "train loss:0.9155599121365973\n",
      "train loss:0.8913939618805578\n",
      "train loss:0.7858281395767263\n",
      "train loss:0.7812403760003193\n",
      "train loss:1.0180055770345442\n",
      "train loss:1.0307001717103395\n",
      "train loss:1.0598243068901017\n",
      "train loss:1.0019147410280655\n",
      "train loss:0.9106329581673281\n",
      "train loss:0.8048892665434224\n",
      "train loss:0.8366845105501571\n",
      "train loss:0.962750890734661\n",
      "train loss:0.8551549847954507\n",
      "train loss:0.9589258718207485\n",
      "train loss:0.9388569140551071\n",
      "train loss:0.8641980940038573\n",
      "train loss:0.8103355674331039\n",
      "train loss:0.8109142847635937\n",
      "train loss:0.9708474301884141\n",
      "train loss:0.8373905997569744\n",
      "train loss:0.9634586028490159\n",
      "train loss:0.9802125102343417\n",
      "train loss:1.0183704242538503\n",
      "train loss:0.8083958277759491\n",
      "train loss:0.7778375637993481\n",
      "train loss:0.7503736202053624\n",
      "train loss:0.757689608310652\n",
      "train loss:0.9217059046344315\n",
      "train loss:0.9754748062477634\n",
      "train loss:0.8433717542022809\n",
      "train loss:1.0194930358915826\n",
      "train loss:0.8521123412262925\n",
      "train loss:0.8253173273213047\n",
      "train loss:0.8270705146055249\n",
      "train loss:0.998740973345887\n",
      "train loss:0.803860424531677\n",
      "train loss:0.8954032207443048\n",
      "train loss:1.0227092296124223\n",
      "train loss:0.707771330943553\n",
      "train loss:1.1458396417817378\n",
      "train loss:0.9633226810988228\n",
      "train loss:1.0465756951970646\n",
      "train loss:0.8893073956105717\n",
      "train loss:0.8858605642391985\n",
      "train loss:0.7609132355245035\n",
      "train loss:1.042551652020757\n",
      "train loss:0.8704708868119672\n",
      "train loss:0.9752485130526943\n",
      "train loss:0.9306147710804905\n",
      "train loss:1.0143794058544173\n",
      "train loss:0.9076907267243789\n",
      "train loss:0.9496443773830912\n",
      "train loss:0.8852517919679584\n",
      "train loss:0.7958662021530166\n",
      "train loss:0.916685896888774\n",
      "train loss:0.78853062684524\n",
      "train loss:1.0950531713690241\n",
      "train loss:1.1339580828253901\n",
      "train loss:0.9252036774723474\n",
      "train loss:0.9942707793215798\n",
      "train loss:0.8323075823328197\n",
      "train loss:1.0695800786346006\n",
      "train loss:0.9265059806617972\n",
      "train loss:1.0239689955500506\n",
      "train loss:0.6434610790658202\n",
      "train loss:0.8021599447368634\n",
      "train loss:0.9466735187351887\n",
      "train loss:0.8732095352619834\n",
      "train loss:1.0671829195485945\n",
      "train loss:0.9716266480451239\n",
      "train loss:0.773901373594713\n",
      "train loss:0.9727658499338253\n",
      "train loss:0.8756691846232307\n",
      "train loss:0.9926711591355765\n",
      "train loss:0.9347343486703322\n",
      "train loss:0.9019293033852962\n",
      "train loss:0.8954071462563554\n",
      "train loss:0.9204653330613393\n",
      "train loss:0.9596415960007814\n",
      "train loss:0.8055536258272027\n",
      "train loss:0.9257910176935128\n",
      "train loss:0.9799856057865287\n",
      "train loss:0.9331072891282167\n",
      "train loss:0.8529396830777407\n",
      "train loss:1.220886857519003\n",
      "train loss:0.7716452445742781\n",
      "train loss:0.8970510492382674\n",
      "train loss:1.0162444173187342\n",
      "train loss:0.878215228046539\n",
      "train loss:1.01419265667547\n",
      "train loss:0.9038163157223252\n",
      "train loss:0.9922907474916332\n",
      "train loss:1.0109539025256877\n",
      "train loss:0.9610230751325642\n",
      "train loss:0.930110150747166\n",
      "train loss:0.8701247794265988\n",
      "train loss:1.0242721873241016\n",
      "train loss:0.9772304151842092\n",
      "train loss:0.9740715174791245\n",
      "train loss:0.8115626433495008\n",
      "train loss:0.9540042722539304\n",
      "train loss:0.928031888078068\n",
      "train loss:0.775287006310176\n",
      "train loss:0.8285073720442891\n",
      "train loss:0.8809992273938002\n",
      "train loss:0.9209100100460289\n",
      "train loss:0.923522926433159\n",
      "train loss:0.8986087804460429\n",
      "train loss:0.8471561965287845\n",
      "train loss:0.9931583156409215\n",
      "train loss:0.856658092526144\n",
      "train loss:1.1221418529608145\n",
      "train loss:0.890392845732817\n",
      "train loss:0.9820936717524387\n",
      "train loss:0.9445198882971333\n",
      "train loss:0.9230240983835551\n",
      "train loss:0.7749928329538247\n",
      "train loss:0.9539722817904333\n",
      "train loss:0.945289325379882\n",
      "train loss:0.8615169856987397\n",
      "train loss:0.8568316437629235\n",
      "train loss:0.8768738490076121\n",
      "train loss:0.9182278673875569\n",
      "train loss:0.8880461453850504\n",
      "train loss:0.8973449554212426\n",
      "train loss:1.054339031897172\n",
      "train loss:0.796256770438425\n",
      "train loss:0.9649028811517494\n",
      "train loss:0.8896491216918911\n",
      "train loss:0.8657724991024932\n",
      "train loss:0.9422964986953918\n",
      "train loss:0.9435248596466296\n",
      "train loss:0.7506157520889307\n",
      "train loss:0.9438108754062329\n",
      "train loss:0.7411703839737092\n",
      "train loss:0.8660934408195634\n",
      "train loss:0.9128405359652747\n",
      "train loss:0.9207227254672088\n",
      "train loss:1.0669764794326932\n",
      "train loss:0.919894981940041\n",
      "train loss:1.0184408902509994\n",
      "train loss:0.9484158660647666\n",
      "train loss:0.7407101249638944\n",
      "train loss:0.849694111384335\n",
      "train loss:0.9145198213242774\n",
      "train loss:0.8532321922927237\n",
      "train loss:0.8712407314776685\n",
      "train loss:0.9582282419913725\n",
      "train loss:0.8740464568325357\n",
      "train loss:0.9526340204090928\n",
      "train loss:0.8654138391835076\n",
      "train loss:0.8258622133151781\n",
      "train loss:0.9722511955645008\n",
      "train loss:1.054875637067411\n",
      "train loss:0.8777057369019904\n",
      "train loss:0.8721631711894071\n",
      "train loss:0.811533110044806\n",
      "train loss:0.8807035017379269\n",
      "train loss:1.0495049235672296\n",
      "train loss:0.7577618778782411\n",
      "train loss:0.7987955049940713\n",
      "train loss:0.9243657393533486\n",
      "train loss:0.8105776790124319\n",
      "train loss:0.8700817055632335\n",
      "train loss:0.8891649209425225\n",
      "train loss:0.8631809527972831\n",
      "train loss:1.0013983988906519\n",
      "train loss:0.9104232278801825\n",
      "train loss:0.8229114852142834\n",
      "train loss:1.0746221135074665\n",
      "train loss:0.9824913259724366\n",
      "train loss:0.9398998434353549\n",
      "train loss:1.0109327359114335\n",
      "train loss:0.8565986939582527\n",
      "train loss:0.8453455425533376\n",
      "train loss:1.0549304144117317\n",
      "train loss:0.872269694171829\n",
      "train loss:1.0414334718846738\n",
      "train loss:1.0051478790814747\n",
      "train loss:0.7907297514247486\n",
      "train loss:0.8308758988874776\n",
      "train loss:0.9204939488557641\n",
      "train loss:0.8030695074160557\n",
      "train loss:0.7840680696554334\n",
      "train loss:0.9084248754004685\n",
      "train loss:0.9101638980617974\n",
      "train loss:0.8251273720457519\n",
      "train loss:0.7788629842860385\n",
      "train loss:1.0162694375928079\n",
      "train loss:0.838777616340696\n",
      "train loss:0.8796713518189558\n",
      "train loss:0.9468421055621863\n",
      "train loss:0.9427865936474946\n",
      "train loss:0.9238113638391036\n",
      "train loss:0.8963136191231229\n",
      "train loss:0.9310186458304852\n",
      "train loss:1.0180543773639388\n",
      "train loss:0.8849456648788997\n",
      "train loss:0.767534618239952\n",
      "train loss:0.9941582121768039\n",
      "train loss:0.8217995905765306\n",
      "train loss:0.8660017605816026\n",
      "train loss:0.862875576739502\n",
      "train loss:1.0098184168903372\n",
      "train loss:0.8910318714582318\n",
      "train loss:0.8800554251727316\n",
      "train loss:0.8004665355875441\n",
      "train loss:0.9597012422452209\n",
      "train loss:0.9787206780703346\n",
      "train loss:0.9566528274489176\n",
      "train loss:0.9221839480745243\n",
      "train loss:0.8974494396403127\n",
      "train loss:1.0271306329560363\n",
      "train loss:0.9541426901503455\n",
      "train loss:0.8557862134596932\n",
      "train loss:0.9904408219681414\n",
      "train loss:0.9415139380306674\n",
      "train loss:0.9850038471066354\n",
      "train loss:1.0140121313455657\n",
      "train loss:0.7022975259179418\n",
      "train loss:0.8406552323310379\n",
      "train loss:0.9152144354667727\n",
      "train loss:0.9505788021398404\n",
      "train loss:0.860470198019466\n",
      "train loss:0.9516454094180169\n",
      "train loss:0.9226286501833603\n",
      "train loss:0.8610466291698748\n",
      "train loss:0.9919154316310507\n",
      "train loss:0.8212792246605366\n",
      "train loss:0.8600492455438379\n",
      "train loss:0.9547498847038054\n",
      "train loss:0.9004964440683412\n",
      "train loss:0.9436212008722435\n",
      "train loss:0.9502392164925731\n",
      "train loss:0.6559036386712549\n",
      "train loss:0.85342491160312\n",
      "train loss:1.0029542499636286\n",
      "train loss:0.7959612222777867\n",
      "train loss:0.8954555383820241\n",
      "train loss:0.9035288446360581\n",
      "train loss:0.846306441898812\n",
      "train loss:0.6901828174476387\n",
      "train loss:1.0124303359042945\n",
      "train loss:1.087284599634372\n",
      "train loss:0.8922987402826571\n",
      "train loss:0.8040937368196139\n",
      "train loss:0.8058572159751144\n",
      "train loss:0.8494009773523374\n",
      "train loss:0.8542674306665383\n",
      "train loss:0.8627030348796808\n",
      "train loss:0.977670107984907\n",
      "train loss:0.9256374855754652\n",
      "train loss:0.9047452337406874\n",
      "train loss:0.8750422141904934\n",
      "train loss:0.9479979393807768\n",
      "train loss:0.9448318857364554\n",
      "train loss:0.901442595890089\n",
      "train loss:1.0317408344842511\n",
      "train loss:0.7894899236836639\n",
      "train loss:0.8571165754357065\n",
      "train loss:0.9767300654491526\n",
      "train loss:0.980912245267527\n",
      "train loss:0.6846226194089283\n",
      "train loss:0.8432151899051225\n",
      "train loss:0.8776690393201677\n",
      "train loss:0.818477890401326\n",
      "train loss:0.9530995895916446\n",
      "train loss:0.8557359057413193\n",
      "train loss:0.9627793911408317\n",
      "train loss:0.9394485076151171\n",
      "train loss:0.8820790499179761\n",
      "train loss:1.002375577402165\n",
      "train loss:0.6945398661726742\n",
      "train loss:0.7607188740226195\n",
      "train loss:0.8663199192827581\n",
      "train loss:0.9121310907256611\n",
      "train loss:0.8333123140089453\n",
      "train loss:0.906133342879462\n",
      "train loss:0.9229187872120318\n",
      "train loss:0.7834586030554412\n",
      "train loss:0.9043748638129973\n",
      "train loss:0.9423860328151608\n",
      "train loss:0.6551479218260997\n",
      "train loss:0.8957588081193478\n",
      "train loss:0.866754098142532\n",
      "train loss:0.8113430684939806\n",
      "train loss:1.005470965105596\n",
      "train loss:1.0413446559042003\n",
      "train loss:0.7735722976627076\n",
      "train loss:0.9174472169361807\n",
      "train loss:0.7892539660173535\n",
      "train loss:0.8846009326300611\n",
      "train loss:0.925077509782735\n",
      "train loss:0.8771986743074072\n",
      "train loss:0.8190211326376796\n",
      "train loss:0.8935776738725465\n",
      "train loss:0.8458755243350901\n",
      "train loss:0.9735532449153764\n",
      "train loss:0.9403446373427397\n",
      "train loss:0.9070682036558583\n",
      "train loss:0.7379682336754747\n",
      "train loss:0.8141861874531859\n",
      "train loss:1.0139320905762963\n",
      "train loss:0.8655398228324864\n",
      "train loss:0.9709328952408\n",
      "train loss:0.985899608313502\n",
      "train loss:0.7312922560952942\n",
      "train loss:0.865437049465538\n",
      "train loss:1.016966117772321\n",
      "train loss:0.8573692941021612\n",
      "train loss:0.8733575761459131\n",
      "train loss:0.8555294235740137\n",
      "train loss:0.9123460015058744\n",
      "train loss:1.0230465398649364\n",
      "train loss:0.8338801324928241\n",
      "train loss:0.8548823080527878\n",
      "train loss:0.8796140408860336\n",
      "train loss:0.7041029976857888\n",
      "train loss:0.9580251069926935\n",
      "train loss:0.8222009505864762\n",
      "train loss:0.9943958426812779\n",
      "train loss:1.0380968150298135\n",
      "train loss:0.8717758259313639\n",
      "train loss:0.9378691735209661\n",
      "train loss:0.8154156826093435\n",
      "train loss:0.9757426744654186\n",
      "train loss:0.8921452394077518\n",
      "train loss:1.1175767991357106\n",
      "train loss:0.7647271394089161\n",
      "train loss:0.8708755808399161\n",
      "train loss:0.9039024001403607\n",
      "train loss:0.8668859709458551\n",
      "train loss:0.9013288409898434\n",
      "train loss:0.8115544948455663\n",
      "train loss:1.049084861298291\n",
      "train loss:0.9877918540918281\n",
      "train loss:0.8902116106864615\n",
      "train loss:0.8849442306286894\n",
      "train loss:0.881318180610694\n",
      "train loss:0.9967363959542846\n",
      "train loss:1.1119706817632538\n",
      "train loss:0.8848909392377619\n",
      "train loss:1.0649269040565497\n",
      "train loss:0.9320762139707334\n",
      "train loss:0.872692236767349\n",
      "train loss:1.0288597695355843\n",
      "train loss:0.8263638176785107\n",
      "train loss:0.7633857303685565\n",
      "train loss:0.7639927055936424\n",
      "train loss:0.9423538934686825\n",
      "train loss:0.9535732916221228\n",
      "train loss:0.962492848552603\n",
      "train loss:0.7436085863780716\n",
      "train loss:0.8278638831975101\n",
      "train loss:0.95125921909859\n",
      "train loss:0.7108982064989366\n",
      "train loss:0.7558105616144017\n",
      "train loss:0.8042407041796703\n",
      "train loss:0.9481217055095259\n",
      "train loss:0.8974485486581877\n",
      "train loss:0.9974183006680607\n",
      "train loss:0.778509348434389\n",
      "train loss:0.802819023739789\n",
      "train loss:0.9840101710170386\n",
      "train loss:0.890520847624661\n",
      "train loss:0.7790490631003351\n",
      "train loss:0.8966449987793161\n",
      "train loss:0.7961607148082582\n",
      "train loss:0.9544425004598451\n",
      "train loss:0.820730551691592\n",
      "train loss:0.7358041753560616\n",
      "train loss:0.941062186408739\n",
      "train loss:0.8436137956848934\n",
      "train loss:0.8788830677025193\n",
      "train loss:0.9657904853836391\n",
      "train loss:0.7740509727948216\n",
      "train loss:0.8557681612364684\n",
      "train loss:0.9514508774642445\n",
      "train loss:0.8524039801130737\n",
      "train loss:0.8672773206012228\n",
      "train loss:0.8494305138550073\n",
      "train loss:0.8594418960923272\n",
      "train loss:0.9013686484092394\n",
      "train loss:0.9454410189977189\n",
      "train loss:0.9689253076833323\n",
      "train loss:0.7509591249425996\n",
      "train loss:0.9928969358908306\n",
      "train loss:0.8637098345875331\n",
      "train loss:0.8331810170673128\n",
      "train loss:1.0806991009229774\n",
      "train loss:0.8186569449612641\n",
      "train loss:0.8607880115502522\n",
      "train loss:0.931516959614395\n",
      "train loss:0.7413306544761702\n",
      "train loss:0.9787535745209485\n",
      "train loss:0.9765039035708634\n",
      "train loss:0.8486504204580604\n",
      "train loss:0.7490190185809915\n",
      "train loss:0.9626346057878863\n",
      "train loss:0.814407056389858\n",
      "train loss:0.9970182901657796\n",
      "train loss:1.1395341268523895\n",
      "train loss:1.0465665650500995\n",
      "train loss:0.901455393334654\n",
      "train loss:0.8128030667773656\n",
      "train loss:0.9689727330642366\n",
      "train loss:0.9538028013690539\n",
      "train loss:0.7532786193987667\n",
      "train loss:0.8573670224439283\n",
      "train loss:0.9382723593959891\n",
      "train loss:0.9609894656964287\n",
      "train loss:1.0549905731794667\n",
      "train loss:1.0156852265517706\n",
      "train loss:0.9950230791652228\n",
      "train loss:0.9513917587151982\n",
      "train loss:0.9155216893284827\n",
      "train loss:0.898783939779973\n",
      "train loss:0.8340035245548455\n",
      "train loss:1.0022000057891989\n",
      "train loss:0.8177413945001262\n",
      "train loss:0.9992371471896836\n",
      "train loss:0.8338642845703275\n",
      "train loss:0.7447346374644193\n",
      "train loss:1.150470222827271\n",
      "train loss:0.9214177643544903\n",
      "train loss:0.9008549412929573\n",
      "train loss:0.8365970356589983\n",
      "train loss:1.038954471292881\n",
      "train loss:0.8181412594376369\n",
      "train loss:0.9673803929216676\n",
      "train loss:0.7365639953463935\n",
      "train loss:0.8152376430147588\n",
      "train loss:0.821143058384135\n",
      "train loss:0.7491520907243054\n",
      "train loss:0.8978377666172705\n",
      "train loss:0.9092197387727111\n",
      "train loss:0.9785586420390561\n",
      "train loss:0.8524201758071038\n",
      "train loss:0.9750668431527367\n",
      "train loss:0.9995699320451414\n",
      "train loss:0.8698885574313024\n",
      "train loss:0.9912723750995306\n",
      "train loss:0.7914704988683131\n",
      "train loss:0.8107398607250942\n",
      "train loss:0.9490661981841358\n",
      "train loss:0.9038824579261882\n",
      "train loss:0.8481551047760323\n",
      "train loss:0.9236003136311516\n",
      "train loss:0.9741478324803075\n",
      "train loss:0.8005867478542602\n",
      "train loss:0.9254066566681937\n",
      "train loss:0.9269801460774643\n",
      "train loss:0.5819631161220219\n",
      "train loss:0.8868329777831292\n",
      "train loss:0.7937594552691268\n",
      "train loss:0.9499467194552424\n",
      "train loss:0.9033636995174887\n",
      "train loss:0.8179369144100272\n",
      "train loss:0.8468607355922678\n",
      "train loss:0.9722202398029759\n",
      "train loss:1.0371553014009784\n",
      "train loss:0.8521195759559559\n",
      "train loss:0.7563467866086836\n",
      "train loss:0.9036464794440565\n",
      "train loss:0.8686217081826737\n",
      "train loss:0.9472533536517763\n",
      "train loss:0.9843657933713204\n",
      "train loss:0.8071788889641363\n",
      "train loss:1.0121005772154574\n",
      "train loss:0.9065429221208856\n",
      "train loss:0.8406301325720621\n",
      "train loss:0.910177473183499\n",
      "train loss:0.7937467979273287\n",
      "train loss:0.8875765303675416\n",
      "train loss:0.8948485465214336\n",
      "train loss:0.840958333355125\n",
      "train loss:1.0305638338965897\n",
      "train loss:0.9320602700966735\n",
      "train loss:0.8847171371087125\n",
      "train loss:0.8814096902274244\n",
      "train loss:0.9881327314840546\n",
      "train loss:0.6693223407221635\n",
      "train loss:0.8787160338074358\n",
      "train loss:0.8772202717082166\n",
      "train loss:0.9440197124959194\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9924\n",
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 \n",
    "# 학습 데이터: x_train, t_train, 평가 데이터: x_test, t_test\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()  \n",
    "\n",
    "# 에폭 수: 10, 미니배치 사이즈: 100, 최적화기법: Adam(Ir: 0.001) \n",
    "# 매 에폭 시 평가에 사용되는 샘플 수: 1000   \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=10, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "# Trainer 클래스 train 함수 호출  \n",
    "trainer.train()\n",
    "\n",
    "# test 데이터 정확도 리스트 저장\n",
    "deep_layered_conv_net_result = trainer.test_acc_list\n",
    "\n",
    "# 매개변수 보관\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple CNN, DeepLayered CNN 정확도 비교 그래프\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, simple_conv_net_result, marker='o', label='Simple ConvNet', markevery=2)\n",
    "plt.plot(x, deep_layered_conv_net_result, marker='s', label='Deep ConvNet', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution, Pooling 층을 하나 더 늘렸다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1}, # filter_num: 필터 수, filter_size: 필터 크기, pad: 패딩, stride: 스트라이드\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2 * filter_pad) // filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size / 2) * (conv_output_size / 2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, filter_num, filter_size, filter_size)\n",
    "        self.params['b2'] = np.zeros(filter_num)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b3'] = np.zeros(hidden_size)\n",
    "        self.params['W4'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b4'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Conv2'] = Convolution(self.params['W2'], self.params['b2'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Pool2'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.layers['Relu3'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W4'], self.params['b4'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()   # 출력층\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Conv2'].dW, self.layers['Conv2'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W4'], grads['b4'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_mnist(flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.301768735863214\n",
      "=== epoch:1, train acc:0.115, test acc:0.125 ===\n",
      "train loss:2.301434872208837\n",
      "train loss:2.3023362277382597\n",
      "train loss:2.3020020564546657\n",
      "train loss:2.302635702935197\n",
      "train loss:2.301777888830582\n",
      "train loss:2.302057188590069\n",
      "train loss:2.3023726768877517\n",
      "train loss:2.3028244411978984\n",
      "train loss:2.3026815636208227\n",
      "train loss:2.3017038835077517\n",
      "train loss:2.3022424248949713\n",
      "train loss:2.302327447827497\n",
      "train loss:2.3025167797414428\n",
      "train loss:2.301678171765047\n",
      "train loss:2.302536713291569\n",
      "train loss:2.3018622368318784\n",
      "train loss:2.3016719328622335\n",
      "train loss:2.301218058028245\n",
      "train loss:2.302940699353775\n",
      "train loss:2.301144457125731\n",
      "train loss:2.300344730780859\n",
      "train loss:2.3013347199754204\n",
      "train loss:2.2987714530755787\n",
      "train loss:2.3039607200627925\n",
      "train loss:2.301171202532858\n",
      "train loss:2.302113183671006\n",
      "train loss:2.3024882489320664\n",
      "train loss:2.299119266631311\n",
      "train loss:2.3002439302152946\n",
      "train loss:2.303765107730723\n",
      "train loss:2.301664514641411\n",
      "train loss:2.3000485862631086\n",
      "train loss:2.302581214249426\n",
      "train loss:2.302979341609994\n",
      "train loss:2.3032821649746453\n",
      "train loss:2.3021130602460653\n",
      "train loss:2.3010843539287094\n",
      "train loss:2.3016225748101924\n",
      "train loss:2.299309101566144\n",
      "train loss:2.300173443075105\n",
      "train loss:2.301711745455378\n",
      "train loss:2.301832863515646\n",
      "train loss:2.3018269891303387\n",
      "train loss:2.3013225472893764\n",
      "train loss:2.3041241299256114\n",
      "train loss:2.3036602742709444\n",
      "train loss:2.302383847763524\n",
      "train loss:2.3022823359776265\n",
      "train loss:2.3015425057538916\n",
      "train loss:2.3016793070089263\n",
      "train loss:2.304917719494795\n",
      "train loss:2.2996930777632842\n",
      "train loss:2.300860759157934\n",
      "train loss:2.302558061095362\n",
      "train loss:2.3041909182620874\n",
      "train loss:2.3022440019557737\n",
      "train loss:2.3040516084914917\n",
      "train loss:2.3030903277907324\n",
      "train loss:2.3010346337999708\n",
      "train loss:2.3006222885656973\n",
      "train loss:2.301666132465958\n",
      "train loss:2.3028165911824807\n",
      "train loss:2.3045301000382734\n",
      "train loss:2.3015038985950937\n",
      "train loss:2.3020226247336444\n",
      "train loss:2.3004994871040703\n",
      "train loss:2.3013769224138967\n",
      "train loss:2.301434939017252\n",
      "train loss:2.3010453286364303\n",
      "train loss:2.299024132066073\n",
      "train loss:2.3019032356003666\n",
      "train loss:2.3056034757667923\n",
      "train loss:2.3024728705751856\n",
      "train loss:2.30099483306157\n",
      "train loss:2.3033656554167\n",
      "train loss:2.29840006098504\n",
      "train loss:2.301915737982312\n",
      "train loss:2.3021593422539803\n",
      "train loss:2.2982534197428572\n",
      "train loss:2.30339648823215\n",
      "train loss:2.2997111913569537\n",
      "train loss:2.3022431906961103\n",
      "train loss:2.3021816910975437\n",
      "train loss:2.3013987654239183\n",
      "train loss:2.300523323634493\n",
      "train loss:2.303171478423419\n",
      "train loss:2.2977619199409167\n",
      "train loss:2.3051009751920386\n",
      "train loss:2.299076442948797\n",
      "train loss:2.300511231609165\n",
      "train loss:2.298583287759844\n",
      "train loss:2.302879095158722\n",
      "train loss:2.3031163034050066\n",
      "train loss:2.304040592078144\n",
      "train loss:2.3018029035259775\n",
      "train loss:2.3041685207984406\n",
      "train loss:2.3077308020072147\n",
      "train loss:2.300343163566435\n",
      "train loss:2.3023457246010484\n",
      "train loss:2.30233159159145\n",
      "train loss:2.3017782915941893\n",
      "train loss:2.303217540472232\n",
      "train loss:2.3011919273913777\n",
      "train loss:2.2984750300867836\n",
      "train loss:2.2990210186862527\n",
      "train loss:2.301402149720353\n",
      "train loss:2.301255364550439\n",
      "train loss:2.30445710033833\n",
      "train loss:2.302186170131042\n",
      "train loss:2.3018674991254424\n",
      "train loss:2.302998950180749\n",
      "train loss:2.3071212005004664\n",
      "train loss:2.302207039540173\n",
      "train loss:2.305493184992509\n",
      "train loss:2.301701349542012\n",
      "train loss:2.3048486108454713\n",
      "train loss:2.3019601475937606\n",
      "train loss:2.306936308092008\n",
      "train loss:2.3033807195793057\n",
      "train loss:2.3012381898203325\n",
      "train loss:2.3021001569297415\n",
      "train loss:2.30609289491011\n",
      "train loss:2.302108267874178\n",
      "train loss:2.304289030862307\n",
      "train loss:2.3008347664468696\n",
      "train loss:2.302533136147972\n",
      "train loss:2.2988250600287308\n",
      "train loss:2.300807215925262\n",
      "train loss:2.3008299927511637\n",
      "train loss:2.3035912592576473\n",
      "train loss:2.300165788678514\n",
      "train loss:2.3038860666322396\n",
      "train loss:2.305249384528877\n",
      "train loss:2.297699558074851\n",
      "train loss:2.3016070396175072\n",
      "train loss:2.3009650204829337\n",
      "train loss:2.3016103389039375\n",
      "train loss:2.299122741444563\n",
      "train loss:2.3054171555606184\n",
      "train loss:2.299544916015356\n",
      "train loss:2.3037274573901576\n",
      "train loss:2.3031708996113687\n",
      "train loss:2.2991873706266555\n",
      "train loss:2.302643919632051\n",
      "train loss:2.3044623537017435\n",
      "train loss:2.303477191997138\n",
      "train loss:2.3014934895201415\n",
      "train loss:2.3041415563393515\n",
      "train loss:2.3002830473951894\n",
      "train loss:2.3042578215600487\n",
      "train loss:2.300603174595018\n",
      "train loss:2.300550258209448\n",
      "train loss:2.3033574057307677\n",
      "train loss:2.302097339057704\n",
      "train loss:2.300829820232295\n",
      "train loss:2.3015071361773947\n",
      "train loss:2.301398161105703\n",
      "train loss:2.3035903894324266\n",
      "train loss:2.302123661931802\n",
      "train loss:2.300320202667273\n",
      "train loss:2.3022724191168713\n",
      "train loss:2.298860247087553\n",
      "train loss:2.3021693481406844\n",
      "train loss:2.3019965538117786\n",
      "train loss:2.299195361961279\n",
      "train loss:2.298539300130022\n",
      "train loss:2.301946876956312\n",
      "train loss:2.3005012671088245\n",
      "train loss:2.301605216216821\n",
      "train loss:2.296911427607191\n",
      "train loss:2.2956242484677722\n",
      "train loss:2.298844037446022\n",
      "train loss:2.2998924279201214\n",
      "train loss:2.2992293501460717\n",
      "train loss:2.302133113605673\n",
      "train loss:2.298191245553107\n",
      "train loss:2.3091783278577984\n",
      "train loss:2.303589593717983\n",
      "train loss:2.305341324293712\n",
      "train loss:2.3040310918550775\n",
      "train loss:2.299575366432369\n",
      "train loss:2.2949787616010076\n",
      "train loss:2.301281427142581\n",
      "train loss:2.301240191674145\n",
      "train loss:2.2999750487645283\n",
      "train loss:2.3034793979753303\n",
      "train loss:2.2991557190152703\n",
      "train loss:2.3032465784702487\n",
      "train loss:2.3025127972534007\n",
      "train loss:2.306639850941128\n",
      "train loss:2.30250450833538\n",
      "train loss:2.301214066814275\n",
      "train loss:2.3041920984173894\n",
      "train loss:2.301427080471001\n",
      "train loss:2.3032369917795616\n",
      "train loss:2.301821910726593\n",
      "train loss:2.2977615059499064\n",
      "train loss:2.30052876410892\n",
      "train loss:2.296116521301114\n",
      "train loss:2.303248951555725\n",
      "train loss:2.3023291801155104\n",
      "train loss:2.2985190952869434\n",
      "train loss:2.30463276051398\n",
      "train loss:2.304152815413252\n",
      "train loss:2.302325267150959\n",
      "train loss:2.30638700219108\n",
      "train loss:2.2983659021942575\n",
      "train loss:2.3001555512531073\n",
      "train loss:2.303745766971402\n",
      "train loss:2.3008787714368304\n",
      "train loss:2.304243262788278\n",
      "train loss:2.303542818475689\n",
      "train loss:2.299667584175058\n",
      "train loss:2.3055265624038683\n",
      "train loss:2.3025906220472105\n",
      "train loss:2.3015853486493167\n",
      "train loss:2.3025592713409897\n",
      "train loss:2.299081527841728\n",
      "train loss:2.30003856396172\n",
      "train loss:2.3049485656197666\n",
      "train loss:2.3017305364183316\n",
      "train loss:2.298619106395661\n",
      "train loss:2.302583804749246\n",
      "train loss:2.3059745433264154\n",
      "train loss:2.2970543491625395\n",
      "train loss:2.3034823529099926\n",
      "train loss:2.3087125770119834\n",
      "train loss:2.2983253970841004\n",
      "train loss:2.2991331653488283\n",
      "train loss:2.301063975994091\n",
      "train loss:2.304936229715518\n",
      "train loss:2.299120601757656\n",
      "train loss:2.304203239173273\n",
      "train loss:2.301117167501029\n",
      "train loss:2.2987130500972612\n",
      "train loss:2.303775954377189\n",
      "train loss:2.299307032263823\n",
      "train loss:2.302374834060629\n",
      "train loss:2.3010684838087934\n",
      "train loss:2.294766619785821\n",
      "train loss:2.3035516443232362\n",
      "train loss:2.3010681952687553\n",
      "train loss:2.305790089825885\n",
      "train loss:2.2992134727502016\n",
      "train loss:2.304297915268938\n",
      "train loss:2.3009524056969957\n",
      "train loss:2.298850965243342\n",
      "train loss:2.298769677449517\n",
      "train loss:2.3017443505441593\n",
      "train loss:2.3029113208973264\n",
      "train loss:2.3001754146127507\n",
      "train loss:2.298952823082991\n",
      "train loss:2.3026098228814704\n",
      "train loss:2.2975681289809033\n",
      "train loss:2.3016142701939297\n",
      "train loss:2.3013324274524334\n",
      "train loss:2.303014921044802\n",
      "train loss:2.300940708332538\n",
      "train loss:2.3048730989812576\n",
      "train loss:2.299392557452842\n",
      "train loss:2.299296417086543\n",
      "train loss:2.299806767577689\n",
      "train loss:2.3003526607517353\n",
      "train loss:2.3060441479810403\n",
      "train loss:2.3017445363431803\n",
      "train loss:2.301632779879815\n",
      "train loss:2.303109259736748\n",
      "train loss:2.3041783519279018\n",
      "train loss:2.298385480085803\n",
      "train loss:2.3032230421277067\n",
      "train loss:2.3024568668156222\n",
      "train loss:2.3002007591272657\n",
      "train loss:2.30859978118667\n",
      "train loss:2.2982279607322647\n",
      "train loss:2.2999863706267165\n",
      "train loss:2.300597033776193\n",
      "train loss:2.306673853886208\n",
      "train loss:2.3044638824881964\n",
      "train loss:2.297740139824157\n",
      "train loss:2.2994424560931392\n",
      "train loss:2.3108579127684106\n",
      "train loss:2.300171778972753\n",
      "train loss:2.309483878008669\n",
      "train loss:2.300062392641749\n",
      "train loss:2.297966343046732\n",
      "train loss:2.3013466835856864\n",
      "train loss:2.2969285490455946\n",
      "train loss:2.3091918415090316\n",
      "train loss:2.3027328036975296\n",
      "train loss:2.3051874301920434\n",
      "train loss:2.303578528903197\n",
      "train loss:2.298598476311788\n",
      "train loss:2.3022318813146834\n",
      "train loss:2.3017494605223012\n",
      "train loss:2.295909435194056\n",
      "train loss:2.306967036389762\n",
      "train loss:2.304636723335246\n",
      "train loss:2.2961677385640002\n",
      "train loss:2.3005186987291157\n",
      "train loss:2.307874571170843\n",
      "train loss:2.2992717294457434\n",
      "train loss:2.299049575607656\n",
      "train loss:2.298358349805577\n",
      "train loss:2.298061891030683\n",
      "train loss:2.3030901162472848\n",
      "train loss:2.2977169091695866\n",
      "train loss:2.3039388246076804\n",
      "train loss:2.300432767144861\n",
      "train loss:2.2987225560421463\n",
      "train loss:2.303093560390372\n",
      "train loss:2.3050471535927133\n",
      "train loss:2.305068703352045\n",
      "train loss:2.2978000124334788\n",
      "train loss:2.3013415989588397\n",
      "train loss:2.3010825157003545\n",
      "train loss:2.3044973699002598\n",
      "train loss:2.299769106104202\n",
      "train loss:2.3015109096220296\n",
      "train loss:2.30495722018967\n",
      "train loss:2.302829497768334\n",
      "train loss:2.3043596664405306\n",
      "train loss:2.2995536544341375\n",
      "train loss:2.305021319544543\n",
      "train loss:2.3035134823714305\n",
      "train loss:2.2956809055799656\n",
      "train loss:2.3057061229973566\n",
      "train loss:2.30366958815726\n",
      "train loss:2.3033652335846018\n",
      "train loss:2.303935990645897\n",
      "train loss:2.2973968342272952\n",
      "train loss:2.298691300105562\n",
      "train loss:2.3048802422658365\n",
      "train loss:2.30097312463894\n",
      "train loss:2.299613186954844\n",
      "train loss:2.303116617252069\n",
      "train loss:2.3050389139621728\n",
      "train loss:2.2979804599623543\n",
      "train loss:2.3004374375742107\n",
      "train loss:2.3013185342153246\n",
      "train loss:2.2948703665689414\n",
      "train loss:2.304643800844283\n",
      "train loss:2.2979428195576945\n",
      "train loss:2.3037956352188247\n",
      "train loss:2.3076508720850435\n",
      "train loss:2.2991295636302973\n",
      "train loss:2.2963454029689943\n",
      "train loss:2.306071702581563\n",
      "train loss:2.2953405140686836\n",
      "train loss:2.298034319899468\n",
      "train loss:2.2962571559773837\n",
      "train loss:2.3030951317851613\n",
      "train loss:2.2954894707317113\n",
      "train loss:2.3021966503194915\n",
      "train loss:2.309691018177029\n",
      "train loss:2.299158584804723\n",
      "train loss:2.304751166035317\n",
      "train loss:2.30142942052352\n",
      "train loss:2.30595854884188\n",
      "train loss:2.296833221206337\n",
      "train loss:2.300928394180039\n",
      "train loss:2.308769652690473\n",
      "train loss:2.302057174508799\n",
      "train loss:2.3050586369427144\n",
      "train loss:2.3075787023643475\n",
      "train loss:2.304443163827594\n",
      "train loss:2.3056754883705888\n",
      "train loss:2.304095605293604\n",
      "train loss:2.2951557401610088\n",
      "train loss:2.297141227558295\n",
      "train loss:2.2952992996873185\n",
      "train loss:2.2971997563304454\n",
      "train loss:2.299394011910021\n",
      "train loss:2.304176942122503\n",
      "train loss:2.3031885452247196\n",
      "train loss:2.300591990121366\n",
      "train loss:2.3062289229772883\n",
      "train loss:2.3036342983849893\n",
      "train loss:2.3114240519500107\n",
      "train loss:2.2961634790657035\n",
      "train loss:2.3087092914767067\n",
      "train loss:2.3084307778187854\n",
      "train loss:2.2995003306513286\n",
      "train loss:2.3043616794153117\n",
      "train loss:2.3069072770029297\n",
      "train loss:2.3037068018788736\n",
      "train loss:2.3025769329490697\n",
      "train loss:2.29993522074494\n",
      "train loss:2.2976136335619235\n",
      "train loss:2.303203130486822\n",
      "train loss:2.306845969190017\n",
      "train loss:2.2976926760799032\n",
      "train loss:2.2996051020590285\n",
      "train loss:2.2985270667981266\n",
      "train loss:2.2947379213441637\n",
      "train loss:2.297935498379255\n",
      "train loss:2.2983195068872835\n",
      "train loss:2.3068897469634306\n",
      "train loss:2.2989996066659373\n",
      "train loss:2.301297257434387\n",
      "train loss:2.2990889746656857\n",
      "train loss:2.307197189354488\n",
      "train loss:2.3021524463236784\n",
      "train loss:2.3003809772757746\n",
      "train loss:2.304851688106333\n",
      "train loss:2.296163960620433\n",
      "train loss:2.29177504151638\n",
      "train loss:2.2983948541792074\n",
      "train loss:2.302253264487252\n",
      "train loss:2.2981914784152306\n",
      "train loss:2.300276753602941\n",
      "train loss:2.3028990883675142\n",
      "train loss:2.302985506588482\n",
      "train loss:2.2986518277684915\n",
      "train loss:2.306184916090765\n",
      "train loss:2.2889614613200706\n",
      "train loss:2.3002059321146824\n",
      "train loss:2.3072482870173\n",
      "train loss:2.2983215916235906\n",
      "train loss:2.306387500390427\n",
      "train loss:2.31007567827546\n",
      "train loss:2.305556679476674\n",
      "train loss:2.300793655388296\n",
      "train loss:2.306573488125756\n",
      "train loss:2.299802420952948\n",
      "train loss:2.293630559605207\n",
      "train loss:2.298619957324325\n",
      "train loss:2.294865607639078\n",
      "train loss:2.297405806448512\n",
      "train loss:2.2955987730070424\n",
      "train loss:2.2985432554389886\n",
      "train loss:2.2940390929666297\n",
      "train loss:2.297403538104852\n",
      "train loss:2.302602692068792\n",
      "train loss:2.296078723216866\n",
      "train loss:2.304982802240665\n",
      "train loss:2.304646290400278\n",
      "train loss:2.291566170608233\n",
      "train loss:2.3007481679536395\n",
      "train loss:2.3047655221354955\n",
      "train loss:2.2992963375697455\n",
      "train loss:2.2992538464561116\n",
      "train loss:2.300913530774711\n",
      "train loss:2.2995083888963377\n",
      "train loss:2.3000413820293772\n",
      "train loss:2.30754957501789\n",
      "train loss:2.3071373113921574\n",
      "train loss:2.300809045264322\n",
      "train loss:2.3000447290589436\n",
      "train loss:2.2955371997451084\n",
      "train loss:2.2996881663574094\n",
      "train loss:2.2985366998414207\n",
      "train loss:2.2986088943536824\n",
      "train loss:2.301328943821969\n",
      "train loss:2.3037879427580377\n",
      "train loss:2.29730215506621\n",
      "train loss:2.29615952306479\n",
      "train loss:2.3093818585639196\n",
      "train loss:2.3050325913225915\n",
      "train loss:2.3087182268730055\n",
      "train loss:2.2996067493178773\n",
      "train loss:2.3060316042683326\n",
      "train loss:2.302228222238626\n",
      "train loss:2.306910941806449\n",
      "train loss:2.2988905555427026\n",
      "train loss:2.296649907841074\n",
      "train loss:2.30327947258431\n",
      "train loss:2.301674725959078\n",
      "train loss:2.2963429534467075\n",
      "train loss:2.2912032804659965\n",
      "train loss:2.2983341935594344\n",
      "train loss:2.297977554126074\n",
      "train loss:2.3034444926940365\n",
      "train loss:2.303837475220146\n",
      "train loss:2.296042030650152\n",
      "train loss:2.295595323029982\n",
      "train loss:2.296987230270484\n",
      "train loss:2.301136413754355\n",
      "train loss:2.296754752808919\n",
      "train loss:2.309611821582744\n",
      "train loss:2.3021739020491268\n",
      "train loss:2.3071295790914914\n",
      "train loss:2.3002649738535785\n",
      "train loss:2.2935293409411512\n",
      "train loss:2.299809617093453\n",
      "train loss:2.2977103060892237\n",
      "train loss:2.2926404347595173\n",
      "train loss:2.29655477307907\n",
      "train loss:2.302023231462692\n",
      "train loss:2.300162017377109\n",
      "train loss:2.2958454604292706\n",
      "train loss:2.299007883633083\n",
      "train loss:2.298381622243535\n",
      "train loss:2.3035180876238583\n",
      "train loss:2.2938854546411545\n",
      "train loss:2.316021882447412\n",
      "train loss:2.308713615607733\n",
      "train loss:2.291747254429863\n",
      "train loss:2.3046309443611555\n",
      "train loss:2.3014282648588793\n",
      "train loss:2.299906495094545\n",
      "train loss:2.306487253006423\n",
      "train loss:2.293071829684621\n",
      "train loss:2.29223052331695\n",
      "train loss:2.297665433784223\n",
      "train loss:2.2913091233030363\n",
      "train loss:2.2939724455032513\n",
      "train loss:2.2924832362950007\n",
      "train loss:2.2979360150112784\n",
      "train loss:2.3030668395279577\n",
      "train loss:2.295980315989493\n",
      "train loss:2.3103993981542157\n",
      "train loss:2.305936138306672\n",
      "train loss:2.3011057276309734\n",
      "train loss:2.2821714396396233\n",
      "train loss:2.293402085809972\n",
      "train loss:2.2977550801204507\n",
      "train loss:2.303069253576835\n",
      "train loss:2.297479627849866\n",
      "train loss:2.2964550220039466\n",
      "train loss:2.2995370332534204\n",
      "train loss:2.300627238072848\n",
      "train loss:2.3099363140512357\n",
      "train loss:2.3196611914391165\n",
      "train loss:2.3057668759177754\n",
      "train loss:2.301982966595446\n",
      "train loss:2.306171873523973\n",
      "train loss:2.29017975915038\n",
      "train loss:2.3014617703550146\n",
      "train loss:2.3076268413049537\n",
      "train loss:2.303932666328915\n",
      "train loss:2.3083489855492054\n",
      "train loss:2.3109648565184617\n",
      "train loss:2.3101420340832144\n",
      "train loss:2.302063323276882\n",
      "train loss:2.3043942189067432\n",
      "train loss:2.306302199322512\n",
      "train loss:2.299768823429609\n",
      "train loss:2.306097581049016\n",
      "train loss:2.2979371443542975\n",
      "train loss:2.289830261206042\n",
      "train loss:2.306435619519819\n",
      "train loss:2.316545642777878\n",
      "train loss:2.3029755297140415\n",
      "train loss:2.31116882833299\n",
      "train loss:2.302136916197097\n",
      "train loss:2.2923280443416125\n",
      "train loss:2.302319339641155\n",
      "train loss:2.2986562294014656\n",
      "train loss:2.3002023332762764\n",
      "train loss:2.3010626946215673\n",
      "train loss:2.2918497051486755\n",
      "train loss:2.3137115622914246\n",
      "train loss:2.2975918795372166\n",
      "train loss:2.2970539671103927\n",
      "train loss:2.3060927867751015\n",
      "train loss:2.291864451202092\n",
      "train loss:2.3061374862835065\n",
      "train loss:2.286399235261168\n",
      "train loss:2.2895614072119583\n",
      "train loss:2.302188561566192\n",
      "train loss:2.295836198516982\n",
      "train loss:2.3045276569392223\n",
      "train loss:2.305258816905367\n",
      "train loss:2.3070662799523016\n",
      "train loss:2.2962654362959194\n",
      "train loss:2.299839763841431\n",
      "train loss:2.2971627803451846\n",
      "train loss:2.2985936993504206\n",
      "train loss:2.309028377453399\n",
      "train loss:2.3071637504786056\n",
      "train loss:2.302527859018692\n",
      "train loss:2.297614532347632\n",
      "train loss:2.292769637418267\n",
      "train loss:2.2992285909915893\n",
      "train loss:2.3012840052092036\n",
      "train loss:2.3031918616687594\n",
      "train loss:2.3052072521363183\n",
      "train loss:2.2899720864135835\n",
      "train loss:2.3088483702115967\n",
      "train loss:2.3035586935353995\n",
      "train loss:2.3017646784067347\n",
      "train loss:2.3141615102614943\n",
      "train loss:2.2999132311565327\n",
      "train loss:2.292842421040253\n",
      "train loss:2.313267238608638\n",
      "train loss:2.3068308152842754\n",
      "train loss:2.2958511324574844\n",
      "train loss:2.3068882919554627\n",
      "train loss:2.295089269338072\n",
      "train loss:2.300442162181871\n",
      "train loss:2.3013421884613603\n",
      "train loss:2.3011791659403986\n",
      "train loss:2.3040537312166505\n",
      "train loss:2.3086022660395575\n",
      "train loss:2.3036215846528503\n",
      "train loss:2.3012521936267065\n",
      "train loss:2.2971567667405903\n",
      "train loss:2.3098835790658248\n",
      "train loss:2.29368077320205\n",
      "train loss:2.2926147124832736\n",
      "train loss:2.2977545255338083\n",
      "train loss:2.298379540816176\n",
      "train loss:2.2958844294562195\n",
      "train loss:2.291304883062442\n",
      "train loss:2.3033048558736082\n",
      "train loss:2.3110007060489006\n",
      "train loss:2.307168821386881\n",
      "train loss:2.3062293019147617\n",
      "train loss:2.3076938228633814\n",
      "train loss:2.304993904139205\n",
      "train loss:2.297589925623698\n",
      "train loss:2.3058156905860745\n",
      "train loss:2.3049872815455434\n",
      "train loss:2.3071211208357525\n",
      "train loss:2.3053516290015965\n",
      "train loss:2.2882909116835375\n",
      "train loss:2.305377524717638\n",
      "train loss:2.2957359240984334\n",
      "train loss:2.3081223139556384\n",
      "train loss:2.3023631155907633\n",
      "train loss:2.302647035693432\n",
      "train loss:2.3046338994935707\n",
      "train loss:2.3141142845534906\n",
      "train loss:2.2961552125983102\n",
      "train loss:2.296860168040449\n",
      "train loss:2.3072313759821137\n",
      "train loss:2.3104766400983943\n",
      "train loss:2.2978080899091236\n",
      "train loss:2.306554393927117\n",
      "train loss:2.3077590168091913\n",
      "train loss:2.2918755664377883\n",
      "train loss:2.3040954047773874\n",
      "train loss:2.298267179807299\n",
      "train loss:2.294350580708225\n",
      "train loss:2.311567514515461\n",
      "train loss:2.310555987184993\n",
      "train loss:2.294841065494456\n",
      "train loss:2.3026677951923586\n",
      "train loss:2.2969225164599525\n",
      "train loss:2.3011850086213146\n",
      "train loss:2.3068102475308105\n",
      "train loss:2.2953394601964323\n",
      "train loss:2.304023585101765\n",
      "train loss:2.2910059675356407\n",
      "train loss:2.308008178464366\n",
      "train loss:2.2972134359497627\n",
      "train loss:2.3143997199818083\n",
      "train loss:2.2974394781618312\n",
      "train loss:2.2990727823707475\n",
      "train loss:2.2978829451456546\n",
      "train loss:2.2987019472123382\n",
      "train loss:2.298080231947848\n",
      "train loss:2.2942741117676335\n",
      "train loss:2.292852933081611\n",
      "train loss:2.3010301859325453\n",
      "train loss:2.3032039225706042\n",
      "train loss:2.3080359112295032\n",
      "train loss:2.299793828328509\n",
      "train loss:2.2999221018481615\n",
      "train loss:2.285795046283485\n",
      "train loss:2.3155315363759756\n",
      "train loss:2.3066860562334663\n",
      "train loss:2.2927487461411107\n",
      "train loss:2.2981450829029475\n",
      "train loss:2.295594603710404\n",
      "train loss:2.317052117141941\n",
      "train loss:2.2962011469066246\n",
      "train loss:2.2854373543750963\n",
      "train loss:2.2941396416545823\n",
      "train loss:2.2929134877222137\n",
      "train loss:2.299517464951396\n",
      "train loss:2.3007632750971947\n",
      "train loss:2.2990256841372942\n",
      "train loss:2.2914774782062133\n",
      "train loss:2.2989578754966837\n",
      "train loss:2.297353984191677\n",
      "train loss:2.3223717600469493\n",
      "train loss:2.302436637730973\n",
      "train loss:2.299948482695507\n",
      "train loss:2.2968112463881796\n",
      "train loss:2.2982560736466016\n",
      "train loss:2.3084398004631845\n",
      "train loss:2.308176839939068\n",
      "train loss:2.2877315196390815\n",
      "train loss:2.2972692894906235\n",
      "train loss:2.320476826436691\n",
      "train loss:2.307435271104367\n",
      "train loss:2.31224012277265\n",
      "train loss:2.309976386704827\n",
      "train loss:2.3117305651427373\n",
      "train loss:2.2918421718074296\n",
      "train loss:2.299985338008139\n",
      "train loss:2.3070471832729136\n",
      "train loss:2.303881948507252\n",
      "train loss:2.2928395203888012\n",
      "train loss:2.2975813566912757\n",
      "train loss:2.299452698624643\n",
      "train loss:2.3061862511105193\n",
      "train loss:2.298780915175109\n",
      "train loss:2.2923351186804473\n",
      "train loss:2.283366544118766\n",
      "train loss:2.298510140742005\n",
      "train loss:2.3023689150993465\n",
      "train loss:2.300262641399546\n",
      "train loss:2.301776601799144\n",
      "train loss:2.304862535054061\n",
      "train loss:2.3054231466309076\n",
      "train loss:2.2941587299196406\n",
      "train loss:2.3041507738891784\n",
      "train loss:2.306989302729226\n",
      "train loss:2.3103026167358487\n",
      "train loss:2.3139940873549\n",
      "train loss:2.299293160568704\n",
      "train loss:2.2979951462959765\n",
      "train loss:2.2981777912571655\n",
      "train loss:2.2904201344830293\n",
      "train loss:2.2999297357977686\n",
      "train loss:2.307975101201748\n",
      "train loss:2.290786700927552\n",
      "train loss:2.2910945134759757\n",
      "train loss:2.300746046950211\n",
      "train loss:2.2979946985173614\n",
      "train loss:2.303548935578094\n",
      "train loss:2.3061955501548574\n",
      "train loss:2.294875234798318\n",
      "train loss:2.303774764605473\n",
      "train loss:2.303216735142703\n",
      "train loss:2.2957503452527193\n",
      "train loss:2.2876379748906275\n",
      "train loss:2.304814871913285\n",
      "train loss:2.3081752513257507\n",
      "train loss:2.305611909677489\n",
      "train loss:2.293581208826324\n",
      "train loss:2.303971588746339\n",
      "train loss:2.3033748331174166\n",
      "train loss:2.2910676286106417\n",
      "train loss:2.304791169129099\n",
      "train loss:2.3068099797930994\n",
      "train loss:2.308501264361294\n",
      "train loss:2.299763000322068\n",
      "train loss:2.2859371759984293\n",
      "train loss:2.2976750809570645\n",
      "train loss:2.3078282887926838\n",
      "train loss:2.302523235588474\n",
      "train loss:2.3075035091503673\n",
      "train loss:2.299183619061111\n",
      "train loss:2.305635513086245\n",
      "train loss:2.289327182356149\n",
      "train loss:2.310549093451983\n",
      "train loss:2.293293027796179\n",
      "train loss:2.2986761595831977\n",
      "train loss:2.3056853693269925\n",
      "train loss:2.30801568651357\n",
      "train loss:2.2996791376978973\n",
      "train loss:2.3038710967395613\n",
      "train loss:2.2930288405156136\n",
      "train loss:2.301479531804602\n",
      "train loss:2.293156793028672\n",
      "train loss:2.300990747620093\n",
      "train loss:2.304226356339504\n",
      "train loss:2.3017467808011296\n",
      "train loss:2.2962429193959863\n",
      "train loss:2.297296337698109\n",
      "train loss:2.3015658211963204\n",
      "train loss:2.2996450950935485\n",
      "train loss:2.3062207553994467\n",
      "train loss:2.2993589858726162\n",
      "train loss:2.3153234139733483\n",
      "train loss:2.2964660907327117\n",
      "train loss:2.2959605315842757\n",
      "train loss:2.3147975702572268\n",
      "train loss:2.3062013596950477\n",
      "train loss:2.2978187996436805\n",
      "train loss:2.293754290395583\n",
      "train loss:2.2942117153076125\n",
      "train loss:2.2961561503662935\n",
      "train loss:2.307078728588406\n",
      "train loss:2.2977330908486637\n",
      "train loss:2.290250005081151\n",
      "train loss:2.3041118953206996\n",
      "train loss:2.2992867327791777\n",
      "train loss:2.3042973708408505\n",
      "train loss:2.311398821369934\n",
      "train loss:2.30239290461083\n",
      "train loss:2.3072273082780144\n",
      "train loss:2.283943403704834\n",
      "train loss:2.3031840738715994\n",
      "train loss:2.2964720760171513\n",
      "train loss:2.300845446906295\n",
      "train loss:2.2952668696091547\n",
      "train loss:2.3042348083435678\n",
      "train loss:2.299485307569272\n",
      "train loss:2.301969392037921\n",
      "train loss:2.296530950665767\n",
      "train loss:2.294724065737759\n",
      "train loss:2.2962465146314477\n",
      "train loss:2.297629041445377\n",
      "train loss:2.3051331559236337\n",
      "train loss:2.3022152524307327\n",
      "train loss:2.3064123880201346\n",
      "train loss:2.313633870489479\n",
      "train loss:2.291667101586424\n",
      "train loss:2.2960935228504766\n",
      "train loss:2.2988514259417974\n",
      "train loss:2.3158565157418702\n",
      "train loss:2.295055231138711\n",
      "train loss:2.30139286983204\n",
      "train loss:2.2947475869150527\n",
      "train loss:2.292609115985636\n",
      "train loss:2.3034399656085194\n",
      "train loss:2.2925455585820327\n",
      "train loss:2.3033231814593873\n",
      "train loss:2.2918923939120774\n",
      "train loss:2.3001245302577216\n",
      "train loss:2.2913331600091222\n",
      "train loss:2.3033270671597914\n",
      "train loss:2.3002820730058886\n",
      "train loss:2.3013265098763713\n",
      "train loss:2.2983115030122576\n",
      "train loss:2.306834579318921\n",
      "train loss:2.2920970163947563\n",
      "train loss:2.2904604860637034\n",
      "train loss:2.309339402103525\n",
      "train loss:2.300938771887669\n",
      "train loss:2.3036051105189665\n",
      "train loss:2.308667389823508\n",
      "train loss:2.2934033499147857\n",
      "train loss:2.3061429637175017\n",
      "train loss:2.29648921104383\n",
      "train loss:2.315493796603829\n",
      "train loss:2.305389475353537\n",
      "train loss:2.317299637587927\n",
      "train loss:2.301530327171724\n",
      "train loss:2.2946765942348435\n",
      "train loss:2.3109872151121067\n",
      "train loss:2.2920567661177724\n",
      "train loss:2.3004343751467307\n",
      "train loss:2.3171240879323376\n",
      "train loss:2.3033361130884895\n",
      "train loss:2.3024638126363857\n",
      "train loss:2.30096083966009\n",
      "train loss:2.2943230588473007\n",
      "train loss:2.306125840364065\n",
      "train loss:2.3044372973236467\n",
      "train loss:2.298720092055556\n",
      "train loss:2.305066694576636\n",
      "train loss:2.3038190851820786\n",
      "train loss:2.2936234374304414\n",
      "train loss:2.3045000304106784\n",
      "train loss:2.2951202096993866\n",
      "train loss:2.3064331241883615\n",
      "train loss:2.2938630766660086\n",
      "train loss:2.299844088699718\n",
      "train loss:2.2836971382672533\n",
      "train loss:2.294548244083424\n",
      "train loss:2.3055164903215077\n",
      "train loss:2.3056472241827435\n",
      "train loss:2.3013857687755293\n",
      "train loss:2.308692317959072\n",
      "train loss:2.3002615512984894\n",
      "train loss:2.3098003987800375\n",
      "train loss:2.302903289185069\n",
      "train loss:2.2928392623495135\n",
      "train loss:2.2837559038538697\n",
      "train loss:2.2894431667039217\n",
      "train loss:2.2955975866609815\n",
      "train loss:2.2972440887114782\n",
      "train loss:2.292470218914852\n",
      "train loss:2.2954339328064135\n",
      "train loss:2.2961761587714236\n",
      "train loss:2.29191263497112\n",
      "train loss:2.3171133261636765\n",
      "train loss:2.309348111591442\n",
      "train loss:2.3112027057988946\n",
      "train loss:2.309684056053789\n",
      "train loss:2.297386414896997\n",
      "train loss:2.301706502918351\n",
      "train loss:2.2867289136846485\n",
      "train loss:2.3095459205581\n",
      "train loss:2.2954985235868737\n",
      "train loss:2.3051899607093276\n",
      "train loss:2.3078263098160496\n",
      "train loss:2.3089865292512077\n",
      "train loss:2.2881337350245357\n",
      "train loss:2.2977444107097873\n",
      "train loss:2.3086540843887025\n",
      "train loss:2.3054482126113887\n",
      "train loss:2.3032767654378103\n",
      "train loss:2.2902943596331484\n",
      "train loss:2.2980265655278913\n",
      "train loss:2.30315587691792\n",
      "train loss:2.3001955504013973\n",
      "train loss:2.299713117471218\n",
      "train loss:2.290115613828771\n",
      "train loss:2.306746892369728\n",
      "train loss:2.3034334872309126\n",
      "train loss:2.298092101366202\n",
      "train loss:2.299164235773423\n",
      "train loss:2.295389100169013\n",
      "train loss:2.303031052184018\n",
      "train loss:2.303161256623244\n",
      "train loss:2.3083723821545665\n",
      "train loss:2.285379218123239\n",
      "train loss:2.2926522982388393\n",
      "train loss:2.2899046023676375\n",
      "train loss:2.28579763833514\n",
      "train loss:2.312491083890167\n",
      "train loss:2.29652213544151\n",
      "train loss:2.298320794867912\n",
      "train loss:2.2975854828082856\n",
      "train loss:2.3188207977233954\n",
      "train loss:2.3092236817199456\n",
      "train loss:2.2939686119706177\n",
      "train loss:2.308125612860803\n",
      "train loss:2.3034112210172406\n",
      "train loss:2.2963477814237523\n",
      "train loss:2.3039420030469375\n",
      "train loss:2.3151526070215263\n",
      "train loss:2.2838164812028006\n",
      "train loss:2.294332617711339\n",
      "train loss:2.3077406429806215\n",
      "train loss:2.3060285359111585\n",
      "train loss:2.2868717089685555\n",
      "train loss:2.290041255394515\n",
      "train loss:2.294709192083642\n",
      "train loss:2.293014990169059\n",
      "train loss:2.3122776878769074\n",
      "train loss:2.2999571168965707\n",
      "train loss:2.3128388559169393\n",
      "train loss:2.311609252283116\n",
      "train loss:2.301162433513106\n",
      "train loss:2.3115389288285355\n",
      "train loss:2.2923128753716107\n",
      "train loss:2.2995229565079685\n",
      "train loss:2.2924080264457336\n",
      "train loss:2.3157501751575946\n",
      "train loss:2.2992659267441127\n",
      "train loss:2.2952593258672906\n",
      "train loss:2.288065977696534\n",
      "train loss:2.299185280269472\n",
      "train loss:2.2886741053316597\n",
      "train loss:2.314984269806043\n",
      "train loss:2.3012303830797727\n",
      "train loss:2.2886918889322576\n",
      "train loss:2.3048983849362497\n",
      "train loss:2.3049683254424096\n",
      "train loss:2.310487251858099\n",
      "train loss:2.302255148821919\n",
      "train loss:2.30512030447313\n",
      "train loss:2.282832949244963\n",
      "train loss:2.299984639841819\n",
      "train loss:2.310108119159387\n",
      "train loss:2.300565266483396\n",
      "train loss:2.3025350022619504\n",
      "train loss:2.3119956889793656\n",
      "train loss:2.304055115018035\n",
      "train loss:2.3001231470306815\n",
      "train loss:2.2967862989862993\n",
      "train loss:2.298444175545437\n",
      "train loss:2.2951516625389523\n",
      "train loss:2.2924203365223597\n",
      "train loss:2.3150415068624746\n",
      "train loss:2.296497488592878\n",
      "train loss:2.3020297480441014\n",
      "train loss:2.298678863668518\n",
      "train loss:2.3053083573352673\n",
      "train loss:2.287032317095266\n",
      "train loss:2.309957689930342\n",
      "train loss:2.296067463663424\n",
      "train loss:2.2901945652181226\n",
      "train loss:2.3162906009966777\n",
      "train loss:2.3156575092191796\n",
      "train loss:2.306404194994079\n",
      "train loss:2.27882310933572\n",
      "train loss:2.3016498879384044\n",
      "train loss:2.294543729824266\n",
      "train loss:2.291580460503008\n",
      "train loss:2.3005829973419747\n",
      "train loss:2.304646645266983\n",
      "train loss:2.301137591342691\n",
      "train loss:2.298187384202044\n",
      "train loss:2.30898336579843\n",
      "train loss:2.2880345482282376\n",
      "train loss:2.3011166084575425\n",
      "train loss:2.310462544837411\n",
      "train loss:2.298620031398211\n",
      "train loss:2.307659839274997\n",
      "train loss:2.296843475267656\n",
      "train loss:2.302025228197106\n",
      "train loss:2.3069083362651908\n",
      "train loss:2.3061971359364764\n",
      "train loss:2.2945710672986905\n",
      "train loss:2.2988254634486234\n",
      "train loss:2.3134787226606894\n",
      "train loss:2.296158987756338\n",
      "train loss:2.282751104410023\n",
      "train loss:2.2963316347665637\n",
      "train loss:2.305454079860195\n",
      "train loss:2.308040518770225\n",
      "train loss:2.312124888279836\n",
      "train loss:2.304788501930595\n",
      "train loss:2.2972589860822366\n",
      "train loss:2.302043498349026\n",
      "train loss:2.2903807233845277\n",
      "train loss:2.3040705243973267\n",
      "train loss:2.3091095497198095\n",
      "train loss:2.296114578213124\n",
      "train loss:2.307132313374421\n",
      "train loss:2.3017292888781737\n",
      "train loss:2.3022956405380417\n",
      "train loss:2.2971223432288275\n",
      "train loss:2.3158676524661477\n",
      "train loss:2.2830008666389796\n",
      "train loss:2.301764620287392\n",
      "train loss:2.3204533637287916\n",
      "train loss:2.2872951064457956\n",
      "train loss:2.308139654007462\n",
      "train loss:2.295639513184644\n",
      "train loss:2.2999711950722403\n",
      "train loss:2.3080439931758465\n",
      "train loss:2.311198300628177\n",
      "train loss:2.309471285799226\n",
      "train loss:2.2942387108473943\n",
      "train loss:2.2950772250928413\n",
      "train loss:2.3070407451461423\n",
      "train loss:2.3041758313445038\n",
      "train loss:2.297617184050772\n",
      "train loss:2.302403327666024\n",
      "train loss:2.310967354747638\n",
      "train loss:2.3125719659895037\n",
      "train loss:2.307892188120538\n",
      "train loss:2.2958245852582597\n",
      "train loss:2.289192405495455\n",
      "train loss:2.305994109065651\n",
      "train loss:2.308835148215975\n",
      "train loss:2.311103387647261\n",
      "train loss:2.303641130730361\n",
      "train loss:2.296668591360524\n",
      "train loss:2.2967347751679554\n",
      "train loss:2.3117201938871004\n",
      "train loss:2.299752017729035\n",
      "train loss:2.3063205292461264\n",
      "train loss:2.299081957009888\n",
      "train loss:2.2912303302043115\n",
      "train loss:2.2978941191502296\n",
      "train loss:2.298682766727139\n",
      "train loss:2.291726719415809\n",
      "train loss:2.303010953469721\n",
      "train loss:2.3005035910358878\n",
      "train loss:2.2836746636321488\n",
      "train loss:2.2899990715798895\n",
      "train loss:2.2957498374709724\n",
      "train loss:2.3010585592587045\n",
      "train loss:2.3008175582938413\n",
      "train loss:2.3015044399344795\n",
      "train loss:2.3099658232809746\n",
      "train loss:2.3052962946320967\n",
      "train loss:2.3067862852179633\n",
      "train loss:2.304795207430971\n",
      "train loss:2.2992108652078014\n",
      "train loss:2.303749472767596\n",
      "train loss:2.296398647512679\n",
      "train loss:2.2852940588803574\n",
      "train loss:2.2953546708929515\n",
      "train loss:2.3013079075919793\n",
      "train loss:2.304194949351915\n",
      "train loss:2.30213615369728\n",
      "train loss:2.311548615474002\n",
      "train loss:2.311221249850626\n",
      "train loss:2.2819452211352167\n",
      "train loss:2.2945110032050824\n",
      "train loss:2.299431657841726\n",
      "train loss:2.3186772526630652\n",
      "train loss:2.3022860662633677\n",
      "train loss:2.3044660347474224\n",
      "train loss:2.2980192750174613\n",
      "train loss:2.279960044362623\n",
      "train loss:2.291799238519367\n",
      "train loss:2.295739532312712\n",
      "train loss:2.2968493178903207\n",
      "train loss:2.3048358272122362\n",
      "train loss:2.312172676659907\n",
      "train loss:2.307680969154136\n",
      "train loss:2.3032129391178744\n",
      "train loss:2.2963850129061227\n",
      "train loss:2.2919248108755177\n",
      "train loss:2.2959755197119676\n",
      "train loss:2.290537086325261\n",
      "train loss:2.3125437696144084\n",
      "train loss:2.3064827323162973\n",
      "train loss:2.3003815560809637\n",
      "train loss:2.29176040322781\n",
      "train loss:2.2959257088652234\n",
      "train loss:2.305027968719095\n",
      "train loss:2.300452142170145\n",
      "train loss:2.3052273246597546\n",
      "train loss:2.288029689124352\n",
      "train loss:2.3042725768165893\n",
      "train loss:2.2810210911900364\n",
      "train loss:2.308471366722001\n",
      "train loss:2.286633030540441\n",
      "train loss:2.308211487845577\n",
      "train loss:2.3040655417613483\n",
      "train loss:2.305766788800205\n",
      "train loss:2.3028649748851846\n",
      "train loss:2.301672582518411\n",
      "train loss:2.311481483689371\n",
      "train loss:2.2865834389901085\n",
      "train loss:2.3025932848001225\n",
      "train loss:2.30163203485958\n",
      "train loss:2.291397358065645\n",
      "train loss:2.300415711095195\n",
      "train loss:2.304341275410233\n",
      "train loss:2.289933571117264\n",
      "train loss:2.3149703979208724\n",
      "train loss:2.29384211497907\n",
      "train loss:2.2998883709137057\n",
      "train loss:2.291151535938789\n",
      "train loss:2.2894259932224355\n",
      "train loss:2.302609846585041\n",
      "train loss:2.303761031544649\n",
      "train loss:2.289915526942936\n",
      "train loss:2.289021827906731\n",
      "train loss:2.292060467185058\n",
      "train loss:2.299640537484852\n",
      "train loss:2.282293186032642\n",
      "train loss:2.300520228767253\n",
      "train loss:2.3079204069902555\n",
      "train loss:2.297365207308797\n",
      "train loss:2.293728879816995\n",
      "train loss:2.3167631863912166\n",
      "train loss:2.2854629687049144\n",
      "train loss:2.3053607582891913\n",
      "train loss:2.288509199049139\n",
      "train loss:2.297980488656444\n",
      "train loss:2.3198249491904805\n",
      "train loss:2.3046689285307487\n",
      "train loss:2.301697737845154\n",
      "train loss:2.3016590033371065\n",
      "train loss:2.28662447467552\n",
      "train loss:2.307571687688793\n",
      "train loss:2.2946516755793924\n",
      "train loss:2.2922165892821695\n",
      "train loss:2.2981770730366478\n",
      "train loss:2.3000317302207054\n",
      "train loss:2.298065665127621\n",
      "train loss:2.289129611121051\n",
      "train loss:2.2983984674037097\n",
      "train loss:2.2881778486552222\n",
      "train loss:2.2771021613001707\n",
      "train loss:2.2973327725508716\n",
      "train loss:2.3089469934506255\n",
      "train loss:2.3069433098238505\n",
      "train loss:2.306445050565309\n",
      "train loss:2.2782353884543456\n",
      "train loss:2.2926840513140583\n",
      "train loss:2.2828240125768944\n",
      "train loss:2.306194190291909\n",
      "train loss:2.3003377556541458\n",
      "train loss:2.303077384620262\n",
      "train loss:2.300527006273715\n",
      "train loss:2.3025532559199986\n",
      "train loss:2.3012857058135845\n",
      "train loss:2.2899253338264933\n",
      "train loss:2.2994013404967055\n",
      "train loss:2.3109154966786205\n",
      "train loss:2.3099101605088874\n",
      "train loss:2.3117461496573997\n",
      "train loss:2.322045686594336\n",
      "train loss:2.3009941365034536\n",
      "train loss:2.2797472660580818\n",
      "train loss:2.291414546853699\n",
      "train loss:2.30376112787786\n",
      "train loss:2.291915932065758\n",
      "train loss:2.3041486996781875\n",
      "train loss:2.287783951567068\n",
      "train loss:2.309946863376795\n",
      "train loss:2.2928835005916293\n",
      "train loss:2.2968958488799935\n",
      "train loss:2.3031349922074202\n",
      "train loss:2.2986357402080086\n",
      "train loss:2.3039373837047767\n",
      "train loss:2.297840491794484\n",
      "train loss:2.295632105018825\n",
      "train loss:2.300828151918388\n",
      "train loss:2.2956897441332718\n",
      "train loss:2.2783032866997486\n",
      "train loss:2.3012235282660045\n",
      "train loss:2.2719102086157776\n",
      "train loss:2.308915918389639\n",
      "train loss:2.298643579044449\n",
      "train loss:2.303736843514758\n",
      "train loss:2.3017214183008483\n",
      "train loss:2.3153400321184368\n",
      "train loss:2.296857482819049\n",
      "train loss:2.3028894877221915\n",
      "train loss:2.291625687512152\n",
      "train loss:2.280891207910325\n",
      "train loss:2.302707508148606\n",
      "train loss:2.2994823585434814\n",
      "train loss:2.28385656616034\n",
      "train loss:2.3055846805560085\n",
      "train loss:2.298287023590312\n",
      "train loss:2.308352569249344\n",
      "train loss:2.2910167632866063\n",
      "train loss:2.3148346473612467\n",
      "train loss:2.302533870890322\n",
      "train loss:2.30731610303325\n",
      "train loss:2.287634026599261\n",
      "train loss:2.3044029891157263\n",
      "train loss:2.2920376751188547\n",
      "train loss:2.295325134296165\n",
      "train loss:2.3056018497618176\n",
      "train loss:2.287201764056383\n",
      "train loss:2.2990107476966353\n",
      "train loss:2.3024071870625527\n",
      "train loss:2.2933040637864557\n",
      "train loss:2.3044316644662297\n",
      "train loss:2.2908839097582527\n",
      "train loss:2.313584968661047\n",
      "train loss:2.3195135782377\n",
      "train loss:2.293521303658304\n",
      "train loss:2.2891504061728907\n",
      "train loss:2.2977334959010047\n",
      "train loss:2.299478900880705\n",
      "train loss:2.298599669305953\n",
      "train loss:2.2896379713316284\n",
      "train loss:2.3020628117522217\n",
      "train loss:2.2851846549874706\n",
      "train loss:2.3027808030459154\n",
      "train loss:2.299078090793591\n",
      "train loss:2.29686480039326\n",
      "train loss:2.285271768495245\n",
      "train loss:2.286366389755087\n",
      "train loss:2.313289362505442\n",
      "train loss:2.3043085875845293\n",
      "train loss:2.302943000762555\n",
      "train loss:2.312359258405682\n",
      "train loss:2.2832351720513024\n",
      "train loss:2.294648975881579\n",
      "train loss:2.282040262418338\n",
      "train loss:2.291201179891414\n",
      "train loss:2.2974073937857273\n",
      "train loss:2.303043633411233\n",
      "train loss:2.3035981059897015\n",
      "train loss:2.2982792649106596\n",
      "train loss:2.3137281110053705\n",
      "train loss:2.2630703507128844\n",
      "train loss:2.2972993138846394\n",
      "train loss:2.302579173733346\n",
      "train loss:2.317212793989188\n",
      "train loss:2.299172265667087\n",
      "train loss:2.3062843498247645\n",
      "train loss:2.306073800041734\n",
      "train loss:2.301256288963272\n",
      "train loss:2.2975690752864577\n",
      "train loss:2.3065442232285256\n",
      "train loss:2.2938504060985947\n",
      "train loss:2.298573958576229\n",
      "train loss:2.2954245477935915\n",
      "train loss:2.2863493503337287\n",
      "train loss:2.2772236305147935\n",
      "train loss:2.3001644015013114\n",
      "train loss:2.3001562162096914\n",
      "train loss:2.2882755929613525\n",
      "train loss:2.2801661631877588\n",
      "train loss:2.289710279047041\n",
      "train loss:2.293824106871667\n",
      "train loss:2.30037613362403\n",
      "train loss:2.318556416919441\n",
      "train loss:2.306790815507541\n",
      "train loss:2.2827050247443963\n",
      "train loss:2.3029292329556625\n",
      "train loss:2.290164647539493\n",
      "train loss:2.3067298214276333\n",
      "train loss:2.299610734306962\n",
      "train loss:2.320011273589846\n",
      "train loss:2.3065444793231187\n",
      "train loss:2.3141252663730176\n",
      "train loss:2.302735423843873\n",
      "train loss:2.2923594946667247\n",
      "train loss:2.300226031103382\n",
      "train loss:2.28034749660684\n",
      "train loss:2.2906831473809097\n",
      "train loss:2.307919858539317\n",
      "train loss:2.30079960707236\n",
      "train loss:2.2944025754903548\n",
      "train loss:2.2835489061641123\n",
      "train loss:2.2919696977189585\n",
      "train loss:2.295747598934594\n",
      "train loss:2.2929241022277522\n",
      "train loss:2.2970526004895886\n",
      "train loss:2.293860583044165\n",
      "train loss:2.297292091466204\n",
      "train loss:2.308705824566588\n",
      "train loss:2.3243032462906883\n",
      "train loss:2.292603585801203\n",
      "train loss:2.2838795486662296\n",
      "train loss:2.2933918250848944\n",
      "train loss:2.2851920728829564\n",
      "train loss:2.290029673661545\n",
      "train loss:2.310379357821858\n",
      "train loss:2.3027406220361493\n",
      "train loss:2.3024344112199895\n",
      "train loss:2.3211684016355276\n",
      "train loss:2.282749779888449\n",
      "train loss:2.3025230415953084\n",
      "train loss:2.3114770058213754\n",
      "train loss:2.295711840990257\n",
      "train loss:2.2973521750967842\n",
      "train loss:2.28901975442685\n",
      "train loss:2.2833793322474056\n",
      "train loss:2.299830445477734\n",
      "train loss:2.3069738988186432\n",
      "train loss:2.2979825702553107\n",
      "train loss:2.3047258269609276\n",
      "train loss:2.292932036916162\n",
      "train loss:2.29460768717243\n",
      "train loss:2.309625015997103\n",
      "train loss:2.3074591049300928\n",
      "train loss:2.2880929527135034\n",
      "train loss:2.2884284982853895\n",
      "train loss:2.283519680893205\n",
      "train loss:2.3126825574153855\n",
      "train loss:2.3068182616475132\n",
      "train loss:2.296830473941128\n",
      "train loss:2.3161988287882\n",
      "train loss:2.2943490458923708\n",
      "train loss:2.292714264861574\n",
      "train loss:2.2918135410904354\n",
      "train loss:2.300255862928846\n",
      "train loss:2.3128855030256\n",
      "train loss:2.2937616778167746\n",
      "train loss:2.322844365585121\n",
      "train loss:2.308138173141888\n",
      "train loss:2.3102548728453414\n",
      "train loss:2.29732748433346\n",
      "train loss:2.281277297357966\n",
      "train loss:2.3114619539297547\n",
      "train loss:2.2958082041463097\n",
      "train loss:2.2757996865452186\n",
      "train loss:2.3121629841922644\n",
      "train loss:2.2826117528391636\n",
      "train loss:2.267626952933333\n",
      "train loss:2.291747530765325\n",
      "train loss:2.29267578290012\n",
      "train loss:2.2882952461340333\n",
      "train loss:2.2985254719110104\n",
      "train loss:2.2846031761209096\n",
      "train loss:2.2809471612388905\n",
      "train loss:2.2730931107256023\n",
      "train loss:2.2954699713480826\n",
      "train loss:2.2860217364462594\n",
      "train loss:2.2960835549862835\n",
      "train loss:2.3070660566642966\n",
      "train loss:2.3027648396220037\n",
      "train loss:2.3021204243390674\n",
      "train loss:2.2841756778211106\n",
      "train loss:2.305060973398332\n",
      "train loss:2.2985328193698145\n",
      "train loss:2.2777793330069156\n",
      "train loss:2.2808305384152163\n",
      "train loss:2.2921825988380653\n",
      "train loss:2.2935978553465324\n",
      "train loss:2.2854203551665693\n",
      "train loss:2.285268212634586\n",
      "train loss:2.3181735883377685\n",
      "train loss:2.3065849656821076\n",
      "train loss:2.292978906686528\n",
      "train loss:2.3042645081547497\n",
      "train loss:2.2805250768824803\n",
      "train loss:2.304696848761338\n",
      "train loss:2.2932321104850026\n",
      "train loss:2.286333908915159\n",
      "train loss:2.3013594316655803\n",
      "train loss:2.2978905253991693\n",
      "train loss:2.2913128446985906\n",
      "train loss:2.2840854973275495\n",
      "train loss:2.2839626132398436\n",
      "train loss:2.30244658803134\n",
      "train loss:2.2940750400333663\n",
      "train loss:2.2943855627347753\n",
      "train loss:2.294810205990091\n",
      "train loss:2.293362802310525\n",
      "train loss:2.2925212262162575\n",
      "train loss:2.3105844020147224\n",
      "train loss:2.3042397893348383\n",
      "train loss:2.29512372262566\n",
      "train loss:2.2728559009537044\n",
      "train loss:2.279729915933049\n",
      "train loss:2.270512257332604\n",
      "train loss:2.2995130043227516\n",
      "train loss:2.278230800747825\n",
      "train loss:2.3018761381761927\n",
      "train loss:2.273284174573744\n",
      "train loss:2.313792340145426\n",
      "train loss:2.303936075885054\n",
      "train loss:2.30036189286988\n",
      "train loss:2.294164776472451\n",
      "train loss:2.2918793709606264\n",
      "train loss:2.301383698188224\n",
      "train loss:2.305541931326142\n",
      "train loss:2.3117525161885153\n",
      "train loss:2.311679845044681\n",
      "train loss:2.28249651865109\n",
      "train loss:2.277684343577592\n",
      "train loss:2.28579038296737\n",
      "train loss:2.304988294626729\n",
      "train loss:2.2721674905459515\n",
      "train loss:2.3067374323026377\n",
      "train loss:2.30671457661008\n",
      "train loss:2.285508537905719\n",
      "train loss:2.30291935621473\n",
      "train loss:2.298571970663555\n",
      "train loss:2.302840121642622\n",
      "train loss:2.2990336430791203\n",
      "train loss:2.2765799767323402\n",
      "train loss:2.3029630580174274\n",
      "train loss:2.311097323722022\n",
      "train loss:2.305826386642602\n",
      "train loss:2.3049783210414505\n",
      "train loss:2.2702201080950766\n",
      "train loss:2.297427102169951\n",
      "train loss:2.295241343121565\n",
      "train loss:2.2772044434522316\n",
      "train loss:2.287334116455822\n",
      "train loss:2.3059487748344827\n",
      "train loss:2.2887123135525824\n",
      "train loss:2.2876404988584724\n",
      "train loss:2.295467887685957\n",
      "train loss:2.303029860640634\n",
      "train loss:2.2889101710206\n",
      "train loss:2.287594112555774\n",
      "train loss:2.284534851220424\n",
      "train loss:2.3028946439654456\n",
      "train loss:2.285103008034222\n",
      "train loss:2.280970676818167\n",
      "train loss:2.298129806119285\n",
      "train loss:2.2893096802361947\n",
      "train loss:2.3132936012682723\n",
      "train loss:2.3021789268856327\n",
      "train loss:2.306598469213087\n",
      "train loss:2.299104888193283\n",
      "train loss:2.2897366216081476\n",
      "train loss:2.288140266425051\n",
      "train loss:2.2988590701706784\n",
      "train loss:2.309821529530262\n",
      "train loss:2.276131707426644\n",
      "train loss:2.290668948948829\n",
      "train loss:2.2977715308258975\n",
      "train loss:2.30960407061801\n",
      "train loss:2.302601450112591\n",
      "train loss:2.2810406436068167\n",
      "train loss:2.281908924741631\n",
      "train loss:2.287016877400873\n",
      "train loss:2.3090464526587215\n",
      "train loss:2.3011730053358432\n",
      "train loss:2.310920309878415\n",
      "train loss:2.296374602331863\n",
      "train loss:2.296524456494753\n",
      "train loss:2.294976137276479\n",
      "train loss:2.283710188409962\n",
      "train loss:2.282160091411986\n",
      "train loss:2.298974971194114\n",
      "train loss:2.264474849116731\n",
      "train loss:2.304187469926692\n",
      "train loss:2.2926139861352186\n",
      "train loss:2.3007142182667004\n",
      "train loss:2.3148723286177413\n",
      "train loss:2.2867463042383624\n",
      "train loss:2.286504365434021\n",
      "train loss:2.2868392617414113\n",
      "train loss:2.311940441630487\n",
      "train loss:2.2969365720440673\n",
      "train loss:2.262719276178506\n",
      "train loss:2.3063002333799365\n",
      "train loss:2.294187604380234\n",
      "train loss:2.2975929036282556\n",
      "train loss:2.281410622531807\n",
      "train loss:2.304123255122968\n",
      "train loss:2.296688374299747\n",
      "train loss:2.293361598912658\n",
      "train loss:2.2951213442029585\n",
      "train loss:2.3146073339360247\n",
      "train loss:2.298320163275619\n",
      "train loss:2.2931011393358416\n",
      "train loss:2.29295433271815\n",
      "train loss:2.2871571925131517\n",
      "train loss:2.294691151061044\n",
      "train loss:2.251776292489812\n",
      "train loss:2.2787124615645404\n",
      "train loss:2.2516724257647924\n",
      "train loss:2.2959310780665065\n",
      "train loss:2.3089714436522026\n",
      "train loss:2.3060286722284973\n",
      "train loss:2.2828960617901712\n",
      "train loss:2.302245683551453\n",
      "train loss:2.2800182891652874\n",
      "train loss:2.3001339556248706\n",
      "train loss:2.2885634413119083\n",
      "train loss:2.309719783349814\n",
      "train loss:2.2881750479552405\n",
      "train loss:2.28826179342862\n",
      "train loss:2.2962937205663385\n",
      "train loss:2.314353449030029\n",
      "train loss:2.3020563147470487\n",
      "train loss:2.2929722204091934\n",
      "train loss:2.304734877304279\n",
      "train loss:2.308982972285785\n",
      "train loss:2.3146188921233977\n",
      "train loss:2.320625143225707\n",
      "train loss:2.29906289538828\n",
      "train loss:2.2925055774323644\n",
      "train loss:2.300895229835575\n",
      "train loss:2.299006529398771\n",
      "train loss:2.26723021886416\n",
      "train loss:2.29889811144018\n",
      "train loss:2.313705960685866\n",
      "train loss:2.287349736730186\n",
      "train loss:2.299997706747024\n",
      "train loss:2.3065374563147523\n",
      "train loss:2.293093290813626\n",
      "train loss:2.2948799820694843\n",
      "train loss:2.308498184214227\n",
      "train loss:2.2936232804257095\n",
      "train loss:2.29982177981278\n",
      "train loss:2.292941543399644\n",
      "train loss:2.306305562057278\n",
      "train loss:2.302103610547469\n",
      "train loss:2.2767181002616095\n",
      "train loss:2.2914817410356996\n",
      "train loss:2.2982496151472436\n",
      "train loss:2.2830719261766896\n",
      "train loss:2.296506649219686\n",
      "train loss:2.293750444719959\n",
      "train loss:2.2937879877759255\n",
      "train loss:2.2755608164514864\n",
      "train loss:2.3034558812762844\n",
      "train loss:2.289114580301253\n",
      "train loss:2.293984002583101\n",
      "train loss:2.29082471994018\n",
      "train loss:2.299688077968119\n",
      "train loss:2.293552707520291\n",
      "train loss:2.266336048729845\n",
      "train loss:2.280573449845285\n",
      "train loss:2.28527033833092\n",
      "train loss:2.276898514345389\n",
      "train loss:2.2987624327339855\n",
      "train loss:2.2932670210472885\n",
      "train loss:2.296903280362721\n",
      "train loss:2.3217693411277684\n",
      "train loss:2.281713704152184\n",
      "train loss:2.3125457184753992\n",
      "train loss:2.2826111447007023\n",
      "train loss:2.282998710164198\n",
      "train loss:2.2850559605307703\n",
      "train loss:2.3022997020666027\n",
      "train loss:2.2970969988311847\n",
      "train loss:2.3032687013458077\n",
      "train loss:2.299998620421636\n",
      "train loss:2.2908764865011624\n",
      "train loss:2.320449869556798\n",
      "train loss:2.2896904499178277\n",
      "train loss:2.2929140114742994\n",
      "train loss:2.28820490591727\n",
      "train loss:2.307942101195048\n",
      "train loss:2.305518065942505\n",
      "train loss:2.291446938292746\n",
      "train loss:2.275879275982363\n",
      "train loss:2.2890076460940825\n",
      "train loss:2.2892950626790145\n",
      "train loss:2.2967492047461624\n",
      "train loss:2.3002458081886497\n",
      "train loss:2.283955540898042\n",
      "train loss:2.275658793019264\n",
      "train loss:2.289086712989784\n",
      "train loss:2.288604278975792\n",
      "train loss:2.3035436415646204\n",
      "train loss:2.282285976198848\n",
      "train loss:2.269449889581953\n",
      "train loss:2.3113598673064844\n",
      "train loss:2.2873645572943673\n",
      "train loss:2.2985552082843936\n",
      "train loss:2.2990909139446956\n",
      "train loss:2.2827631140729263\n",
      "train loss:2.29125344900262\n",
      "train loss:2.3022870033184986\n",
      "train loss:2.289895292123789\n",
      "train loss:2.2896324734172095\n",
      "train loss:2.2930170876929856\n",
      "train loss:2.279798628609227\n",
      "train loss:2.2832995787370134\n",
      "train loss:2.27102812981361\n",
      "train loss:2.292912874356311\n",
      "train loss:2.30092732880928\n",
      "train loss:2.2880656521497267\n",
      "train loss:2.2879881022531743\n",
      "train loss:2.2890302072854745\n",
      "train loss:2.2962471676343585\n",
      "train loss:2.289787355105804\n",
      "train loss:2.2941298941096653\n",
      "train loss:2.2925305184893903\n",
      "train loss:2.2924509390985794\n",
      "train loss:2.2735721215731215\n",
      "train loss:2.266286973971218\n",
      "train loss:2.31338255772726\n",
      "train loss:2.276163675469772\n",
      "train loss:2.2980662471584807\n",
      "train loss:2.3097761733972257\n",
      "train loss:2.2828989267960287\n",
      "train loss:2.2866404205108433\n",
      "train loss:2.296302891337936\n",
      "train loss:2.2836083276272863\n",
      "train loss:2.2835289856927994\n",
      "train loss:2.280212723062717\n",
      "train loss:2.2755428805997937\n",
      "train loss:2.2839166269176303\n",
      "train loss:2.2858335047832217\n",
      "train loss:2.278907590100057\n",
      "train loss:2.3054928983590646\n",
      "train loss:2.2861886204627835\n",
      "train loss:2.2868077494347547\n",
      "train loss:2.2627965084950734\n",
      "train loss:2.287449258901079\n",
      "train loss:2.283838304535302\n",
      "train loss:2.2934638516946517\n",
      "train loss:2.299238734012823\n",
      "train loss:2.2932140239592442\n",
      "train loss:2.2832722869202637\n",
      "train loss:2.2709945558631928\n",
      "train loss:2.29585315710149\n",
      "train loss:2.2702952907475313\n",
      "train loss:2.2924513825322177\n",
      "train loss:2.2713811461014313\n",
      "train loss:2.2883296110050324\n",
      "train loss:2.258544014935818\n",
      "train loss:2.2976495232223177\n",
      "train loss:2.2966128094375846\n",
      "train loss:2.276027563638937\n",
      "train loss:2.2835199619230986\n",
      "train loss:2.2734768885845558\n",
      "train loss:2.301211824406659\n",
      "train loss:2.3009226763946673\n",
      "train loss:2.293478973100739\n",
      "train loss:2.2651926846946124\n",
      "train loss:2.2822510257545714\n",
      "train loss:2.2851170253091224\n",
      "train loss:2.304455957058802\n",
      "train loss:2.2977207346487525\n",
      "train loss:2.2798603898379075\n",
      "train loss:2.306224313018058\n",
      "train loss:2.2803962985556194\n",
      "train loss:2.2924475725386\n",
      "train loss:2.275492544901523\n",
      "train loss:2.2831698621858454\n",
      "train loss:2.278771778136162\n",
      "train loss:2.274126797312289\n",
      "train loss:2.273807235704388\n",
      "train loss:2.2860824683035963\n",
      "train loss:2.270486114583794\n",
      "train loss:2.3014331107764194\n",
      "train loss:2.2693712293511226\n",
      "train loss:2.2689341133247796\n",
      "train loss:2.2867470467493254\n",
      "train loss:2.2795947805597905\n",
      "train loss:2.278951452355705\n",
      "train loss:2.284561884648965\n",
      "train loss:2.2795234153127883\n",
      "train loss:2.2541655596961263\n",
      "train loss:2.279184490602895\n",
      "train loss:2.2809908744828866\n",
      "train loss:2.2930320324008604\n",
      "train loss:2.3039154782293885\n",
      "train loss:2.277619234954817\n",
      "train loss:2.294455383112512\n",
      "train loss:2.260099193517979\n",
      "train loss:2.2619973693426227\n",
      "train loss:2.274594464230028\n",
      "train loss:2.2966795564878764\n",
      "train loss:2.296360914167831\n",
      "train loss:2.28498177899634\n",
      "train loss:2.2922642823229853\n",
      "train loss:2.270243840458502\n",
      "train loss:2.281376900101418\n",
      "train loss:2.272072626094614\n",
      "train loss:2.273770284741707\n",
      "train loss:2.2782316294037446\n",
      "train loss:2.283875829338741\n",
      "train loss:2.275937310246421\n",
      "train loss:2.2811772351379975\n",
      "train loss:2.283827932290603\n",
      "train loss:2.282764670051182\n",
      "train loss:2.2878411155899063\n",
      "train loss:2.2695468944915858\n",
      "train loss:2.2565342367035552\n",
      "train loss:2.276198651094336\n",
      "train loss:2.263017544290354\n",
      "train loss:2.2857444397855593\n",
      "train loss:2.2910859713317517\n",
      "train loss:2.2973864373125603\n",
      "train loss:2.255393935428768\n",
      "train loss:2.268287296018924\n",
      "train loss:2.2834201261406317\n",
      "train loss:2.2634334000176297\n",
      "train loss:2.300036551148538\n",
      "train loss:2.2790077207419257\n",
      "train loss:2.257122775788084\n",
      "train loss:2.3047908097841567\n",
      "train loss:2.2722133078755227\n",
      "train loss:2.3026744944280413\n",
      "train loss:2.2437733106406865\n",
      "train loss:2.2513206795271477\n",
      "train loss:2.2740299791474294\n",
      "train loss:2.3069481535506724\n",
      "train loss:2.285071312220723\n",
      "train loss:2.279333662008936\n",
      "train loss:2.2675791379364387\n",
      "train loss:2.2890732731618972\n",
      "train loss:2.2924955633655504\n",
      "train loss:2.2843022101492387\n",
      "train loss:2.2662721668806336\n",
      "train loss:2.2905275881017038\n",
      "train loss:2.26489405148412\n",
      "train loss:2.2608717150148303\n",
      "train loss:2.2868687518184685\n",
      "train loss:2.2817672949678856\n",
      "train loss:2.244374614434523\n",
      "train loss:2.276543647262449\n",
      "train loss:2.2885858648510293\n",
      "train loss:2.2701981951668087\n",
      "train loss:2.2566138885642673\n",
      "train loss:2.2653555681894946\n",
      "train loss:2.2823334658079686\n",
      "train loss:2.2559298640862275\n",
      "train loss:2.2815780070892733\n",
      "train loss:2.274549065315297\n",
      "train loss:2.273894113228683\n",
      "train loss:2.2687499851584105\n",
      "train loss:2.255095069381662\n",
      "train loss:2.260496071289212\n",
      "train loss:2.2592334570976327\n",
      "train loss:2.2424160499543357\n",
      "train loss:2.2657120591019013\n",
      "train loss:2.2639100546414297\n",
      "train loss:2.27327465382704\n",
      "train loss:2.260662358473873\n",
      "train loss:2.237283910353494\n",
      "train loss:2.277732134242032\n",
      "train loss:2.286935036767564\n",
      "train loss:2.2637021355520024\n",
      "train loss:2.2410423251858185\n",
      "train loss:2.2274561471347702\n",
      "train loss:2.293619856690322\n",
      "train loss:2.2503377557929474\n",
      "train loss:2.295425378400952\n",
      "train loss:2.2790189528676468\n",
      "train loss:2.27217786154956\n",
      "train loss:2.2879954594495135\n",
      "train loss:2.2919437473116377\n",
      "train loss:2.26106115557352\n",
      "train loss:2.245759036344018\n",
      "train loss:2.2286468630927505\n",
      "train loss:2.256048939690142\n",
      "train loss:2.285076219890281\n",
      "train loss:2.2965811149811963\n",
      "train loss:2.2483754122944397\n",
      "train loss:2.264289815637689\n",
      "train loss:2.2762013438008504\n",
      "train loss:2.2665105079740173\n",
      "train loss:2.2646548150177708\n",
      "train loss:2.259513660834792\n",
      "train loss:2.247219011509357\n",
      "train loss:2.2747135932584173\n",
      "train loss:2.2490874520215782\n",
      "train loss:2.2434337528740023\n",
      "train loss:2.234880616008717\n",
      "train loss:2.3006471628099643\n",
      "train loss:2.268078014631217\n",
      "train loss:2.264550227103652\n",
      "train loss:2.248168478956203\n",
      "train loss:2.265385383798872\n",
      "train loss:2.2564447032424892\n",
      "train loss:2.240142944665773\n",
      "train loss:2.3021907906918226\n",
      "train loss:2.237271491415206\n",
      "train loss:2.269744897823596\n",
      "train loss:2.276884372863664\n",
      "train loss:2.2649251770210057\n",
      "train loss:2.223191142724588\n",
      "train loss:2.2492134855426302\n",
      "train loss:2.2671964139006646\n",
      "train loss:2.2472190526116265\n",
      "train loss:2.199918162883138\n",
      "train loss:2.2572232564837322\n",
      "train loss:2.20887815313881\n",
      "train loss:2.1914941351931354\n",
      "train loss:2.2548478946249095\n",
      "train loss:2.26722551256389\n",
      "train loss:2.230964295694634\n",
      "train loss:2.254619920050133\n",
      "train loss:2.2853897584198073\n",
      "train loss:2.2310725348851066\n",
      "train loss:2.2670067358461585\n",
      "train loss:2.2628657084059647\n",
      "train loss:2.2660917488723946\n",
      "train loss:2.2629407159118196\n",
      "train loss:2.2519233210699374\n",
      "train loss:2.286953124445661\n",
      "train loss:2.2488972825717646\n",
      "train loss:2.254036752091518\n",
      "train loss:2.237353429567027\n",
      "train loss:2.2375363101295456\n",
      "train loss:2.242665029757185\n",
      "train loss:2.2486770137538588\n",
      "train loss:2.2581289819908585\n",
      "train loss:2.2484329070779814\n",
      "train loss:2.220721338500581\n",
      "train loss:2.2616441126314784\n",
      "train loss:2.2326111335519636\n",
      "train loss:2.23787356280084\n",
      "train loss:2.24078225756243\n",
      "train loss:2.23316763768072\n",
      "train loss:2.217157444839599\n",
      "train loss:2.263506083015364\n",
      "train loss:2.219145368444508\n",
      "train loss:2.2391672684928183\n",
      "train loss:2.273385881303989\n",
      "train loss:2.1880037350385577\n",
      "train loss:2.226724627379965\n",
      "train loss:2.2552045659449087\n",
      "train loss:2.2156675491585434\n",
      "train loss:2.2503441106691824\n",
      "train loss:2.2031907731270874\n",
      "train loss:2.254738260702724\n",
      "train loss:2.2289812978153627\n",
      "train loss:2.2465227174261435\n",
      "train loss:2.2382066071719326\n",
      "train loss:2.2291052929557917\n",
      "train loss:2.2230396117818243\n",
      "train loss:2.2273082044219388\n",
      "train loss:2.2663325624838384\n",
      "train loss:2.218871334376555\n",
      "train loss:2.214503653908801\n",
      "train loss:2.210225882891167\n",
      "train loss:2.210198606655183\n",
      "train loss:2.214126238387462\n",
      "train loss:2.2285419144394574\n",
      "train loss:2.2467434544966665\n",
      "train loss:2.261389588146823\n",
      "train loss:2.22671703319734\n",
      "train loss:2.278755047330312\n",
      "train loss:2.221213728723856\n",
      "train loss:2.206518765251328\n",
      "train loss:2.1521211324297798\n",
      "train loss:2.1958077346466016\n",
      "train loss:2.1845970751361756\n",
      "train loss:2.1473565386202518\n",
      "train loss:2.2379865159178296\n",
      "train loss:2.2148637031393483\n",
      "train loss:2.269626867672879\n",
      "train loss:2.2279329671716943\n",
      "train loss:2.2236081071315184\n",
      "train loss:2.235523051701367\n",
      "train loss:2.1652755747928936\n",
      "train loss:2.251969187782423\n",
      "train loss:2.187016469113461\n",
      "train loss:2.2190150311745187\n",
      "train loss:2.221021824842141\n",
      "train loss:2.2132381517833397\n",
      "train loss:2.1844704951920244\n",
      "train loss:2.271803393213035\n",
      "train loss:2.156648076554688\n",
      "train loss:2.1955467645682347\n",
      "train loss:2.2457580945839086\n",
      "train loss:2.261520259653532\n",
      "train loss:2.2309626726504934\n",
      "train loss:2.2476483896896102\n",
      "train loss:2.2299651263176776\n",
      "train loss:2.1589056781846425\n",
      "train loss:2.1658311842290496\n",
      "train loss:2.2287730070394787\n",
      "train loss:2.214818752293068\n",
      "train loss:2.123673082918549\n",
      "train loss:2.1793016210089835\n",
      "train loss:2.1851632856493968\n",
      "train loss:2.200404503235263\n",
      "train loss:2.201967474448659\n",
      "train loss:2.1933347364737212\n",
      "train loss:2.126294153416163\n",
      "train loss:2.191079572466921\n",
      "train loss:2.238035932240231\n",
      "train loss:2.1955640001124856\n",
      "train loss:2.1886712458247493\n",
      "train loss:2.1567850667399506\n",
      "train loss:2.131582615802042\n",
      "train loss:2.088198696005018\n",
      "train loss:2.1370613359730104\n",
      "train loss:2.0824557082024153\n",
      "train loss:2.1986781455531474\n",
      "train loss:2.123451866878857\n",
      "train loss:2.230384913737052\n",
      "train loss:2.1754245189220485\n",
      "train loss:2.0867732573973266\n",
      "train loss:2.1312632950442136\n",
      "train loss:2.191391779068873\n",
      "train loss:2.170752948648027\n",
      "train loss:2.116766099667304\n",
      "train loss:2.146400160785183\n",
      "train loss:2.1157534249346934\n",
      "train loss:2.1490357293203557\n",
      "train loss:2.1513839207242422\n",
      "train loss:2.11721537401682\n",
      "train loss:2.163029564776023\n",
      "train loss:2.0268850821911615\n",
      "train loss:2.0708896001796018\n",
      "train loss:2.1605017562351447\n",
      "train loss:2.205851334503529\n",
      "train loss:2.158850387044451\n",
      "train loss:2.1252403939764624\n",
      "train loss:2.111549503156863\n",
      "train loss:2.163856575268024\n",
      "train loss:2.08886665140737\n",
      "train loss:2.0740331490736468\n",
      "train loss:2.20919466829064\n",
      "train loss:2.1623613418658536\n",
      "train loss:1.9120447664595113\n",
      "train loss:2.0820653912098286\n",
      "train loss:2.2365644271675977\n",
      "train loss:2.1363678952815115\n",
      "train loss:2.0638159236846\n",
      "train loss:2.1658497691976906\n",
      "train loss:2.116218006998027\n",
      "train loss:2.0312139910411826\n",
      "train loss:2.145110625331453\n",
      "train loss:2.136913770488604\n",
      "train loss:2.1567782841182135\n",
      "train loss:2.108258660973754\n",
      "train loss:1.9339391412015066\n",
      "train loss:2.2227131199237062\n",
      "train loss:2.0255288099695337\n",
      "train loss:2.0599340504574712\n",
      "train loss:2.156010038874137\n",
      "train loss:2.1720550388055684\n",
      "train loss:2.0919391130700697\n",
      "train loss:2.0793683331127255\n",
      "train loss:2.138857631628247\n",
      "train loss:2.077503887590407\n",
      "train loss:1.8170432594523565\n",
      "train loss:2.0184370347021225\n",
      "train loss:2.220484677216495\n",
      "train loss:2.0766592525235072\n",
      "train loss:2.1673642061633616\n",
      "train loss:2.0949433702510563\n",
      "train loss:2.0789197974201454\n",
      "train loss:2.194115044098099\n",
      "train loss:2.01761148147477\n",
      "train loss:2.04596739126654\n",
      "train loss:2.047628174976846\n",
      "train loss:1.9583306091489314\n",
      "train loss:2.097171914227535\n",
      "train loss:2.102298909808356\n",
      "train loss:2.042560179043404\n",
      "train loss:1.9914623263716122\n",
      "train loss:2.063009924475659\n",
      "train loss:2.036300108809615\n",
      "train loss:2.0794448626111603\n",
      "train loss:2.1401433148832765\n",
      "train loss:1.9194768766013937\n",
      "train loss:2.0549905677161195\n",
      "train loss:2.0012534524582533\n",
      "train loss:2.0659470299796583\n",
      "train loss:2.0984608546366386\n",
      "train loss:2.052996561705278\n",
      "train loss:1.9732365570465733\n",
      "train loss:1.9471232033249568\n",
      "train loss:1.9522975195998125\n",
      "train loss:1.8831824983037007\n",
      "train loss:2.0195665917929766\n",
      "train loss:2.009630597341481\n",
      "train loss:2.0031235241856846\n",
      "train loss:1.94997868961452\n",
      "train loss:2.0434893937644425\n",
      "train loss:2.0397827316589363\n",
      "train loss:2.047610659803671\n",
      "train loss:1.9554659505442153\n",
      "train loss:1.963289674801574\n",
      "train loss:2.0627337600075464\n",
      "train loss:2.0235288206590845\n",
      "train loss:2.015767312977563\n",
      "train loss:1.964932035558034\n",
      "train loss:1.9297790363429639\n",
      "train loss:2.0026817906895102\n",
      "train loss:2.0305776628178434\n",
      "train loss:1.9471733496427883\n",
      "train loss:1.8412063466633453\n",
      "train loss:1.68874537525833\n",
      "train loss:1.9574234816417848\n",
      "train loss:2.0190397551880572\n",
      "train loss:1.8567200789418579\n",
      "train loss:1.7769096463694682\n",
      "train loss:1.780124850711156\n",
      "train loss:1.7927526024922908\n",
      "train loss:1.9240614745591313\n",
      "train loss:1.894399816952717\n",
      "train loss:1.8769922999621405\n",
      "train loss:1.7424184686870516\n",
      "train loss:1.970583107925917\n",
      "train loss:1.9040882737872231\n",
      "train loss:1.9614691639743214\n",
      "train loss:1.8738182216296762\n",
      "train loss:2.0230200748629414\n",
      "train loss:1.9964889000488304\n",
      "train loss:1.852696463373125\n",
      "train loss:1.9658184951813336\n",
      "train loss:1.6562549500986665\n",
      "train loss:1.660325315418419\n",
      "train loss:1.9484811155437631\n",
      "train loss:1.7400433583744856\n",
      "train loss:1.658632948088665\n",
      "train loss:1.9014025192423407\n",
      "train loss:1.999698784739621\n",
      "train loss:1.863295859262753\n",
      "train loss:1.8117913518636777\n",
      "train loss:1.3876644576718715\n",
      "train loss:1.77345321022594\n",
      "train loss:1.8094489650086691\n",
      "train loss:1.9316488835996917\n",
      "train loss:1.6307771702779357\n",
      "train loss:1.8274216530314198\n",
      "train loss:1.7876033010285841\n",
      "train loss:1.8866297165209795\n",
      "train loss:1.8534380715489776\n",
      "train loss:1.716574383091218\n",
      "train loss:1.7019878880581043\n",
      "train loss:1.6920843528912957\n",
      "train loss:1.6942962884344068\n",
      "train loss:1.759869992366161\n",
      "train loss:1.6559351278395977\n",
      "train loss:1.636367511079598\n",
      "train loss:1.841399324437267\n",
      "train loss:1.624417598352609\n",
      "train loss:1.7349086987423599\n",
      "train loss:1.732805804468253\n",
      "train loss:1.8018172071766387\n",
      "train loss:1.7744086131888794\n",
      "train loss:1.843974407485701\n",
      "train loss:1.589825819154544\n",
      "train loss:1.5308942940049128\n",
      "train loss:1.3997282658600088\n",
      "train loss:1.5469781283038895\n",
      "train loss:1.6844149570819378\n",
      "train loss:1.7235784172160826\n",
      "train loss:1.4909814411338562\n",
      "train loss:1.5113129317545309\n",
      "train loss:1.6849902503476288\n",
      "train loss:1.3797596803916004\n",
      "train loss:1.5039686602712727\n",
      "train loss:1.6151323789224254\n",
      "train loss:1.3194278040431704\n",
      "train loss:1.6600965603777762\n",
      "train loss:1.6161130500183272\n",
      "train loss:1.5694639918097564\n",
      "train loss:1.3785080217615904\n",
      "train loss:1.450093592212054\n",
      "train loss:1.623826622538527\n",
      "train loss:1.2760838615251742\n",
      "train loss:1.2402049398492103\n",
      "train loss:1.4944565074374245\n",
      "train loss:1.4552094232302342\n",
      "train loss:1.1843421277754564\n",
      "train loss:1.449601199539409\n",
      "train loss:1.6120882789477518\n",
      "train loss:1.4029976201624952\n",
      "train loss:1.4162709396720943\n",
      "train loss:1.3401467665347573\n",
      "train loss:1.470506298119334\n",
      "train loss:1.1623570672726284\n",
      "train loss:1.446489762626151\n",
      "train loss:1.5865022746740252\n",
      "train loss:1.4750864929247183\n",
      "train loss:1.1098480522032763\n",
      "train loss:1.2198354958922208\n",
      "train loss:1.3481055744817814\n",
      "train loss:1.4253885231033676\n",
      "train loss:1.432809576150432\n",
      "train loss:1.1447322751175253\n",
      "train loss:1.2093112405332478\n",
      "train loss:1.374493646435382\n",
      "train loss:1.3469516361369611\n",
      "train loss:1.279057152668691\n",
      "train loss:1.3693359316698497\n",
      "train loss:1.307615282749692\n",
      "train loss:1.3643622616494657\n",
      "train loss:1.2217174351358533\n",
      "train loss:1.4624153652243788\n",
      "train loss:1.4873194436203572\n",
      "train loss:1.4410436492599707\n",
      "train loss:1.3167665837721332\n",
      "train loss:1.4010950926773693\n",
      "train loss:1.259753143505458\n",
      "train loss:1.0984685636037712\n",
      "train loss:1.3449923603728529\n",
      "train loss:1.420576495289294\n",
      "train loss:1.290325172303016\n",
      "train loss:1.3494949854565124\n",
      "train loss:1.1828483074146088\n",
      "train loss:1.2667228183307042\n",
      "train loss:1.1503392208037724\n",
      "train loss:1.092963893404256\n",
      "train loss:1.3150526199916373\n",
      "train loss:1.284629466737298\n",
      "train loss:0.9847165215591379\n",
      "train loss:1.0339320230063906\n",
      "train loss:1.2100873496285547\n",
      "train loss:1.2011108655289395\n",
      "train loss:1.32918151931895\n",
      "train loss:1.4230549995785293\n",
      "train loss:1.2198260625340538\n",
      "train loss:1.10946254718373\n",
      "train loss:0.938549095317417\n",
      "train loss:1.2776482726132017\n",
      "train loss:1.2436038415652146\n",
      "train loss:1.0945952822521765\n",
      "train loss:1.0878066212389566\n",
      "train loss:1.144607908773412\n",
      "train loss:1.1024113321547253\n",
      "train loss:1.2163493788552504\n",
      "train loss:0.8910946575279333\n",
      "train loss:1.2991888173304398\n",
      "train loss:1.0292316742665335\n",
      "train loss:0.9339232746260484\n",
      "train loss:1.344767638906759\n",
      "train loss:1.0425107285827993\n",
      "train loss:0.8708570393896218\n",
      "train loss:1.1599735037287873\n",
      "train loss:1.1108146625889401\n",
      "train loss:1.0652956488680276\n",
      "train loss:0.9758687324441873\n",
      "train loss:0.9659474184171416\n",
      "train loss:1.0969738134595133\n",
      "train loss:1.034861470192723\n",
      "train loss:0.6621376109062674\n",
      "train loss:1.051218866890948\n",
      "train loss:1.1696549801560774\n",
      "train loss:1.2903820891676183\n",
      "train loss:0.9526339695096641\n",
      "train loss:1.139698819635642\n",
      "train loss:0.9721155540843535\n",
      "train loss:0.7956978920773742\n",
      "train loss:1.040115953710968\n",
      "train loss:0.8119809291953215\n",
      "train loss:0.7402201615806825\n",
      "train loss:0.7781669513696013\n",
      "train loss:1.0605569458261466\n",
      "train loss:0.8212739611917804\n",
      "train loss:1.1422959697185733\n",
      "train loss:0.8105058049580509\n",
      "train loss:0.8413040712589704\n",
      "train loss:0.8819261128430917\n",
      "train loss:0.9656199009985659\n",
      "train loss:1.0257392059689263\n",
      "train loss:0.9700322019853134\n",
      "train loss:0.8770197436480124\n",
      "train loss:0.9910220756284527\n",
      "train loss:1.2110668403761333\n",
      "train loss:1.1066499653746826\n",
      "train loss:1.0047179385662885\n",
      "train loss:1.2858871761845716\n",
      "train loss:0.8704608095911853\n",
      "train loss:0.6503329701205298\n",
      "train loss:0.9163989796046546\n",
      "train loss:0.9220763351923762\n",
      "train loss:0.8066837879681603\n",
      "train loss:1.0732820896634598\n",
      "train loss:0.8792037226550091\n",
      "train loss:1.135368404652715\n",
      "train loss:1.2574609941868398\n",
      "train loss:0.8646562555312666\n",
      "train loss:0.7604119888136155\n",
      "train loss:0.8223076891102683\n",
      "train loss:0.945499948619531\n",
      "train loss:0.5383722134024573\n",
      "train loss:1.0192897637549763\n",
      "train loss:0.7795661481318967\n",
      "train loss:1.2771580487375906\n",
      "train loss:0.6981178253454964\n",
      "train loss:0.8946317066906035\n",
      "train loss:0.8409653494588492\n",
      "train loss:0.810116237474744\n",
      "train loss:0.7159849576254482\n",
      "train loss:0.586801993067471\n",
      "train loss:0.8113558758416894\n",
      "train loss:0.943609483874601\n",
      "train loss:0.678762096605741\n",
      "train loss:0.944709947638481\n",
      "train loss:0.7553896333828858\n",
      "train loss:0.8835641566518633\n",
      "train loss:1.0361539796509731\n",
      "train loss:0.6857433849429373\n",
      "train loss:0.8481318585910071\n",
      "train loss:0.8723748874054227\n",
      "train loss:0.9269176642971754\n",
      "train loss:0.8300968860727262\n",
      "train loss:0.8457435948343697\n",
      "train loss:0.8658514822968648\n",
      "train loss:1.0517607786457281\n",
      "train loss:0.7546183266746098\n",
      "train loss:0.5130554513204604\n",
      "train loss:0.7688018714800348\n",
      "train loss:0.6769453600655502\n",
      "train loss:0.9152956186771122\n",
      "train loss:0.7816238474563193\n",
      "train loss:0.7265886179625667\n",
      "train loss:0.647319690286569\n",
      "train loss:0.8462816898271334\n",
      "train loss:1.0214241742456212\n",
      "train loss:0.7372762280926106\n",
      "train loss:0.46018035346919506\n",
      "train loss:0.919597678588652\n",
      "train loss:0.8189997392206843\n",
      "train loss:0.8892773415705721\n",
      "train loss:0.7577284552238508\n",
      "train loss:0.8772221875278405\n",
      "train loss:0.5544144921080292\n",
      "train loss:0.5299768374744815\n",
      "train loss:0.7444474532389508\n",
      "train loss:0.8094209329626656\n",
      "train loss:0.8588089980034312\n",
      "train loss:0.5991688693339781\n",
      "train loss:0.8031202297257636\n",
      "train loss:0.5986540732170671\n",
      "train loss:0.831919851005152\n",
      "train loss:0.5394988965467697\n",
      "train loss:0.6072300564666466\n",
      "train loss:0.7619521129096106\n",
      "train loss:0.6759967359871468\n",
      "train loss:0.8874904991684007\n",
      "train loss:0.6097571812337937\n",
      "train loss:0.6151439776336449\n",
      "train loss:0.8053195923285257\n",
      "train loss:0.9399769591600411\n",
      "train loss:0.7534701506029168\n",
      "train loss:0.8817266149645554\n",
      "train loss:0.9649041112646919\n",
      "train loss:0.7259677234485069\n",
      "train loss:0.9376014538078776\n",
      "train loss:0.4884914958490831\n",
      "train loss:0.7900961602880838\n",
      "train loss:0.7643790201328701\n",
      "train loss:0.890717125310387\n",
      "train loss:0.4673104381175531\n",
      "train loss:0.5073180474828248\n",
      "train loss:0.9149983614123084\n",
      "train loss:0.6554714860576468\n",
      "train loss:0.7012621663897071\n",
      "train loss:0.6633206106989706\n",
      "train loss:0.7396361039239752\n",
      "train loss:0.7505162151017349\n",
      "train loss:0.8023548157314817\n",
      "train loss:0.736545134042454\n",
      "train loss:0.7463979998344198\n",
      "train loss:0.6830897212289835\n",
      "train loss:1.0710566929055476\n",
      "train loss:0.676585512224509\n",
      "train loss:0.46713236345556003\n",
      "train loss:0.36689152397605923\n",
      "train loss:0.4234859179448164\n",
      "train loss:0.5141390131517736\n",
      "train loss:0.5802217187918409\n",
      "train loss:0.5328827214286096\n",
      "train loss:0.6512697607712152\n",
      "train loss:0.42478995514286\n",
      "train loss:0.6094106147808149\n",
      "train loss:0.7664657905027086\n",
      "train loss:0.5096733794506991\n",
      "train loss:0.5297698468301404\n",
      "train loss:0.6952007268009799\n",
      "train loss:0.6563458262666718\n",
      "train loss:0.5499527863822866\n",
      "train loss:0.6206270483247106\n",
      "train loss:0.4782356388953644\n",
      "train loss:0.5258198457847478\n",
      "train loss:0.6293044772667625\n",
      "train loss:0.40735964310936573\n",
      "train loss:0.7376032908365031\n",
      "train loss:0.5602040193930236\n",
      "train loss:0.5844700400603735\n",
      "train loss:0.5947759103446264\n",
      "train loss:0.8977345925852258\n",
      "train loss:0.43693458727541423\n",
      "train loss:0.725325320630793\n",
      "train loss:0.6533124699367873\n",
      "train loss:0.6993137944221008\n",
      "train loss:0.3772887213988938\n",
      "train loss:0.37541457291651387\n",
      "train loss:0.5594714564902018\n",
      "train loss:0.5281051462453813\n",
      "train loss:0.6618999963249952\n",
      "train loss:0.6306087323826742\n",
      "train loss:0.8164237995756045\n",
      "train loss:0.7810231345406382\n",
      "train loss:0.7899641843748545\n",
      "train loss:0.5327250505114919\n",
      "train loss:0.8259406192750282\n",
      "train loss:0.5413299353649843\n",
      "train loss:0.576400599724016\n",
      "train loss:0.5285540819370629\n",
      "train loss:0.45432880877092807\n",
      "train loss:0.6156433910280394\n",
      "train loss:0.677684775632774\n",
      "train loss:0.6935268361271882\n",
      "train loss:0.4963182110069342\n",
      "train loss:0.9080824870744716\n",
      "train loss:0.5064336150823925\n",
      "train loss:0.4214352239419065\n",
      "train loss:0.6527015945110142\n",
      "train loss:0.48637128209708175\n",
      "train loss:0.5566746554229198\n",
      "train loss:0.7703533825672977\n",
      "train loss:0.6241382740317951\n",
      "train loss:0.43344519427348027\n",
      "train loss:0.43210910768302185\n",
      "train loss:0.5209647227649636\n",
      "train loss:0.5884291112440487\n",
      "train loss:0.47189694248371894\n",
      "train loss:0.4768481984552661\n",
      "train loss:0.31863167699225803\n",
      "train loss:0.9528495013734033\n",
      "train loss:0.5377021190386034\n",
      "train loss:0.6650219567269013\n",
      "train loss:0.4407405454964599\n",
      "train loss:0.6996890646431226\n",
      "train loss:0.5656223290562035\n",
      "train loss:0.3995850218011643\n",
      "train loss:0.6833626482586024\n",
      "train loss:0.7061798604371168\n",
      "train loss:0.40163543425260106\n",
      "train loss:0.563706907514145\n",
      "train loss:0.5890957679417016\n",
      "train loss:0.8951808357834785\n",
      "train loss:0.6691051078368508\n",
      "train loss:0.5296081403580676\n",
      "train loss:0.2875003274090441\n",
      "train loss:0.5818389306176271\n",
      "train loss:0.533056657661951\n",
      "train loss:0.41007161726459\n",
      "train loss:0.48314494129661245\n",
      "train loss:0.6850495812369399\n",
      "train loss:0.5021070356592922\n",
      "train loss:0.7053404209732439\n",
      "train loss:0.38552907428653993\n",
      "train loss:0.3126064979631401\n",
      "train loss:0.8696800643011986\n",
      "train loss:0.5745931922230956\n",
      "train loss:0.4849735239249173\n",
      "train loss:0.44014527804988285\n",
      "train loss:0.882857322555823\n",
      "train loss:0.3429967536102946\n",
      "train loss:0.5056562243700989\n",
      "train loss:0.522200186676663\n",
      "train loss:0.6391882268079444\n",
      "train loss:0.3853650011599799\n",
      "train loss:0.8399940744720061\n",
      "train loss:0.4344540080491194\n",
      "train loss:0.3555456252622535\n",
      "train loss:0.8612121977840154\n",
      "train loss:0.47907489413338616\n",
      "train loss:0.8075714410688953\n",
      "train loss:0.7158611033310497\n",
      "train loss:0.5386050360720458\n",
      "train loss:0.2453910557254661\n",
      "train loss:0.6026589035570136\n",
      "train loss:0.4721366696221646\n",
      "train loss:0.38925466923743546\n",
      "train loss:0.5564346398243657\n",
      "train loss:0.43703157854326\n",
      "train loss:0.7848183048326984\n",
      "train loss:0.8114799881908753\n",
      "train loss:0.5246639139955486\n",
      "train loss:0.3847435216389047\n",
      "train loss:0.34809617019036004\n",
      "train loss:0.47325178094978193\n",
      "train loss:0.4867347708738935\n",
      "train loss:0.7822693231204279\n",
      "train loss:0.5371382337017627\n",
      "train loss:0.22760662042118496\n",
      "train loss:0.4547650633936009\n",
      "train loss:0.4335171555931445\n",
      "train loss:0.39090566272923494\n",
      "train loss:0.5207228487902353\n",
      "train loss:0.5544679498564042\n",
      "train loss:0.3777237171161155\n",
      "train loss:0.6428151060637471\n",
      "train loss:0.5591707429085444\n",
      "train loss:0.37044449163332627\n",
      "train loss:0.32584016497890955\n",
      "train loss:0.5101543974411745\n",
      "train loss:0.49748776464780675\n",
      "train loss:0.43150041348171747\n",
      "train loss:0.5804975014486139\n",
      "train loss:0.7293237777361271\n",
      "train loss:0.3172822482817535\n",
      "train loss:0.4935195833591543\n",
      "train loss:0.25663095424723315\n",
      "train loss:0.6940791161339668\n",
      "train loss:0.5321621530493541\n",
      "train loss:0.4548588073360335\n",
      "train loss:0.2996651449941635\n",
      "train loss:0.8943585865267087\n",
      "train loss:0.5683909301417278\n",
      "train loss:0.8150347935084772\n",
      "train loss:0.47900906954301015\n",
      "train loss:0.5266399852216643\n",
      "train loss:0.23382300014973972\n",
      "train loss:0.4367056834249645\n",
      "train loss:0.4765452190575278\n",
      "train loss:0.35119702849991263\n",
      "train loss:0.40544843737553926\n",
      "train loss:0.6616975369522177\n",
      "train loss:0.41225299627698253\n",
      "train loss:0.6496654285904214\n",
      "train loss:0.5345756749555723\n",
      "train loss:0.34756150906473543\n",
      "train loss:0.35495414386701357\n",
      "train loss:0.34852894773942883\n",
      "train loss:0.28278885614650395\n",
      "train loss:0.7301820452188486\n",
      "train loss:0.9048457374288752\n",
      "train loss:0.4395433654710529\n",
      "train loss:0.4188824217206203\n",
      "train loss:0.2911083019047145\n",
      "train loss:0.3175973633451954\n",
      "train loss:0.3937323818299109\n",
      "train loss:0.35348492176864127\n",
      "train loss:0.3246138651624375\n",
      "train loss:0.25688566091929893\n",
      "train loss:0.26235018982423564\n",
      "train loss:0.6118677627469112\n",
      "train loss:0.8722982108494475\n",
      "train loss:0.5166808934598728\n",
      "train loss:1.1211998919245634\n",
      "train loss:0.4537060166929786\n",
      "train loss:0.25646528581244843\n",
      "train loss:0.24249419520312235\n",
      "train loss:0.2592804780318503\n",
      "train loss:0.41243181961964903\n",
      "train loss:0.6343096708538105\n",
      "train loss:0.5391246350412875\n",
      "train loss:0.347228930051927\n",
      "train loss:0.6129388238300311\n",
      "train loss:0.37892743011949703\n",
      "train loss:0.3865222438118905\n",
      "train loss:0.5612497821292126\n",
      "train loss:0.6451241845266995\n",
      "train loss:0.4813332392241327\n",
      "train loss:0.4292916345795553\n",
      "train loss:0.26422632743367236\n",
      "train loss:0.5004671371392756\n",
      "train loss:0.3998130060998638\n",
      "train loss:0.7421702989633086\n",
      "train loss:0.3556762440250566\n",
      "train loss:0.2985269821375576\n",
      "train loss:0.5194837032383384\n",
      "train loss:0.43217176906713384\n",
      "train loss:0.4981483269710223\n",
      "train loss:0.4038420079611406\n",
      "train loss:0.2953732699931436\n",
      "train loss:0.48882890624712283\n",
      "train loss:0.336765685554123\n",
      "train loss:0.37473975100473195\n",
      "train loss:0.3703801493037138\n",
      "train loss:0.9641104002357838\n",
      "train loss:0.4676628765756131\n",
      "train loss:0.22169713141372285\n",
      "train loss:0.15013578081352721\n",
      "train loss:0.4261211632343713\n",
      "train loss:0.42607381380506215\n",
      "train loss:0.4946300226837008\n",
      "train loss:1.0275731330142348\n",
      "train loss:0.3090061263288505\n",
      "train loss:0.3647485978965779\n",
      "train loss:0.41010562404112816\n",
      "train loss:0.39125645482902593\n",
      "train loss:0.3921743599959707\n",
      "train loss:0.2815728140842336\n",
      "train loss:0.33175672609159834\n",
      "train loss:0.2923202660466831\n",
      "train loss:0.371844762723165\n",
      "train loss:0.9488818307923236\n",
      "train loss:0.3866361232311959\n",
      "train loss:0.4122582596544621\n",
      "train loss:0.43688040207115225\n",
      "train loss:0.4844558942794477\n",
      "train loss:0.41046557200934286\n",
      "train loss:0.27729409695227525\n",
      "train loss:0.4477682550908493\n",
      "train loss:0.490073154372471\n",
      "train loss:0.4854288973358579\n",
      "train loss:0.3834072111062405\n",
      "train loss:0.5694596702685568\n",
      "train loss:0.343805076863133\n",
      "train loss:0.26082137827343743\n",
      "train loss:1.1314429359558726\n",
      "train loss:0.3807406542400386\n",
      "train loss:0.3663351305424146\n",
      "train loss:0.44912301213507433\n",
      "train loss:0.3557315326826019\n",
      "train loss:0.30807020314014477\n",
      "train loss:0.3532514200086229\n",
      "train loss:0.2419503873487717\n",
      "train loss:0.24217871442701172\n",
      "train loss:0.6614649857390216\n",
      "train loss:0.6537031804364456\n",
      "train loss:0.2721395004777295\n",
      "train loss:0.26863632928367237\n",
      "train loss:0.9719762203286363\n",
      "train loss:0.4896891482684555\n",
      "train loss:0.3958640072275072\n",
      "train loss:0.7005239093647511\n",
      "train loss:0.35857268726645514\n",
      "train loss:0.32113205732915967\n",
      "train loss:0.2750237353893118\n",
      "train loss:0.8016716296412854\n",
      "train loss:0.6014401321312368\n",
      "train loss:0.23004115589404273\n",
      "train loss:0.7716247535851283\n",
      "train loss:0.3618121691575076\n",
      "train loss:0.39974801024115675\n",
      "train loss:0.388960683186616\n",
      "train loss:0.7021423627283243\n",
      "train loss:0.5811194210136161\n",
      "train loss:0.4455459584035271\n",
      "train loss:0.3099564376717378\n",
      "train loss:0.20942901718414786\n",
      "train loss:0.2991599133715597\n",
      "train loss:0.6121148919546459\n",
      "train loss:0.4974148835067946\n",
      "train loss:0.22286783304198018\n",
      "train loss:0.39923195917099996\n",
      "train loss:0.40467026105249165\n",
      "train loss:0.26176983219710614\n",
      "train loss:0.5288506886430169\n",
      "train loss:0.18358279067293\n",
      "train loss:0.963112878348017\n",
      "train loss:0.35588623625567384\n",
      "train loss:0.25931595505870686\n",
      "train loss:0.0664408929551327\n",
      "train loss:0.3854551607441894\n",
      "train loss:0.6595210505640208\n",
      "train loss:0.761142973821513\n",
      "train loss:0.43516372364492745\n",
      "train loss:0.22779816275005993\n",
      "train loss:0.34303502443400924\n",
      "train loss:0.5371973820649422\n",
      "train loss:0.7347805938595606\n",
      "train loss:0.2657943197448562\n",
      "train loss:0.402811838842173\n",
      "train loss:0.7020585371502206\n",
      "train loss:0.4171290005125597\n",
      "train loss:0.08415191234625907\n",
      "train loss:0.2801226088459394\n",
      "train loss:0.29716009518744907\n",
      "train loss:0.39907605572707516\n",
      "train loss:0.19723078608428202\n",
      "train loss:0.5369493001640229\n",
      "train loss:0.398070552149844\n",
      "train loss:0.48229644971063357\n",
      "train loss:0.30927516077555073\n",
      "train loss:0.3032292247748073\n",
      "train loss:0.3808195386412332\n",
      "train loss:0.2730668656100889\n",
      "train loss:0.4572866385497105\n",
      "train loss:0.3445862487466459\n",
      "train loss:0.6024938650387838\n",
      "train loss:0.16245163378986718\n",
      "train loss:0.33203109705668504\n",
      "train loss:0.2417346409175914\n",
      "train loss:0.5072529107151735\n",
      "train loss:0.16041040934239256\n",
      "train loss:0.4164524410667173\n",
      "train loss:0.34159113149718656\n",
      "train loss:0.41150566496931457\n",
      "train loss:0.4306510198163821\n",
      "train loss:0.881479370036608\n",
      "train loss:0.3583695749997881\n",
      "train loss:0.3091718516454758\n",
      "train loss:0.3044315503869137\n",
      "train loss:0.440251720414389\n",
      "train loss:0.29718904069657837\n",
      "train loss:0.37977462784892707\n",
      "train loss:0.33748477654656456\n",
      "train loss:0.36256926839790193\n",
      "train loss:0.30512478878000354\n",
      "train loss:0.2524588001204966\n",
      "train loss:0.49089831933429845\n",
      "train loss:0.6174488533144773\n",
      "train loss:0.27567099814934104\n",
      "train loss:0.5671968540603403\n",
      "train loss:0.21556665297151906\n",
      "train loss:0.22218535573247378\n",
      "train loss:0.3510271285159513\n",
      "train loss:0.32491969111044383\n",
      "train loss:0.46463879703689415\n",
      "train loss:0.5693635627871383\n",
      "train loss:0.46334350744147124\n",
      "train loss:0.41238901549901463\n",
      "train loss:0.5039199254822753\n",
      "train loss:0.36075036955127837\n",
      "train loss:0.30551968273452257\n",
      "train loss:0.44162039506533046\n",
      "train loss:0.46600302578746206\n",
      "train loss:1.1015187916684235\n",
      "train loss:0.3581691872796416\n",
      "train loss:0.7776889171262532\n",
      "train loss:0.9300244601362798\n",
      "train loss:0.3112172854822076\n",
      "train loss:0.2883901896123869\n",
      "train loss:0.3361495523652004\n",
      "train loss:0.25475011830681843\n",
      "train loss:0.3764538267794575\n",
      "train loss:0.5699226524922759\n",
      "train loss:0.4569576372976798\n",
      "train loss:0.41981433649060496\n",
      "train loss:0.6794992299711209\n",
      "train loss:0.47896344750692055\n",
      "train loss:0.14393059098818345\n",
      "train loss:0.664956225753681\n",
      "train loss:0.6385677045884628\n",
      "train loss:0.35243293573818235\n",
      "train loss:0.19840551591214908\n",
      "train loss:0.4878865534706256\n",
      "train loss:0.4400029240909047\n",
      "train loss:0.18298140657992457\n",
      "train loss:0.22350437657821975\n",
      "train loss:0.19677972782821634\n",
      "train loss:0.2532044998022549\n",
      "train loss:0.34015312746926535\n",
      "train loss:0.42616198221310986\n",
      "train loss:0.33069393107623324\n",
      "train loss:0.22615648938652574\n",
      "train loss:0.33643173529260184\n",
      "train loss:0.38631121384972916\n",
      "train loss:0.3189026154683375\n",
      "train loss:0.394401704080204\n",
      "train loss:0.15233492925424758\n",
      "train loss:0.40794299092268105\n",
      "train loss:0.3056990051053889\n",
      "train loss:0.3347653012318622\n",
      "train loss:0.4941209408900774\n",
      "train loss:0.209070442860924\n",
      "train loss:0.430262522015763\n",
      "train loss:0.26223170927759093\n",
      "train loss:0.32516145562644266\n",
      "train loss:0.09914521713841493\n",
      "train loss:0.4251537156202026\n",
      "train loss:0.5010678823226806\n",
      "train loss:0.631044894405185\n",
      "train loss:0.444677243877605\n",
      "train loss:0.8068452637823987\n",
      "train loss:0.3552598105989098\n",
      "train loss:0.2718795445281472\n",
      "train loss:0.3864144082216652\n",
      "train loss:0.17898268576102805\n",
      "train loss:0.45190096376968963\n",
      "train loss:0.5911554689136095\n",
      "train loss:0.2412101425577045\n",
      "train loss:0.19557950860990608\n",
      "train loss:0.6209829549020983\n",
      "train loss:0.5824221104385023\n",
      "train loss:0.7007897510454257\n",
      "train loss:0.6983145658213401\n",
      "train loss:0.49247882153102385\n",
      "train loss:0.5933493015933954\n",
      "train loss:0.22032510804079838\n",
      "train loss:0.33158758167044855\n",
      "train loss:0.37763150021156017\n",
      "train loss:0.7896174898554167\n",
      "train loss:0.12346779328357532\n",
      "train loss:0.37797029314853836\n",
      "train loss:0.3885698443509014\n",
      "train loss:0.5844964752893935\n",
      "train loss:0.4008467550038961\n",
      "train loss:0.2166121451432468\n",
      "train loss:0.2768455764038307\n",
      "train loss:0.19878447484224193\n",
      "train loss:0.32102880500429354\n",
      "train loss:0.2832497834237908\n",
      "train loss:0.34605705641779416\n",
      "train loss:0.2508294040455052\n",
      "train loss:1.553872903931036\n",
      "train loss:0.11953629937499723\n",
      "train loss:0.12962425272650063\n",
      "train loss:0.2647391261270522\n",
      "train loss:0.2878368470288289\n",
      "train loss:0.3383468723797214\n",
      "train loss:0.5898071756292691\n",
      "train loss:0.2701615963584654\n",
      "train loss:0.36844804392683084\n",
      "train loss:0.14087158749596504\n",
      "train loss:0.422402214394569\n",
      "train loss:0.40125497436730273\n",
      "train loss:0.39251680899879976\n",
      "train loss:0.16064799838577534\n",
      "train loss:0.11750427761264844\n",
      "train loss:0.224262702922732\n",
      "train loss:0.3535404495725025\n",
      "train loss:0.3276244803128516\n",
      "train loss:0.48354989114405966\n",
      "train loss:0.37990563839526315\n",
      "train loss:0.24073059486268422\n",
      "train loss:0.1824447377070248\n",
      "train loss:0.2693624904698516\n",
      "train loss:0.9818625302315719\n",
      "train loss:0.22774464136445643\n",
      "train loss:0.4487453887186754\n",
      "train loss:0.20336156785427345\n",
      "train loss:0.27159652648584154\n",
      "train loss:0.5257682933464087\n",
      "train loss:0.4555627239466856\n",
      "train loss:0.19076354836922538\n",
      "train loss:0.20545538395443394\n",
      "train loss:0.1500823186240382\n",
      "train loss:0.22736013820114337\n",
      "train loss:0.2780791216595119\n",
      "train loss:0.4843149228820016\n",
      "train loss:0.33314256949279514\n",
      "train loss:0.20659013187698586\n",
      "train loss:0.6629842486304716\n",
      "train loss:0.41145012193928093\n",
      "train loss:0.24982843326392834\n",
      "train loss:0.21735133772469412\n",
      "train loss:0.23160571553024908\n",
      "train loss:0.39464496974319746\n",
      "train loss:0.062045461488345756\n",
      "train loss:0.4799983102767507\n",
      "train loss:0.2583012331651444\n",
      "train loss:0.20741680097452844\n",
      "train loss:0.130367695176477\n",
      "train loss:0.4409483255238565\n",
      "train loss:0.24323111774245842\n",
      "train loss:0.4457607319125655\n",
      "train loss:0.3052653125617772\n",
      "train loss:0.17263329406291675\n",
      "train loss:0.22306485154424102\n",
      "train loss:0.6840386826101266\n",
      "train loss:0.8368706847458254\n",
      "train loss:0.2925007577154995\n",
      "train loss:0.34001158344976506\n",
      "train loss:0.26334354679700034\n",
      "train loss:0.49739336996232664\n",
      "train loss:0.35452691384438795\n",
      "train loss:0.5023716419487001\n",
      "train loss:0.22268807164229903\n",
      "train loss:0.15632966187312924\n",
      "train loss:0.25826077964745103\n",
      "train loss:0.31307743031459323\n",
      "train loss:0.22457339427841594\n",
      "train loss:0.21337240343061128\n",
      "train loss:0.18407545130176944\n",
      "train loss:0.4546346812353206\n",
      "train loss:0.8264672460863094\n",
      "train loss:0.319205059369863\n",
      "train loss:0.22970727785032258\n",
      "train loss:0.07233440557280169\n",
      "train loss:0.21028202373311916\n",
      "train loss:0.14655543103293348\n",
      "train loss:0.3912917789690151\n",
      "train loss:0.21058737574832515\n",
      "train loss:0.2002004042370276\n",
      "train loss:0.5323271826146018\n",
      "train loss:0.3637785513530897\n",
      "train loss:0.4935769251072435\n",
      "train loss:0.3629050319566425\n",
      "train loss:0.6537260031247362\n",
      "train loss:0.22605737511534776\n",
      "train loss:0.3218198217720089\n",
      "train loss:0.4626593247945715\n",
      "train loss:0.14695045246146277\n",
      "train loss:0.3018000441098214\n",
      "train loss:0.3477891475943804\n",
      "train loss:0.48082617804895583\n",
      "train loss:0.4920137470650584\n",
      "train loss:0.11097343705597416\n",
      "train loss:0.21168212039766252\n",
      "train loss:0.3966466583226843\n",
      "train loss:0.22884252772376942\n",
      "train loss:0.28420436599863336\n",
      "train loss:0.5312699205267466\n",
      "train loss:0.43186862719145963\n",
      "train loss:0.22746840321649286\n",
      "train loss:0.5023106261722741\n",
      "train loss:0.4288605894735652\n",
      "train loss:0.19913370419191007\n",
      "train loss:0.21966589357111238\n",
      "train loss:0.33555217390009656\n",
      "train loss:0.36263683307698036\n",
      "train loss:0.08607814661121352\n",
      "train loss:0.24905781806837202\n",
      "train loss:0.30982433246601593\n",
      "train loss:0.5022033745023692\n",
      "train loss:0.30322619144931395\n",
      "train loss:0.35646308016090095\n",
      "train loss:0.5633285854561454\n",
      "train loss:0.47980432098280057\n",
      "train loss:0.27176591343172163\n",
      "train loss:0.45263350184393\n",
      "train loss:0.14752180135049953\n",
      "train loss:0.4376890062031843\n",
      "train loss:0.44693311430950966\n",
      "train loss:0.3235796716299758\n",
      "train loss:0.19391144582672268\n",
      "train loss:0.3302622009687238\n",
      "train loss:0.11190328673842354\n",
      "train loss:0.2529281930620413\n",
      "train loss:0.18221446062928712\n",
      "train loss:0.1257996635233236\n",
      "train loss:0.6516654382008141\n",
      "train loss:0.5209166635130889\n",
      "train loss:0.08549136800059115\n",
      "train loss:0.11982353997681196\n",
      "train loss:0.23990524382607373\n",
      "train loss:0.3040901738198175\n",
      "train loss:0.11140786049936562\n",
      "train loss:0.24508364259439308\n",
      "train loss:0.5373968622735505\n",
      "train loss:0.12000443396319721\n",
      "train loss:0.38320574469354024\n",
      "train loss:0.5121688606498633\n",
      "train loss:0.46380824191677583\n",
      "train loss:0.31988013639320445\n",
      "train loss:0.18775564426730434\n",
      "train loss:0.36815443483818977\n",
      "train loss:0.24225614568697584\n",
      "train loss:0.14828712632296592\n",
      "train loss:0.263610480649078\n",
      "train loss:0.1497113708552369\n",
      "train loss:0.558260067337892\n",
      "train loss:0.1868491637158854\n",
      "train loss:0.29061980466485915\n",
      "train loss:0.32877064214270474\n",
      "train loss:0.42681800791951\n",
      "train loss:0.26672323529219444\n",
      "train loss:0.2037011429322897\n",
      "train loss:0.5398828969267244\n",
      "train loss:0.2588304607040684\n",
      "train loss:0.21748762763262294\n",
      "train loss:0.2773123391119722\n",
      "train loss:0.1823868336004552\n",
      "train loss:0.3478764309146181\n",
      "train loss:0.33172585624034767\n",
      "train loss:0.6514936043488957\n",
      "train loss:0.7211447031766338\n",
      "train loss:0.5599027313358962\n",
      "train loss:0.17416448270615803\n",
      "train loss:0.24750789837127507\n",
      "train loss:0.47008488578459123\n",
      "train loss:0.184313275195576\n",
      "train loss:0.49240290006039045\n",
      "train loss:0.22162011641528884\n",
      "train loss:0.24471421890212136\n",
      "train loss:0.20793253441922757\n",
      "train loss:0.14346406653963745\n",
      "train loss:0.20162203092209002\n",
      "train loss:0.15875551067325339\n",
      "train loss:0.34880548469323736\n",
      "train loss:0.30404794166278704\n",
      "train loss:0.41631732585172443\n",
      "train loss:0.3035174752172225\n",
      "train loss:0.22587082210407566\n",
      "train loss:0.42587759947162895\n",
      "train loss:0.36635228651574586\n",
      "train loss:0.5161776165588579\n",
      "train loss:0.34715142964225393\n",
      "train loss:0.10144907459029548\n",
      "train loss:0.15029129463479324\n",
      "train loss:0.13989602296804324\n",
      "train loss:0.28317042180867236\n",
      "train loss:0.10873184987168494\n",
      "train loss:0.3221793670877762\n",
      "train loss:0.21500098956200808\n",
      "train loss:0.12995655051824462\n",
      "train loss:0.21551536418842857\n",
      "train loss:0.3397335899537591\n",
      "train loss:0.3364499015146034\n",
      "train loss:0.27988334108796437\n",
      "train loss:0.36136221985338857\n",
      "train loss:0.38068643401808144\n",
      "train loss:0.23353818688903488\n",
      "train loss:0.23145043845121072\n",
      "train loss:0.1587883495418024\n",
      "train loss:0.18266615518646465\n",
      "train loss:0.5184861881916234\n",
      "train loss:0.19589139375685607\n",
      "train loss:0.30955425851837787\n",
      "train loss:0.24118349992460844\n",
      "train loss:0.4148313997989547\n",
      "train loss:0.6080226819909895\n",
      "train loss:0.2537684151854959\n",
      "train loss:0.1827522125237386\n",
      "train loss:0.35329679140842024\n",
      "train loss:0.5463863769458557\n",
      "train loss:0.696255847111233\n",
      "train loss:0.1739792824107646\n",
      "train loss:0.14521648164945355\n",
      "train loss:0.5129180758541227\n",
      "train loss:0.2315865542221882\n",
      "train loss:0.6978996917256168\n",
      "train loss:0.2884294441928934\n",
      "train loss:0.32079072679949705\n",
      "train loss:0.1609687390199825\n",
      "train loss:0.5056195531202307\n",
      "train loss:0.12405318273684089\n",
      "train loss:0.4122825956736589\n",
      "train loss:0.24332130936099902\n",
      "train loss:0.20355220164748408\n",
      "train loss:0.22666912080708754\n",
      "train loss:0.19744172730953458\n",
      "train loss:0.24404731327833268\n",
      "train loss:0.10902023635526405\n",
      "train loss:0.3901888025555701\n",
      "train loss:0.3352788837151173\n",
      "train loss:0.2791314554282518\n",
      "train loss:0.08202268664284329\n",
      "train loss:0.11152607607221585\n",
      "train loss:0.6459698621447597\n",
      "train loss:0.20940757294243922\n",
      "train loss:0.2013696569702488\n",
      "train loss:0.3754305742357603\n",
      "train loss:0.2885094698341895\n",
      "train loss:0.13440706142801429\n",
      "train loss:0.44702495190167124\n",
      "train loss:0.24201671741608907\n",
      "train loss:0.35086518268029854\n",
      "train loss:0.567743385977879\n",
      "train loss:0.2637810487126439\n",
      "train loss:0.24495113034106636\n",
      "train loss:0.2922736830851009\n",
      "train loss:0.2108048191526335\n",
      "train loss:0.23492530884801516\n",
      "train loss:0.11876365564238361\n",
      "train loss:0.1466460816655344\n",
      "train loss:0.6854566007371765\n",
      "train loss:0.4009957165840735\n",
      "train loss:0.19111296665955912\n",
      "train loss:0.5248778269676021\n",
      "train loss:0.2559590439834439\n",
      "train loss:0.23636167977419723\n",
      "train loss:0.21015691644302525\n",
      "train loss:0.29693393470692075\n",
      "train loss:0.31171653911544717\n",
      "train loss:0.0882688601151802\n",
      "train loss:0.14284693302277565\n",
      "train loss:0.23990667558278478\n",
      "train loss:0.2291799543220347\n",
      "train loss:0.20925112572294335\n",
      "train loss:0.1871925954189258\n",
      "train loss:0.23898798957816553\n",
      "train loss:0.10342750068306372\n",
      "train loss:0.11396438207972041\n",
      "train loss:0.2690007585162034\n",
      "train loss:0.15079171420577858\n",
      "train loss:0.22254134187080832\n",
      "train loss:0.16005305210094561\n",
      "train loss:0.30460975776609783\n",
      "train loss:0.22483739129293326\n",
      "train loss:0.43433568253491184\n",
      "train loss:0.4072276069074864\n",
      "train loss:0.19049676857897793\n",
      "train loss:0.5969807314875886\n",
      "train loss:0.10405619823423122\n",
      "train loss:0.2631823835745827\n",
      "train loss:0.11967551236280541\n",
      "train loss:0.23969332751552924\n",
      "train loss:0.21530294609669867\n",
      "train loss:0.3314993890757949\n",
      "train loss:0.23766328372502593\n",
      "train loss:0.36150199142435097\n",
      "train loss:0.6401124783427382\n",
      "train loss:0.08536243908473012\n",
      "train loss:0.2604747259574942\n",
      "train loss:0.34403843837897224\n",
      "train loss:0.111377823024492\n",
      "train loss:0.3603652035498284\n",
      "train loss:0.13030003009927932\n",
      "train loss:0.2581716746817597\n",
      "train loss:0.4771491748545713\n",
      "train loss:0.14514488476982051\n",
      "train loss:0.19470749788963482\n",
      "train loss:0.08455265658605053\n",
      "train loss:0.4365483973901755\n",
      "train loss:0.3302078200341327\n",
      "train loss:0.30443366569291225\n",
      "train loss:0.1743448081429725\n",
      "train loss:0.3608170980639074\n",
      "train loss:0.31084036216289423\n",
      "train loss:0.11496566032370382\n",
      "train loss:0.23783905562562865\n",
      "train loss:0.1739155264072349\n",
      "train loss:0.21968440460166533\n",
      "train loss:0.17850212358382567\n",
      "train loss:0.26892372217498556\n",
      "train loss:0.35869807948469945\n",
      "train loss:0.2278126970838126\n",
      "train loss:0.43342832763207306\n",
      "train loss:0.6489784428938509\n",
      "train loss:0.6086430225198879\n",
      "train loss:0.15083817000173244\n",
      "train loss:0.27993971351949815\n",
      "train loss:0.23609404281325416\n",
      "train loss:0.30726634154447646\n",
      "train loss:0.14320890042782003\n",
      "train loss:0.15126111637674927\n",
      "train loss:0.10020420424419688\n",
      "train loss:0.6831417330891636\n",
      "train loss:0.2639921423153527\n",
      "train loss:0.32868563682080854\n",
      "train loss:0.29645557642800446\n",
      "train loss:0.25365225610489056\n",
      "train loss:0.1738476753217968\n",
      "train loss:0.41764007278074583\n",
      "train loss:0.3618878340582682\n",
      "train loss:0.11678046474491417\n",
      "train loss:0.18928493454445544\n",
      "train loss:0.3779974722380598\n",
      "train loss:0.47802086440300545\n",
      "train loss:0.3588230630413489\n",
      "train loss:0.19570423631896355\n",
      "train loss:0.09826180312007624\n",
      "train loss:0.17533292138665119\n",
      "train loss:0.9632793808176108\n",
      "train loss:0.5086871220685028\n",
      "train loss:0.21085583581692954\n",
      "train loss:0.38873731340660345\n",
      "train loss:0.5007386626876132\n",
      "train loss:0.36207129213524786\n",
      "train loss:0.15074589545536987\n",
      "train loss:0.17856740530191934\n",
      "train loss:0.48056697757328093\n",
      "train loss:0.10947857274047584\n",
      "train loss:0.1798513893053392\n",
      "train loss:0.15980995839280254\n",
      "train loss:0.15432650591220937\n",
      "train loss:0.2535910954981391\n",
      "train loss:0.11903392129375298\n",
      "train loss:0.45699994472686817\n",
      "train loss:0.17433921762010457\n",
      "train loss:0.23119539326270286\n",
      "train loss:0.07762102112844319\n",
      "train loss:0.393607315335525\n",
      "train loss:0.28784894683090434\n",
      "train loss:0.17850865412388175\n",
      "train loss:0.3175954249909368\n",
      "train loss:0.13529182620836044\n",
      "train loss:0.7589193004965888\n",
      "train loss:0.41536866286221397\n",
      "train loss:0.24691276383542493\n",
      "train loss:0.1335054675571375\n",
      "train loss:0.03267421175238104\n",
      "train loss:0.06348204782750289\n",
      "train loss:0.3198451262633826\n",
      "train loss:0.6225428246729398\n",
      "train loss:0.1886604894596803\n",
      "train loss:0.3178007172406619\n",
      "train loss:0.2830349192870758\n",
      "train loss:0.2331455669117267\n",
      "train loss:0.29243896566861927\n",
      "train loss:0.12087371740321667\n",
      "train loss:0.212458208596137\n",
      "train loss:0.4471991263703397\n",
      "train loss:0.1322289289405946\n",
      "train loss:0.4457189719531578\n",
      "train loss:0.32790285643375616\n",
      "train loss:0.2676635534791083\n",
      "train loss:0.3462165452521916\n",
      "train loss:0.22620853999268362\n",
      "train loss:0.4666518282782957\n",
      "train loss:0.1791507415825929\n",
      "train loss:0.31022431243893617\n",
      "train loss:0.43152239339255094\n",
      "train loss:0.1753884563459514\n",
      "train loss:0.1997884608867282\n",
      "train loss:0.3017366753813775\n",
      "train loss:0.2308196635340306\n",
      "train loss:0.5549694252945018\n",
      "train loss:0.2849013545722522\n",
      "train loss:0.17076599845568013\n",
      "train loss:0.0845080460581325\n",
      "train loss:0.1159744876162718\n",
      "train loss:0.40280027212386504\n",
      "train loss:0.2879246566837551\n",
      "train loss:0.1947072821215878\n",
      "train loss:0.16801679469453681\n",
      "train loss:0.05937762772248607\n",
      "train loss:0.38316582029907337\n",
      "train loss:0.19657929679258224\n",
      "train loss:0.24883392891532474\n",
      "train loss:0.1691036793526196\n",
      "train loss:0.08602698988879229\n",
      "train loss:0.1277267691846186\n",
      "train loss:0.1300030455686148\n",
      "train loss:0.30270020921966134\n",
      "train loss:0.37350172229392375\n",
      "train loss:0.5077912356159344\n",
      "train loss:0.3743447773134078\n",
      "train loss:0.3125770614193417\n",
      "train loss:0.34560155755051286\n",
      "train loss:0.1616611529214128\n",
      "train loss:0.26825354095966836\n",
      "train loss:0.25086547659222785\n",
      "train loss:0.40044841824718247\n",
      "train loss:0.26306154674711757\n",
      "train loss:0.5940893158662536\n",
      "train loss:0.11037286942096348\n",
      "train loss:0.21316092730347277\n",
      "train loss:0.09423476563391997\n",
      "train loss:0.3550378115704431\n",
      "train loss:0.6207740809124457\n",
      "train loss:0.49936679199309214\n",
      "train loss:0.23948492700951435\n",
      "train loss:0.07802104006497025\n",
      "train loss:0.3467572989327252\n",
      "train loss:0.12997440662735127\n",
      "train loss:0.16584167487275625\n",
      "train loss:0.5478204382335234\n",
      "train loss:0.2439028792343127\n",
      "train loss:0.22230046877373877\n",
      "train loss:0.27518573566029647\n",
      "train loss:0.2062285600639714\n",
      "train loss:0.3364047674872641\n",
      "train loss:0.21563366396717693\n",
      "train loss:0.22433214839793053\n",
      "train loss:0.10989523209714608\n",
      "train loss:0.23768294575182541\n",
      "train loss:0.3504028228724643\n",
      "train loss:0.2043592417701331\n",
      "train loss:0.13642755033958753\n",
      "train loss:0.16096997866430024\n",
      "train loss:0.20879295641497372\n",
      "train loss:0.34944689018699016\n",
      "train loss:0.2386174568837501\n",
      "train loss:0.44301687909258997\n",
      "train loss:0.17793460667441421\n",
      "train loss:0.17245401790591175\n",
      "train loss:0.15928921328316023\n",
      "train loss:0.14271549572467962\n",
      "train loss:0.2790277225154972\n",
      "train loss:0.342683013642628\n",
      "train loss:0.7327921917579787\n",
      "train loss:0.19017467056966073\n",
      "train loss:0.4426249864655907\n",
      "train loss:0.350926838859435\n",
      "train loss:0.5719206652351085\n",
      "train loss:0.24877786534036855\n",
      "train loss:0.3413663763770658\n",
      "train loss:0.15556808399796834\n",
      "train loss:0.17979712609776588\n",
      "train loss:0.2023907495025501\n",
      "train loss:0.11170907353929639\n",
      "train loss:0.34152461412347135\n",
      "train loss:0.23805748967213866\n",
      "train loss:0.2983648155116171\n",
      "train loss:0.4258521868911542\n",
      "train loss:0.1675874405524988\n",
      "train loss:0.12540893709162076\n",
      "train loss:0.2966241808115278\n",
      "train loss:0.22499691049543363\n",
      "train loss:0.5538303044042969\n",
      "train loss:0.07229310433700437\n",
      "train loss:0.20595386901033041\n",
      "train loss:0.2414487463557416\n",
      "train loss:0.08868157274473105\n",
      "train loss:0.33525828355197707\n",
      "train loss:0.277446513666834\n",
      "train loss:0.10728448645594593\n",
      "train loss:0.35642825299154424\n",
      "train loss:0.2977186009473225\n",
      "train loss:0.09720318931241667\n",
      "train loss:0.3501391211921292\n",
      "train loss:0.24767641178023952\n",
      "train loss:0.14577816843651173\n",
      "train loss:0.12140407842015358\n",
      "train loss:0.10871422899178922\n",
      "train loss:0.6340058761770614\n",
      "train loss:0.1474334642088606\n",
      "train loss:0.12176557711491744\n",
      "train loss:0.16503350729616856\n",
      "train loss:0.3489076227225808\n",
      "train loss:0.2449698470046243\n",
      "train loss:0.8981172115840743\n",
      "train loss:0.1501282541821914\n",
      "train loss:0.25740465904173976\n",
      "train loss:0.10011439281128462\n",
      "train loss:0.08004921268490361\n",
      "train loss:0.771720606669478\n",
      "train loss:0.2838255175411821\n",
      "train loss:0.11398521073333065\n",
      "train loss:0.14348617709861639\n",
      "train loss:0.2394659043233236\n",
      "train loss:0.33265011144304263\n",
      "train loss:0.39372615612929746\n",
      "train loss:0.3692130336412447\n",
      "train loss:0.371368064288171\n",
      "train loss:0.1415813316444894\n",
      "train loss:0.33805635643388776\n",
      "train loss:0.3895805140566605\n",
      "train loss:0.6168451852568488\n",
      "train loss:0.13906970291215637\n",
      "train loss:0.38704257197786024\n",
      "train loss:0.1867999694859524\n",
      "train loss:0.1880290463384254\n",
      "train loss:0.07544904826261614\n",
      "train loss:0.697846096252493\n",
      "train loss:0.37530570618477554\n",
      "train loss:0.1217092213700269\n",
      "train loss:0.2594192511258836\n",
      "train loss:0.11653224006133193\n",
      "train loss:0.11373920670813395\n",
      "train loss:0.11665073892664403\n",
      "train loss:0.2529036046962827\n",
      "train loss:0.32944310040798575\n",
      "train loss:0.10965908109273001\n",
      "train loss:0.12745359365296616\n",
      "train loss:0.36196612473369\n",
      "train loss:0.17596901161620873\n",
      "train loss:0.19783679230787898\n",
      "train loss:0.18065700833140272\n",
      "train loss:0.08065102346408089\n",
      "train loss:0.26937537305414994\n",
      "train loss:0.09834432266919421\n",
      "train loss:0.43452791397891055\n",
      "train loss:0.22996080861190846\n",
      "train loss:0.3666920422897732\n",
      "train loss:0.1979662902460453\n",
      "train loss:0.1434698669162947\n",
      "train loss:0.17898533243650266\n",
      "train loss:0.08648957219883657\n",
      "train loss:0.25944925811169556\n",
      "train loss:0.11112528901296101\n",
      "train loss:0.2526850696301951\n",
      "train loss:0.20163883410289996\n",
      "train loss:0.4046248295037805\n",
      "train loss:0.18086901328243174\n",
      "train loss:0.3664294259481755\n",
      "train loss:0.16205938181466978\n",
      "train loss:0.2611196676543651\n",
      "train loss:0.4996179452647773\n",
      "train loss:0.20344929944248302\n",
      "train loss:0.12954230669003286\n",
      "train loss:0.0843565715720199\n",
      "train loss:0.4622905224570276\n",
      "train loss:0.24862993345026282\n",
      "train loss:0.14768507280505544\n",
      "train loss:0.09679452492022508\n",
      "train loss:0.6165796103202685\n",
      "train loss:0.5986548333605766\n",
      "train loss:0.5223281273760324\n",
      "train loss:0.24699463608901986\n",
      "train loss:0.23722713942555482\n",
      "train loss:0.13147814279344733\n",
      "train loss:0.3537924075437047\n",
      "train loss:0.18142081048374825\n",
      "train loss:0.15847678410044658\n",
      "train loss:0.20162583893771852\n",
      "train loss:0.21707192784857998\n",
      "train loss:0.4432005381431515\n",
      "train loss:0.38510294769666803\n",
      "train loss:0.21317999988880454\n",
      "train loss:0.2625187348630968\n",
      "train loss:0.16066188547826527\n",
      "train loss:0.3432198956886444\n",
      "train loss:0.16312980462814503\n",
      "train loss:0.5689316130212118\n",
      "train loss:0.30736985929692123\n",
      "train loss:0.30369727484910664\n",
      "train loss:0.04631878261389501\n",
      "train loss:0.17938715290863008\n",
      "train loss:0.10913490754888322\n",
      "train loss:0.12993381581683752\n",
      "train loss:0.14347945607482582\n",
      "train loss:0.16193424785125166\n",
      "train loss:0.39996809621820795\n",
      "train loss:0.20627754221832112\n",
      "train loss:0.12268557025688094\n",
      "train loss:0.312785973606162\n",
      "train loss:0.4200124348722777\n",
      "train loss:0.18160973761799312\n",
      "train loss:0.3123314363755783\n",
      "train loss:0.5000442334354545\n",
      "train loss:0.21701728760247777\n",
      "train loss:0.26979053677715537\n",
      "train loss:0.2515256787129895\n",
      "train loss:0.06310797423594276\n",
      "train loss:0.19742291480429586\n",
      "train loss:0.18931564665868428\n",
      "train loss:0.06816574348601159\n",
      "train loss:0.21700619490878217\n",
      "train loss:0.2133400198431355\n",
      "train loss:0.10116917085711294\n",
      "train loss:0.2823454331613838\n",
      "train loss:0.29222990169194346\n",
      "train loss:0.23437201580993086\n",
      "train loss:0.1785098892849141\n",
      "train loss:0.07323018626600389\n",
      "train loss:0.1258053139496104\n",
      "train loss:0.18250971052540815\n",
      "train loss:0.25231826928761986\n",
      "train loss:0.09080799405442899\n",
      "train loss:0.26781412052437825\n",
      "train loss:0.7442188647671863\n",
      "train loss:0.33511877653007965\n",
      "train loss:0.34735018980673943\n",
      "train loss:0.40744801729558217\n",
      "train loss:0.1298645626682691\n",
      "train loss:0.25671122493373\n",
      "train loss:0.3110839665600416\n",
      "train loss:0.09204759892389316\n",
      "train loss:0.48414113138074666\n",
      "train loss:0.18385341633420071\n",
      "train loss:0.16497629995991409\n",
      "train loss:0.12388255917637792\n",
      "train loss:0.1389654251434525\n",
      "train loss:0.26711448985589725\n",
      "train loss:0.16629470447827618\n",
      "train loss:0.6468482662850836\n",
      "train loss:0.31138308297437994\n",
      "train loss:0.2312404737672028\n",
      "train loss:0.049941424318894416\n",
      "train loss:0.40573617885571533\n",
      "train loss:0.16317772126983993\n",
      "train loss:0.3400089153540832\n",
      "train loss:0.6243231466854782\n",
      "train loss:0.16097261447287117\n",
      "train loss:0.10188305147016485\n",
      "train loss:0.758036602105334\n",
      "train loss:0.1702244839744871\n",
      "train loss:0.2902082722982215\n",
      "train loss:0.03300884332227149\n",
      "train loss:0.22870527172866698\n",
      "train loss:0.2767227998324745\n",
      "train loss:0.21821588550937332\n",
      "train loss:0.5019193756151489\n",
      "train loss:0.04562835120499109\n",
      "train loss:0.18174459875911986\n",
      "train loss:0.1985396098303137\n",
      "train loss:0.43170576994578597\n",
      "train loss:0.26297060094705665\n",
      "train loss:0.23700794117816165\n",
      "train loss:0.23536735710038023\n",
      "train loss:0.2660892092327337\n",
      "train loss:0.37778659398544334\n",
      "train loss:0.15062125745754196\n",
      "train loss:0.22567751211792625\n",
      "train loss:0.5115885715658388\n",
      "train loss:0.48734966678696423\n",
      "train loss:0.22343388611734383\n",
      "train loss:0.31973849433608387\n",
      "train loss:0.23395700547089784\n",
      "train loss:0.4166572788856701\n",
      "train loss:0.20245069123136766\n",
      "train loss:0.05370556628421691\n",
      "train loss:0.27665094605626994\n",
      "train loss:0.165667037411996\n",
      "train loss:0.30047391475364915\n",
      "train loss:0.11004950623965949\n",
      "train loss:0.11516670359700115\n",
      "train loss:0.1220240237995172\n",
      "train loss:0.361488768436359\n",
      "train loss:0.1349759211036259\n",
      "train loss:0.29860993096872746\n",
      "train loss:0.6118141657154073\n",
      "train loss:0.23349168703150003\n",
      "train loss:0.14994230993600144\n",
      "train loss:0.22070157753267033\n",
      "train loss:0.14763012809909387\n",
      "train loss:0.41637731327061267\n",
      "train loss:0.1437213774430674\n",
      "train loss:0.2607847426668806\n",
      "train loss:0.14527552829320928\n",
      "train loss:0.3690240983813322\n",
      "train loss:0.07373350150456441\n",
      "train loss:0.21797785288928434\n",
      "train loss:0.677859691457086\n",
      "train loss:0.25144563696165007\n",
      "train loss:0.1950719121000589\n",
      "train loss:0.08746453062981963\n",
      "train loss:0.19107786961103806\n",
      "train loss:0.14333909338375247\n",
      "train loss:0.15051342039200302\n",
      "train loss:0.1088201908936506\n",
      "train loss:0.38708026161304876\n",
      "train loss:0.23119603268536315\n",
      "train loss:0.24610705812266295\n",
      "train loss:0.16302274244015985\n",
      "train loss:0.09372575909453157\n",
      "train loss:0.23860123953141027\n",
      "train loss:0.27209098292773337\n",
      "train loss:0.10122061005957836\n",
      "train loss:0.12830477047388883\n",
      "train loss:0.2083978224380693\n",
      "train loss:0.17060645118157775\n",
      "train loss:0.08099390021433486\n",
      "train loss:0.2849000192473813\n",
      "train loss:0.3475243838252102\n",
      "train loss:0.17201094601157596\n",
      "train loss:0.34475583848441055\n",
      "train loss:0.13707196964142607\n",
      "train loss:0.09998445975637586\n",
      "train loss:0.8668290679654053\n",
      "train loss:0.469135766861408\n",
      "train loss:0.12859848954382821\n",
      "train loss:0.10621125030241352\n",
      "train loss:0.13514529007491066\n",
      "train loss:0.1576020812109027\n",
      "train loss:0.3682039329329463\n",
      "train loss:0.2545053757362595\n",
      "train loss:0.1803896417479078\n",
      "train loss:0.06049635108032009\n",
      "train loss:0.4322554654668306\n",
      "train loss:0.38054388863612015\n",
      "train loss:0.13303976160555975\n",
      "train loss:0.18351622453357727\n",
      "train loss:0.3179645465604616\n",
      "train loss:0.22788772484051759\n",
      "train loss:0.09513323280736548\n",
      "train loss:0.39328296633421866\n",
      "train loss:0.13468920087497954\n",
      "train loss:0.42163110781176566\n",
      "train loss:0.3780830959704832\n",
      "train loss:0.06477513950264371\n",
      "train loss:0.10352714543430662\n",
      "train loss:0.07358096336293254\n",
      "train loss:0.15497108653858058\n",
      "train loss:0.30541975121177256\n",
      "train loss:0.20416057742838412\n",
      "train loss:0.19317836054098514\n",
      "train loss:0.25411130633640466\n",
      "train loss:0.15932658191718863\n",
      "train loss:0.7053462045982336\n",
      "train loss:0.616094916135105\n",
      "train loss:0.2604422023532461\n",
      "train loss:0.12661017485288953\n",
      "train loss:0.214620863815549\n",
      "train loss:0.10378866090416573\n",
      "train loss:0.1055860869749928\n",
      "train loss:0.21505149541522806\n",
      "train loss:0.5675773368104153\n",
      "train loss:0.2934621914121674\n",
      "train loss:0.20710458863706221\n",
      "train loss:0.11111273110644967\n",
      "train loss:0.1410272126573045\n",
      "train loss:0.4166994116850981\n",
      "train loss:0.41745583523413154\n",
      "train loss:0.21577955909559238\n",
      "train loss:0.28035346645253756\n",
      "train loss:0.3651427407903798\n",
      "train loss:0.07344965216945139\n",
      "train loss:0.11261537749376169\n",
      "train loss:0.306327653173885\n",
      "train loss:0.14738342933583054\n",
      "train loss:0.18713881443432148\n",
      "train loss:0.658889064977436\n",
      "train loss:0.5260227722884694\n",
      "train loss:0.22330858340697285\n",
      "train loss:0.7419870497991281\n",
      "train loss:0.25619341084620584\n",
      "train loss:0.3319201347242181\n",
      "train loss:0.1841888210227131\n",
      "train loss:0.3804115912780762\n",
      "train loss:0.31361018538942653\n",
      "train loss:0.08305194182222761\n",
      "train loss:0.13654462646809473\n",
      "train loss:0.3894694258871276\n",
      "train loss:0.2775188054844263\n",
      "train loss:0.1246583768192902\n",
      "train loss:0.4612394023076864\n",
      "train loss:0.1840889397509733\n",
      "train loss:0.45847216917055295\n",
      "train loss:0.25770859692172854\n",
      "train loss:0.06289418900511017\n",
      "train loss:0.06312153563529996\n",
      "train loss:0.2902280319190862\n",
      "train loss:0.22500544438923012\n",
      "train loss:0.47924167832705405\n",
      "train loss:0.17069065426127827\n",
      "train loss:0.12533708412883438\n",
      "train loss:0.17718176050524692\n",
      "train loss:0.24021487262496535\n",
      "train loss:0.13139250466169064\n",
      "train loss:0.34044840894227185\n",
      "train loss:0.16462756665551254\n",
      "train loss:0.061329194573755616\n",
      "train loss:0.34926336773618377\n",
      "train loss:0.6167533381490977\n",
      "train loss:0.13401178920662726\n",
      "train loss:0.35041317579844916\n",
      "train loss:0.1489084943473067\n",
      "train loss:0.1410111497737909\n",
      "train loss:0.9405125483166903\n",
      "train loss:0.40061220260346264\n",
      "train loss:0.12057491484172118\n",
      "train loss:0.45887649158947397\n",
      "train loss:0.36843444516807494\n",
      "train loss:0.3667442662598379\n",
      "train loss:0.1697538976147035\n",
      "train loss:0.31170919261682695\n",
      "train loss:0.17395657947986012\n",
      "train loss:0.12819920709794355\n",
      "train loss:0.21970402037208003\n",
      "train loss:0.5382473320887637\n",
      "train loss:0.22365148388735764\n",
      "train loss:0.16076018949473403\n",
      "train loss:0.17520279666464855\n",
      "train loss:0.3283135513685219\n",
      "train loss:0.41346491682395065\n",
      "train loss:0.16582277602100415\n",
      "train loss:0.1282112428055855\n",
      "train loss:0.31909479312017375\n",
      "train loss:0.1871699268805715\n",
      "train loss:0.40528433503154926\n",
      "train loss:0.4961366761862715\n",
      "train loss:0.26339051595730084\n",
      "train loss:0.17734566952676917\n",
      "train loss:0.4806979313482056\n",
      "train loss:0.6476668179189052\n",
      "train loss:0.3808950078958992\n",
      "train loss:0.17694859969563925\n",
      "train loss:0.196922032500302\n",
      "train loss:0.14146623030491956\n",
      "train loss:0.24861024049458386\n",
      "train loss:0.23002516530635458\n",
      "train loss:0.09060010661646714\n",
      "train loss:0.23420699872119438\n",
      "train loss:0.1980555615082874\n",
      "train loss:0.09772963593247701\n",
      "train loss:0.15209678896260836\n",
      "train loss:0.1165444055992233\n",
      "train loss:0.11253433979453106\n",
      "train loss:0.43192902586532667\n",
      "train loss:0.14874776949817425\n",
      "train loss:0.16624992350396872\n",
      "train loss:0.13864351234438982\n",
      "train loss:0.13811214403878352\n",
      "train loss:0.35409162142696754\n",
      "train loss:0.8894750990237507\n",
      "train loss:0.08763068761349715\n",
      "train loss:0.2902874231500356\n",
      "train loss:0.1814554113231367\n",
      "train loss:0.6558292657498936\n",
      "train loss:0.1452520443549492\n",
      "train loss:0.12048020681966792\n",
      "train loss:0.11803787470755134\n",
      "train loss:0.30693449317485155\n",
      "train loss:0.11856708843938493\n",
      "train loss:0.091463078806373\n",
      "train loss:0.03343300533035026\n",
      "train loss:0.48464119025245755\n",
      "train loss:0.20073121166128638\n",
      "train loss:0.255944489813236\n",
      "train loss:0.16145465182406152\n",
      "train loss:0.4578004781624294\n",
      "train loss:0.16428436774799793\n",
      "train loss:0.13045994983935397\n",
      "train loss:0.08122913270308586\n",
      "train loss:0.5565283124223475\n",
      "train loss:0.03902096892388579\n",
      "train loss:0.2213850324600986\n",
      "train loss:0.08830069420373687\n",
      "train loss:0.44278550520033877\n",
      "train loss:0.30265933596767997\n",
      "train loss:0.3441728204000838\n",
      "train loss:0.2010932199495375\n",
      "train loss:0.29589325815677286\n",
      "train loss:0.20413464619741906\n",
      "train loss:0.5074551567318203\n",
      "train loss:0.14253486775546337\n",
      "train loss:0.1253054239932977\n",
      "train loss:0.23689314290294866\n",
      "train loss:0.44760370197687577\n",
      "train loss:0.44623612554379494\n",
      "train loss:0.3496115470010713\n",
      "train loss:0.9732371662032614\n",
      "train loss:0.1611974307885956\n",
      "train loss:0.45373818208228256\n",
      "train loss:0.1775239865772778\n",
      "train loss:0.23488349363585848\n",
      "train loss:0.4735186499495764\n",
      "train loss:0.47514081880754555\n",
      "train loss:0.7317038468806554\n",
      "train loss:0.5101136250791461\n",
      "train loss:0.43074958493001536\n",
      "train loss:0.07549316210382762\n",
      "train loss:0.4314541007913053\n",
      "train loss:0.3232522996540067\n",
      "train loss:0.04305175691859944\n",
      "train loss:0.21169674789931564\n",
      "train loss:0.32033815962407775\n",
      "train loss:0.20967829878928668\n",
      "train loss:0.14202138322781488\n",
      "train loss:0.11278594604809752\n",
      "train loss:0.08583952599619887\n",
      "train loss:0.12084091958046997\n",
      "train loss:0.40499270569548707\n",
      "train loss:0.1849643162776009\n",
      "train loss:0.3164331644268894\n",
      "train loss:0.4611252096035378\n",
      "train loss:0.18922119197414936\n",
      "train loss:0.15083769828591867\n",
      "train loss:0.11463047760754325\n",
      "train loss:0.24103753416365092\n",
      "train loss:0.11940645890999466\n",
      "train loss:0.1331289726843651\n",
      "train loss:0.16125101167960015\n",
      "train loss:0.08415305940529985\n",
      "train loss:0.06347579849103235\n",
      "train loss:0.2202960570668116\n",
      "train loss:0.36763563903034197\n",
      "train loss:0.13014705880977945\n",
      "train loss:0.3552927908173318\n",
      "train loss:0.30037435449770833\n",
      "train loss:0.5924097104205541\n",
      "train loss:0.0947881878770398\n",
      "train loss:0.3794428666501812\n",
      "train loss:0.07105600777735727\n",
      "train loss:0.5211826973741761\n",
      "train loss:0.42988609951686674\n",
      "train loss:0.39768097861679574\n",
      "train loss:0.09421482510512255\n",
      "train loss:0.1263117118062275\n",
      "train loss:0.1092486784923691\n",
      "train loss:0.18206600641184928\n",
      "train loss:0.0899386431286803\n",
      "train loss:0.08047962443133118\n",
      "train loss:0.47687264039851046\n",
      "train loss:0.48485110709006873\n",
      "train loss:0.27132940915992904\n",
      "train loss:0.1514355410850048\n",
      "train loss:0.1607839686871841\n",
      "train loss:0.3687723839394971\n",
      "train loss:0.692808363890012\n",
      "train loss:0.24178337421738438\n",
      "train loss:0.1475030372990692\n",
      "train loss:0.26284904224899075\n",
      "train loss:0.06363603575274636\n",
      "train loss:0.13268569701755592\n",
      "train loss:0.24512946589101944\n",
      "train loss:0.2141927814463882\n",
      "train loss:0.09227301727853127\n",
      "train loss:0.31043406307835625\n",
      "train loss:0.5230970881339101\n",
      "train loss:0.17708404599204453\n",
      "train loss:0.22704275136728896\n",
      "train loss:0.14251732676402276\n",
      "train loss:0.19662846309361864\n",
      "train loss:0.4484873014025984\n",
      "train loss:0.5166606896401814\n",
      "train loss:0.17517157982744175\n",
      "train loss:0.12063776491913492\n",
      "train loss:0.7645777637572377\n",
      "train loss:0.2874400332491276\n",
      "train loss:0.15659444050576662\n",
      "train loss:0.2933987734188717\n",
      "train loss:0.058011824408196695\n",
      "train loss:0.11802891206004076\n",
      "train loss:0.2733945130288693\n",
      "train loss:0.21991725981734425\n",
      "train loss:0.300593710928003\n",
      "train loss:0.20520438843794286\n",
      "train loss:0.3092204380235459\n",
      "train loss:0.11517910653015384\n",
      "train loss:0.06576564059084519\n",
      "train loss:0.09231044713208247\n",
      "train loss:0.15979965486133763\n",
      "train loss:0.14854674088929437\n",
      "train loss:0.25192605636898996\n",
      "train loss:0.0950282473666762\n",
      "train loss:0.5201201522175888\n",
      "train loss:0.30990991428387843\n",
      "train loss:0.19767410340511085\n",
      "train loss:0.3041641123144899\n",
      "train loss:0.09958097439013527\n",
      "train loss:0.47973325108526554\n",
      "train loss:0.12421372038131458\n",
      "train loss:0.16338665523503712\n",
      "train loss:0.2568747430831376\n",
      "train loss:0.10624156878221927\n",
      "train loss:0.17327445546659956\n",
      "train loss:0.15205959909540606\n",
      "train loss:0.1721654812369504\n",
      "train loss:0.19655358266263\n",
      "train loss:0.19171657977434223\n",
      "train loss:0.7497995463358151\n",
      "train loss:0.2801676485622438\n",
      "train loss:0.11546535612947362\n",
      "train loss:0.20441055197338773\n",
      "train loss:0.3371969117921828\n",
      "train loss:0.1649617666884678\n",
      "train loss:0.06828778029166063\n",
      "train loss:0.29994519663709523\n",
      "train loss:0.273477129622938\n",
      "train loss:0.05822133492626734\n",
      "train loss:0.2530604483361111\n",
      "train loss:0.2171525440680477\n",
      "train loss:0.24378021067095096\n",
      "train loss:0.0941752611777606\n",
      "train loss:0.24001122137792325\n",
      "train loss:0.10623413478462373\n",
      "train loss:0.19883042128933595\n",
      "train loss:0.47963987014378984\n",
      "train loss:0.3416515303047729\n",
      "train loss:0.10395153030182638\n",
      "train loss:0.18576778099973432\n",
      "train loss:0.2330310855932749\n",
      "train loss:0.10464254879644622\n",
      "train loss:0.12752197739177484\n",
      "train loss:0.0842093669008531\n",
      "train loss:0.30005183054288564\n",
      "train loss:0.2150840910532712\n",
      "train loss:0.19687799527339284\n",
      "train loss:0.37590131975608676\n",
      "train loss:0.10491752443635763\n",
      "train loss:0.2440358606084758\n",
      "train loss:0.2984762438843336\n",
      "train loss:0.19577100944843714\n",
      "train loss:0.1592891279300645\n",
      "train loss:0.13318385344759026\n",
      "train loss:0.33123109851435706\n",
      "train loss:0.5287939379639279\n",
      "train loss:0.3419398485299809\n",
      "train loss:0.28924413263136156\n",
      "train loss:0.48596907093300906\n",
      "train loss:0.14672402900339984\n",
      "train loss:0.1308869705821299\n",
      "train loss:0.562565344194895\n",
      "train loss:0.26789993042592036\n",
      "train loss:0.09631981470202362\n",
      "train loss:0.10917791538017667\n",
      "train loss:0.09584722427077512\n",
      "train loss:0.10202740212031637\n",
      "train loss:0.3098117626828921\n",
      "train loss:0.11051158898745063\n",
      "train loss:0.7619025877069883\n",
      "train loss:0.15965418910346302\n",
      "train loss:0.4760171254722897\n",
      "train loss:0.29103701593084913\n",
      "train loss:0.1061861751630585\n",
      "train loss:0.15400985493209507\n",
      "train loss:0.19313180109884398\n",
      "train loss:0.24493360908615355\n",
      "train loss:0.7450847434186009\n",
      "train loss:0.44506200378520705\n",
      "=== epoch:2, train acc:0.876, test acc:0.887 ===\n",
      "train loss:0.22322467517762207\n",
      "train loss:0.03961170359241721\n",
      "train loss:0.23074108701522178\n",
      "train loss:0.17348165680886\n",
      "train loss:0.4973177681499215\n",
      "train loss:0.3041304852032491\n",
      "train loss:0.3464669727506971\n",
      "train loss:0.20523446570200482\n",
      "train loss:0.33134393818480956\n",
      "train loss:0.22650624455607726\n",
      "train loss:0.10254999254846678\n",
      "train loss:0.08232789601402124\n",
      "train loss:0.0700059589176531\n",
      "train loss:0.12774059592693465\n",
      "train loss:0.15498606360188869\n",
      "train loss:0.4553669999158309\n",
      "train loss:0.23746024204131816\n",
      "train loss:0.22386208723768394\n",
      "train loss:0.13826881023759335\n",
      "train loss:0.4204409571980119\n",
      "train loss:0.310867184812228\n",
      "train loss:0.07601204196462477\n",
      "train loss:0.23502987595026023\n",
      "train loss:0.08819608851514604\n",
      "train loss:0.09536102902049626\n",
      "train loss:0.18569059117011863\n",
      "train loss:0.12172682962889311\n",
      "train loss:0.43113402888584956\n",
      "train loss:0.10609118807442908\n",
      "train loss:0.1381162294972928\n",
      "train loss:0.06382135587920948\n",
      "train loss:0.13491265591489052\n",
      "train loss:0.5971194800367119\n",
      "train loss:0.10872322792778522\n",
      "train loss:0.05079713581184844\n",
      "train loss:0.04169905823700064\n",
      "train loss:0.30558803450200545\n",
      "train loss:0.3064606596098583\n",
      "train loss:0.3619566099756423\n",
      "train loss:0.09864901368198636\n",
      "train loss:0.19820904698666647\n",
      "train loss:0.3487691659327818\n",
      "train loss:0.4023970584424666\n",
      "train loss:0.2821322964304438\n",
      "train loss:0.12556659794638497\n",
      "train loss:1.032856467155596\n",
      "train loss:0.25102888539618434\n",
      "train loss:0.47958489819260847\n",
      "train loss:0.233943914042625\n",
      "train loss:0.17834269386251309\n",
      "train loss:0.4579600506817343\n",
      "train loss:0.19577190933636837\n",
      "train loss:0.11550207528731596\n",
      "train loss:0.2277763606367017\n",
      "train loss:0.36434254944976885\n",
      "train loss:0.23034148356294615\n",
      "train loss:0.09133768581384279\n",
      "train loss:0.17217612769465973\n",
      "train loss:0.20499111094833633\n",
      "train loss:0.07682224227684267\n",
      "train loss:0.1157439901840279\n",
      "train loss:0.26039146352073567\n",
      "train loss:0.11956624379087702\n",
      "train loss:0.5122751799202361\n",
      "train loss:0.0613422926459361\n",
      "train loss:0.17296665357030427\n",
      "train loss:0.08075075858770382\n",
      "train loss:0.08829710339081714\n",
      "train loss:0.0856890815443699\n",
      "train loss:0.12278221175369694\n",
      "train loss:0.39060397704982086\n",
      "train loss:0.5626001202161821\n",
      "train loss:0.10690229119762237\n",
      "train loss:0.24398748833254574\n",
      "train loss:0.18482995982419106\n",
      "train loss:0.16562417921102196\n",
      "train loss:0.24704163485573427\n",
      "train loss:0.37342944694573843\n",
      "train loss:0.24790787852380888\n",
      "train loss:0.7662622841922563\n",
      "train loss:0.1760851410314665\n",
      "train loss:0.4548095355999937\n",
      "train loss:0.2533182209253756\n",
      "train loss:0.1514806779403512\n",
      "train loss:0.06635526032477902\n",
      "train loss:0.11204766539270666\n",
      "train loss:0.06467023601594864\n",
      "train loss:0.15661702593685858\n",
      "train loss:0.10186260962075294\n",
      "train loss:0.09247150357488584\n",
      "train loss:0.05342539004785972\n",
      "train loss:0.32021625300907064\n",
      "train loss:0.16557736941725842\n",
      "train loss:0.10256027596065569\n",
      "train loss:0.13806205238530156\n",
      "train loss:0.08232766056303664\n",
      "train loss:0.353852575558243\n",
      "train loss:0.11201650922582189\n",
      "train loss:0.2547249307863275\n",
      "train loss:0.5132928003925201\n",
      "train loss:0.2677282387923011\n",
      "train loss:0.8994105251266773\n",
      "train loss:0.202160968284175\n",
      "train loss:0.23197369205244078\n",
      "train loss:0.07716855668995251\n",
      "train loss:0.1557463925191793\n",
      "train loss:0.19638691276035558\n",
      "train loss:0.1008940323418274\n",
      "train loss:0.13656406343838645\n",
      "train loss:0.20909602477486955\n",
      "train loss:0.07041851831966635\n",
      "train loss:0.5644555860984402\n",
      "train loss:0.09892696038904822\n",
      "train loss:0.2628412960989745\n",
      "train loss:0.1494752965334625\n",
      "train loss:0.1082507449412385\n",
      "train loss:0.0847184004778623\n",
      "train loss:0.31654830635593995\n",
      "train loss:0.4102694379775476\n",
      "train loss:0.21695724727541726\n",
      "train loss:0.34637867149292684\n",
      "train loss:0.07209960290778428\n",
      "train loss:0.3219225993721617\n",
      "train loss:0.06230468984027133\n",
      "train loss:0.033385020740392776\n",
      "train loss:0.06965894757377894\n",
      "train loss:0.06894493783899483\n",
      "train loss:0.1542769714611978\n",
      "train loss:0.14900931198756737\n",
      "train loss:0.2237000499873741\n",
      "train loss:0.5326200125423239\n",
      "train loss:0.352664720103106\n",
      "train loss:0.14744117917188712\n",
      "train loss:0.26364399726530496\n",
      "train loss:0.05207375167113655\n",
      "train loss:0.4135067090809299\n",
      "train loss:0.25591982237549643\n",
      "train loss:0.414193100705325\n",
      "train loss:0.07076938670172248\n",
      "train loss:0.13972561134702932\n",
      "train loss:0.1306324143937453\n",
      "train loss:0.12615376245184812\n",
      "train loss:0.2156721525413126\n",
      "train loss:0.2720400267079032\n",
      "train loss:0.18938849427105545\n",
      "train loss:0.07821490157990332\n",
      "train loss:0.07897541329818787\n",
      "train loss:0.21972596989952153\n",
      "train loss:0.23503174455601442\n",
      "train loss:0.15061768189486163\n",
      "train loss:0.06364208365185928\n",
      "train loss:0.11070288882827184\n",
      "train loss:0.576458875580554\n",
      "train loss:0.31905604011989397\n",
      "train loss:0.0842957986795707\n",
      "train loss:0.11623118976675978\n",
      "train loss:0.12417451784966393\n",
      "train loss:0.25869430549206474\n",
      "train loss:0.1323169223419896\n",
      "train loss:0.05658236437206075\n",
      "train loss:0.1367354914528197\n",
      "train loss:0.09066351863975655\n",
      "train loss:0.1556760983693189\n",
      "train loss:0.11537965892186464\n",
      "train loss:0.13463624205685257\n",
      "train loss:0.8402299000102806\n",
      "train loss:0.15774482540482987\n",
      "train loss:0.5948334276182179\n",
      "train loss:0.38883868049654885\n",
      "train loss:0.11922697693890172\n",
      "train loss:0.14135385571465836\n",
      "train loss:0.30352048596355674\n",
      "train loss:0.30574611631146764\n",
      "train loss:0.12426543198260748\n",
      "train loss:0.22228530657198237\n",
      "train loss:0.29524516100759685\n",
      "train loss:0.7665357142997097\n",
      "train loss:0.09740051894968837\n",
      "train loss:0.06476064175495228\n",
      "train loss:0.21973723559768293\n",
      "train loss:0.22633421493138112\n",
      "train loss:0.2844307815760564\n",
      "train loss:0.10268901542817571\n",
      "train loss:0.2905391814600257\n",
      "train loss:0.08978972607579588\n",
      "train loss:0.19586184862709466\n",
      "train loss:0.07934902927863033\n",
      "train loss:0.1273806317533403\n",
      "train loss:0.14771869916582778\n",
      "train loss:0.13253817475866408\n",
      "train loss:0.15104283334332116\n",
      "train loss:0.19932325912389742\n",
      "train loss:0.08182780206188293\n",
      "train loss:0.12907294203677755\n",
      "train loss:0.2681284523614261\n",
      "train loss:0.1549261776229151\n",
      "train loss:0.12439866950237508\n",
      "train loss:0.20159304345733045\n",
      "train loss:0.15539342046274307\n",
      "train loss:0.30927744984649647\n",
      "train loss:0.12281300141885569\n",
      "train loss:0.22689011116337732\n",
      "train loss:0.12427940002970506\n",
      "train loss:0.11658019533372188\n",
      "train loss:0.052671820428137096\n",
      "train loss:0.16417374350211922\n",
      "train loss:0.08863860636557563\n",
      "train loss:0.11510348487544056\n",
      "train loss:0.3867115119876249\n",
      "train loss:0.21103445853731873\n",
      "train loss:0.1819626311287177\n",
      "train loss:0.13554797673172936\n",
      "train loss:0.20469287691514762\n",
      "train loss:0.052929765777033747\n",
      "train loss:0.24828974659594966\n",
      "train loss:0.5806055571272298\n",
      "train loss:0.1710050287055128\n",
      "train loss:0.12290102024552949\n",
      "train loss:0.20026219890922148\n",
      "train loss:0.10362577093949192\n",
      "train loss:0.08608657575407183\n",
      "train loss:0.22466119331305695\n",
      "train loss:0.3538369448262442\n",
      "train loss:0.2210160375981566\n",
      "train loss:0.15856812046654323\n",
      "train loss:0.10781061833351427\n",
      "train loss:0.06937223237623202\n",
      "train loss:0.307479077141946\n",
      "train loss:0.6344391944766242\n",
      "train loss:0.290915972664644\n",
      "train loss:0.3496896673178821\n",
      "train loss:0.15368934030502332\n",
      "train loss:0.1739837043331512\n",
      "train loss:0.07612745583026274\n",
      "train loss:0.1379227646314947\n",
      "train loss:0.3439963084988305\n",
      "train loss:0.12875212914254602\n",
      "train loss:0.8218785409483153\n",
      "train loss:0.18593129894945856\n",
      "train loss:0.29665280259203197\n",
      "train loss:0.07409135209497497\n",
      "train loss:0.3277400192900316\n",
      "train loss:0.1286008118065809\n",
      "train loss:0.40117972786983275\n",
      "train loss:0.06960370004424059\n",
      "train loss:0.27010167476324115\n",
      "train loss:0.12036119368427824\n",
      "train loss:0.2321758140406703\n",
      "train loss:0.37891928854682816\n",
      "train loss:0.9106470099539197\n",
      "train loss:0.2117650840566883\n",
      "train loss:0.08142899210082974\n",
      "train loss:0.13288514513962013\n",
      "train loss:0.335055734470854\n",
      "train loss:0.11073630728789007\n",
      "train loss:0.5661218859010233\n",
      "train loss:0.2673154191386199\n",
      "train loss:0.10387189723573563\n",
      "train loss:0.2696290714929786\n",
      "train loss:0.053017125885629734\n",
      "train loss:0.17837594750284796\n",
      "train loss:0.2721776370988872\n",
      "train loss:0.2998452728544623\n",
      "train loss:0.49798601792611774\n",
      "train loss:0.3030178730055697\n",
      "train loss:0.11461163033859989\n",
      "train loss:0.20640226562109518\n",
      "train loss:0.3345597355907639\n",
      "train loss:0.33641676516385616\n",
      "train loss:0.2127001152504252\n",
      "train loss:0.11239564457925053\n",
      "train loss:0.5306260913088806\n",
      "train loss:0.17472913791697375\n",
      "train loss:0.3041410567998049\n",
      "train loss:0.2122121540676137\n",
      "train loss:0.2566171714748405\n",
      "train loss:0.1691843270264854\n",
      "train loss:0.11824096857708866\n",
      "train loss:0.1107641315796063\n",
      "train loss:0.05141758490322178\n",
      "train loss:0.12567526763913717\n",
      "train loss:0.2987674212457204\n",
      "train loss:0.14883391105528168\n",
      "train loss:0.06756024506049482\n",
      "train loss:0.22773105551245143\n",
      "train loss:0.4140722941063977\n",
      "train loss:0.7533536452256938\n",
      "train loss:0.2340013143026609\n",
      "train loss:0.598560730774394\n",
      "train loss:0.34557613737815424\n",
      "train loss:0.07661872300184827\n",
      "train loss:0.18105902246147432\n",
      "train loss:0.08859224329760992\n",
      "train loss:0.0900816873734311\n",
      "train loss:0.3148262338026414\n",
      "train loss:0.2758322439973758\n",
      "train loss:0.3629720424049181\n",
      "train loss:0.17897284299388533\n",
      "train loss:0.25599609783781846\n",
      "train loss:0.3608479623531339\n",
      "train loss:0.21483531586824622\n",
      "train loss:0.07217911394056659\n",
      "train loss:0.11102869147592245\n",
      "train loss:0.17634567810202167\n",
      "train loss:0.15081339440358527\n",
      "train loss:0.3671634763533283\n",
      "train loss:0.08500739808113264\n",
      "train loss:0.08845602301449489\n",
      "train loss:0.03160666491327794\n",
      "train loss:0.14450669270687588\n",
      "train loss:0.3071803673713793\n",
      "train loss:0.34859066313196935\n",
      "train loss:0.058895969703515724\n",
      "train loss:0.17690470174802125\n",
      "train loss:0.07248292933763334\n",
      "train loss:0.09365124871379588\n",
      "train loss:0.16331406833876758\n",
      "train loss:0.34834401172222684\n",
      "train loss:0.05860291264301737\n",
      "train loss:0.09704935163933792\n",
      "train loss:0.052179573463612794\n",
      "train loss:0.2369804250016067\n",
      "train loss:0.9224101923743827\n",
      "train loss:0.1083248245871135\n",
      "train loss:0.47928961985983615\n",
      "train loss:0.18495791238758896\n",
      "train loss:0.2446123253113163\n",
      "train loss:0.6007423542422513\n",
      "train loss:0.15573779232605361\n",
      "train loss:0.25166978325138006\n",
      "train loss:0.07644742062271279\n",
      "train loss:0.30972714043567473\n",
      "train loss:0.17244162586086875\n",
      "train loss:0.17220139492054184\n",
      "train loss:0.15574984598448788\n",
      "train loss:0.08601468235220458\n",
      "train loss:0.4206815533981296\n",
      "train loss:0.16202911997625066\n",
      "train loss:0.0460803831276868\n",
      "train loss:0.16676305753359882\n",
      "train loss:0.2539206054758649\n",
      "train loss:0.07944063913296984\n",
      "train loss:0.1761471191598032\n",
      "train loss:0.15322755627428852\n",
      "train loss:0.15899194022556834\n",
      "train loss:0.09083421575338244\n",
      "train loss:0.27667271753384587\n",
      "train loss:0.09631385602066685\n",
      "train loss:0.29428865806631044\n",
      "train loss:0.17747482105983325\n",
      "train loss:0.10435210904841649\n",
      "train loss:0.12943976650738087\n",
      "train loss:0.09510472770275352\n",
      "train loss:0.2613848159108743\n",
      "train loss:0.2215959094333203\n",
      "train loss:0.32272727485669245\n",
      "train loss:0.3898613069208788\n",
      "train loss:0.4263948093904772\n",
      "train loss:0.22881688702532596\n",
      "train loss:0.2875551274871404\n",
      "train loss:0.4938275674700786\n",
      "train loss:0.5116518552663889\n",
      "train loss:0.09299959402547463\n",
      "train loss:0.547322165743314\n",
      "train loss:0.24687298010006417\n",
      "train loss:0.08951678491862261\n",
      "train loss:0.09423562817096903\n",
      "train loss:0.29264039887792337\n",
      "train loss:0.085157241494837\n",
      "train loss:0.056318022175512863\n",
      "train loss:0.4321562924437611\n",
      "train loss:0.6183161937126097\n",
      "train loss:0.47018568160777374\n",
      "train loss:0.08804701273708665\n",
      "train loss:0.385371053755259\n",
      "train loss:0.13896303445914338\n",
      "train loss:0.15685588310102197\n",
      "train loss:0.7802512597682412\n",
      "train loss:0.20206234150769892\n",
      "train loss:0.15696661997025013\n",
      "train loss:0.11116295035170201\n",
      "train loss:0.3716825066138452\n",
      "train loss:0.26193418014359443\n",
      "train loss:0.12431656529469014\n",
      "train loss:0.11634960450828691\n",
      "train loss:0.13403417218644154\n",
      "train loss:0.09960051272650004\n",
      "train loss:0.42668475199973355\n",
      "train loss:0.27697927293069535\n",
      "train loss:0.1553068664094357\n",
      "train loss:0.3958906021139147\n",
      "train loss:0.0988384597974044\n",
      "train loss:0.07705574161408346\n",
      "train loss:0.44788743261863434\n",
      "train loss:0.21140233639375156\n",
      "train loss:0.16088582884431143\n",
      "train loss:0.0862277742750986\n",
      "train loss:0.30856701639134554\n",
      "train loss:0.36126895919898233\n",
      "train loss:0.30108198774731176\n",
      "train loss:0.17323750706416235\n",
      "train loss:0.15258438812149544\n",
      "train loss:0.3786196285135517\n",
      "train loss:0.313179520924873\n",
      "train loss:0.1951844430459098\n",
      "train loss:0.4027468656049734\n",
      "train loss:0.32228719818202944\n",
      "train loss:0.18144397746661864\n",
      "train loss:0.18949027319978506\n",
      "train loss:0.20008179306857363\n",
      "train loss:0.10157782371497975\n",
      "train loss:0.038621131593079464\n",
      "train loss:0.10810996665211663\n",
      "train loss:0.31686754861882005\n",
      "train loss:0.26647771107954443\n",
      "train loss:0.39126930055731085\n",
      "train loss:0.054809704389827865\n",
      "train loss:0.11916255481866045\n",
      "train loss:0.12088760007135181\n",
      "train loss:0.15552659059512525\n",
      "train loss:0.1791143096155071\n",
      "train loss:0.37502089116217546\n",
      "train loss:0.19358492719015866\n",
      "train loss:0.2143625391710708\n",
      "train loss:0.08839205539754859\n",
      "train loss:0.13830756027021288\n",
      "train loss:0.3167466363147132\n",
      "train loss:0.19256052344208308\n",
      "train loss:0.05157412751557584\n",
      "train loss:0.2841391399253784\n",
      "train loss:0.44501176613489235\n",
      "train loss:0.132518637927004\n",
      "train loss:0.33162780505553957\n",
      "train loss:1.1101297046542196\n",
      "train loss:0.17782035363400642\n",
      "train loss:0.2318766570940708\n",
      "train loss:0.728715593556907\n",
      "train loss:0.41829073877880807\n",
      "train loss:0.08860177829406778\n",
      "train loss:0.12707678403447592\n",
      "train loss:0.04180589277589081\n",
      "train loss:0.14371654646348705\n",
      "train loss:0.5752377970278023\n",
      "train loss:0.2805606380683078\n",
      "train loss:0.08711432708002537\n",
      "train loss:0.14355452840165034\n",
      "train loss:0.04220914966286768\n",
      "train loss:0.13245435490998222\n",
      "train loss:0.1581088054960117\n",
      "train loss:0.29158244775808595\n",
      "train loss:0.3545552925678366\n",
      "train loss:0.19908164411750756\n",
      "train loss:0.22312699548548962\n",
      "train loss:0.26214086972817063\n",
      "train loss:0.05129897157068484\n",
      "train loss:0.26931316617972795\n",
      "train loss:0.03579323982318221\n",
      "train loss:0.0849666239454539\n",
      "train loss:0.12485036194260862\n",
      "train loss:0.1225059976836719\n",
      "train loss:0.12452569089885013\n",
      "train loss:0.07241180073726881\n",
      "train loss:0.10844100611154842\n",
      "train loss:0.41982601232959404\n",
      "train loss:0.24226118503139965\n",
      "train loss:0.12149151164755984\n",
      "train loss:0.11884820813723672\n",
      "train loss:0.12408319646854636\n",
      "train loss:0.31200688749279076\n",
      "train loss:0.05338597065171512\n",
      "train loss:0.12087879798456103\n",
      "train loss:0.11984091307102282\n",
      "train loss:0.09474452036911399\n",
      "train loss:0.24429190062332629\n",
      "train loss:0.31074079255208836\n",
      "train loss:0.12073964434992461\n",
      "train loss:0.12083077039510576\n",
      "train loss:0.05681990735001616\n",
      "train loss:0.19391339551446835\n",
      "train loss:0.5140519260160743\n",
      "train loss:0.36411622393879733\n",
      "train loss:0.29754146879249566\n",
      "train loss:0.38340236539637307\n",
      "train loss:0.27672815601058354\n",
      "train loss:0.6946944908823737\n",
      "train loss:0.04143435774149963\n",
      "train loss:0.2454969150035268\n",
      "train loss:0.2184310142665555\n",
      "train loss:0.10589631099024088\n",
      "train loss:0.10894274968225817\n",
      "train loss:0.11230375567967077\n",
      "train loss:0.2219369586944269\n",
      "train loss:0.2792534780227256\n",
      "train loss:0.32157924120544296\n",
      "train loss:0.4777741485073677\n",
      "train loss:0.2368274194089831\n",
      "train loss:0.157841317830176\n",
      "train loss:0.2177596512968345\n",
      "train loss:0.09442168273611494\n",
      "train loss:0.16289532992310596\n",
      "train loss:0.47676386273010357\n",
      "train loss:0.04990874136960394\n",
      "train loss:0.38065984904898076\n",
      "train loss:0.6334897067215146\n",
      "train loss:0.2673343923263383\n",
      "train loss:0.40916770450062845\n",
      "train loss:0.21185760220576672\n",
      "train loss:0.21897696076715845\n",
      "train loss:0.8637715208483125\n",
      "train loss:0.5022792424623085\n",
      "train loss:0.09142393066192889\n",
      "train loss:0.16122000501813094\n",
      "train loss:0.2737573916256332\n",
      "train loss:0.0999872478896732\n",
      "train loss:0.21756085555110194\n",
      "train loss:0.05200313447806091\n",
      "train loss:0.11255902384627522\n",
      "train loss:0.06883436300609175\n",
      "train loss:0.25140205250535463\n",
      "train loss:0.29776665488212106\n",
      "train loss:0.1692808193886033\n",
      "train loss:0.10162413383095206\n",
      "train loss:0.09967998870727235\n",
      "train loss:0.1030713534631704\n",
      "train loss:0.09644605117832225\n",
      "train loss:0.06979659120679385\n",
      "train loss:0.08680184148645477\n",
      "train loss:0.08001408086368869\n",
      "train loss:0.15844183072362045\n",
      "train loss:0.19291221932758654\n",
      "train loss:0.17150098027788882\n",
      "train loss:0.23421372457087428\n",
      "train loss:0.15049008125864405\n",
      "train loss:0.16472012560175453\n",
      "train loss:0.32968565808951206\n",
      "train loss:0.18351563632195028\n",
      "train loss:0.23739800682161794\n",
      "train loss:0.2811101219116728\n",
      "train loss:0.547504866322482\n",
      "train loss:0.36551930523925874\n",
      "train loss:0.10358905116984732\n",
      "train loss:0.09237596795870515\n",
      "train loss:0.484321239376064\n",
      "train loss:0.1389139228835949\n",
      "train loss:0.6093338605055393\n",
      "train loss:0.22183327889276452\n",
      "train loss:0.5291466807254278\n",
      "train loss:0.3403541784198401\n",
      "train loss:0.3058425951629117\n",
      "train loss:0.18697300837744665\n",
      "train loss:0.025195528102773583\n",
      "train loss:0.03930221513397716\n",
      "train loss:0.21800354983015335\n",
      "train loss:0.27077240513592615\n",
      "train loss:0.6984360436320315\n",
      "train loss:0.8282892891211224\n",
      "train loss:0.3938276337105957\n",
      "train loss:0.19255054192396018\n",
      "train loss:0.2706133876812651\n",
      "train loss:0.06301323515439883\n",
      "train loss:0.1338129929868952\n",
      "train loss:0.7334933979411697\n",
      "train loss:0.3996083177262152\n",
      "train loss:0.15541888311509755\n",
      "train loss:0.15592853650968105\n",
      "train loss:0.4936657462553598\n",
      "train loss:0.15626185156057887\n",
      "train loss:0.11995300715841505\n",
      "train loss:0.5124518150452242\n",
      "train loss:0.2739083566664101\n",
      "train loss:0.15907906525643561\n",
      "train loss:0.5941720404491292\n",
      "train loss:0.16891416813774557\n",
      "train loss:0.2172305169956845\n",
      "train loss:0.09727652977483839\n",
      "train loss:0.2110483877100911\n",
      "train loss:0.12317343973964828\n",
      "train loss:0.12173344915236586\n",
      "train loss:0.2728879860835301\n",
      "train loss:0.5515215070229266\n",
      "train loss:0.18851135755099124\n",
      "train loss:0.3451485501790743\n",
      "train loss:0.64615930483284\n",
      "train loss:0.44094196418369724\n",
      "train loss:0.11584956812933003\n",
      "train loss:0.20306580816760317\n",
      "train loss:0.07531286179873065\n",
      "train loss:0.0885692016436948\n",
      "train loss:0.26485107523563445\n",
      "train loss:0.1755737079276331\n",
      "train loss:0.12446302253907426\n",
      "train loss:0.4877643835647747\n",
      "train loss:0.09407247303572151\n",
      "train loss:0.17188456198319113\n",
      "train loss:0.4854394672633993\n",
      "train loss:0.03483803640270733\n",
      "train loss:0.2032917892909264\n",
      "train loss:0.15650833531979327\n",
      "train loss:0.0988872529162568\n",
      "train loss:0.19506389018260623\n",
      "train loss:0.057738271483399745\n",
      "train loss:0.08611435463080591\n",
      "train loss:0.43147237062790295\n",
      "train loss:0.20158300250955757\n",
      "train loss:0.0844163793894965\n",
      "train loss:0.13246686628815346\n",
      "train loss:0.30126923389486726\n",
      "train loss:0.22923371992630753\n",
      "train loss:0.26004998190402717\n",
      "train loss:0.10281707919035532\n",
      "train loss:0.5093743805001282\n",
      "train loss:0.7838305993001627\n",
      "train loss:0.2134233836872179\n",
      "train loss:0.15110705431094243\n",
      "train loss:0.532784951848998\n",
      "train loss:0.24743461631658129\n",
      "train loss:0.08763860654585051\n",
      "train loss:0.1604570739683127\n",
      "train loss:0.4493116015707035\n",
      "train loss:0.31122336236409065\n",
      "train loss:0.13658588768870628\n",
      "train loss:0.04457895815022627\n",
      "train loss:0.23504066016792294\n",
      "train loss:0.2385500452207202\n",
      "train loss:0.08859696652000547\n",
      "train loss:0.2473921989492284\n",
      "train loss:0.32339682030691463\n",
      "train loss:0.10153235544492555\n",
      "train loss:0.19517686449068908\n",
      "train loss:0.13451291316287375\n",
      "train loss:0.15503654493814134\n",
      "train loss:0.06067750505024264\n",
      "train loss:0.08274961520346313\n",
      "train loss:0.2202482611723362\n",
      "train loss:0.6126293947102062\n",
      "train loss:0.14813995326345894\n",
      "train loss:0.17738992644288204\n",
      "train loss:0.06894562843341123\n",
      "train loss:0.10717421036681579\n",
      "train loss:0.30592464741684267\n",
      "train loss:0.14167984363278782\n",
      "train loss:0.5469686447682514\n",
      "train loss:0.12348072428983006\n",
      "train loss:0.10376488207477652\n",
      "train loss:0.14247304203718295\n",
      "train loss:0.9144565355644061\n",
      "train loss:0.40367045058054823\n",
      "train loss:0.18606982161535324\n",
      "train loss:0.1606094251969472\n",
      "train loss:0.2555492470262463\n",
      "train loss:0.12849389828995467\n",
      "train loss:0.18694046196430944\n",
      "train loss:0.08046451030720062\n",
      "train loss:0.044201859360301174\n",
      "train loss:0.24568441744412797\n",
      "train loss:0.21106423369549648\n",
      "train loss:0.07554183775789863\n",
      "train loss:0.24335927816911251\n",
      "train loss:0.11826486000850496\n",
      "train loss:0.14431021128666188\n",
      "train loss:0.4977288365420714\n",
      "train loss:0.48312090852566636\n",
      "train loss:0.25623795336695526\n",
      "train loss:0.10965020880512602\n",
      "train loss:0.28415881850490754\n",
      "train loss:0.3820981824343129\n",
      "train loss:0.13991578164619914\n",
      "train loss:0.09030771277433988\n",
      "train loss:0.14038802851606472\n",
      "train loss:0.21033756041793286\n",
      "train loss:0.04013025802826452\n",
      "train loss:0.08688754688124124\n",
      "train loss:0.10623868740358201\n",
      "train loss:0.14434883637949225\n",
      "train loss:0.13136903050720133\n",
      "train loss:0.04392652523066879\n",
      "train loss:0.4539643483513287\n",
      "train loss:0.47470159700430387\n",
      "train loss:0.1415039704692581\n",
      "train loss:0.8153551399668372\n",
      "train loss:0.2877888928429422\n",
      "train loss:0.1388541070467716\n",
      "train loss:0.3853645104412412\n",
      "train loss:0.1527091422329872\n",
      "train loss:0.09500239101584546\n",
      "train loss:0.13158820133802557\n",
      "train loss:0.14357025302814497\n",
      "train loss:0.6284312893224442\n",
      "train loss:0.10178507007686026\n",
      "train loss:0.187281025774399\n",
      "train loss:0.10572238700333272\n",
      "train loss:0.15714876825380936\n",
      "train loss:0.15386698050229908\n",
      "train loss:0.4326117622664309\n",
      "train loss:0.38136479550557134\n",
      "train loss:0.15238016330426232\n",
      "train loss:0.13827540644721822\n",
      "train loss:0.288570712047704\n",
      "train loss:0.23662678539648735\n",
      "train loss:0.12287494979128173\n",
      "train loss:0.27661265664103146\n",
      "train loss:0.082546193273795\n",
      "train loss:0.08061717529486073\n",
      "train loss:0.17220954789903373\n",
      "train loss:0.6189561873380807\n",
      "train loss:0.43531758493357675\n",
      "train loss:0.18281521613766052\n",
      "train loss:0.13787748320902832\n",
      "train loss:0.31654800346202755\n",
      "train loss:0.1667546330938312\n",
      "train loss:0.1654308808967846\n",
      "train loss:0.33467509302278575\n",
      "train loss:0.24861738669327263\n",
      "train loss:0.07609210689217785\n",
      "train loss:0.11128953426624702\n",
      "train loss:0.1595877268264746\n",
      "train loss:0.21855186665406579\n",
      "train loss:0.05939930697625182\n",
      "train loss:0.3007984042518841\n",
      "train loss:0.182382243920357\n",
      "train loss:0.49239767070730556\n",
      "train loss:0.10828927950983488\n",
      "train loss:0.33517587061423343\n",
      "train loss:0.1035043526178494\n",
      "train loss:0.29055034438330096\n",
      "train loss:0.1453820478349461\n",
      "train loss:0.23911937378571058\n",
      "train loss:0.18293218474089157\n",
      "train loss:0.1633901142519391\n",
      "train loss:0.1500381006607091\n",
      "train loss:0.20920278830861075\n",
      "train loss:0.10478511458642353\n",
      "train loss:0.1309610583729804\n",
      "train loss:0.5125623727828161\n",
      "train loss:0.1814347767504756\n",
      "train loss:0.15975146040190452\n",
      "train loss:0.2344454220663028\n",
      "train loss:0.06784983532646977\n",
      "train loss:0.0995185065589271\n",
      "train loss:0.11805055094045247\n",
      "train loss:0.6595816342087854\n",
      "train loss:0.2951386639188782\n",
      "train loss:0.23347239291140093\n",
      "train loss:0.5459318677366851\n",
      "train loss:0.18933509939536736\n",
      "train loss:0.1205579891488667\n",
      "train loss:0.4982324961057992\n",
      "train loss:0.4953951930993245\n",
      "train loss:0.461186855443539\n",
      "train loss:0.15889762013066733\n",
      "train loss:0.1382858797949364\n",
      "train loss:0.19695982183146693\n",
      "train loss:0.060863798229414956\n",
      "train loss:0.07097622032651896\n",
      "train loss:0.07443358540331732\n",
      "train loss:0.11417916727368102\n",
      "train loss:0.1903440996829597\n",
      "train loss:0.11841000927239284\n",
      "train loss:0.08291142258836537\n",
      "train loss:0.2832689100783826\n",
      "train loss:0.03509637428049879\n",
      "train loss:0.2963483469078347\n",
      "train loss:0.13985484235605675\n",
      "train loss:0.1410212410400729\n",
      "train loss:0.18324932573781685\n",
      "train loss:0.09052792039042765\n",
      "train loss:0.15802154061957036\n",
      "train loss:0.0814192378933333\n",
      "train loss:0.13227090624511414\n",
      "train loss:0.3114344109538601\n",
      "train loss:0.12366695457104997\n",
      "train loss:0.09149737409921921\n",
      "train loss:0.1771378334859\n",
      "train loss:0.23159173085119428\n",
      "train loss:0.13095916835378588\n",
      "train loss:0.0615950016279359\n",
      "train loss:0.24913437172302955\n",
      "train loss:0.13269642723102698\n",
      "train loss:0.28279193028605437\n",
      "train loss:0.21693734156465008\n",
      "train loss:0.04820365892877261\n",
      "train loss:0.12930702324211335\n",
      "train loss:0.03619763464275925\n",
      "train loss:0.08440583909575344\n",
      "train loss:0.31730847707501086\n",
      "train loss:0.4053470788360418\n",
      "train loss:0.4182586833194764\n",
      "train loss:0.07606961432634796\n",
      "train loss:0.4122803668708175\n",
      "train loss:0.12343748600828608\n",
      "train loss:0.1470256434142859\n",
      "train loss:0.13746487750257064\n",
      "train loss:0.14934429126028972\n",
      "train loss:0.422398170023378\n",
      "train loss:0.12513457947493004\n",
      "train loss:0.10879520362667246\n",
      "train loss:0.15672643954458246\n",
      "train loss:0.24277015959046017\n",
      "train loss:0.5560756798862758\n",
      "train loss:0.1877456120286485\n",
      "train loss:0.1647428074812845\n",
      "train loss:0.06392021064538475\n",
      "train loss:0.28823557076709816\n",
      "train loss:0.06458022743437059\n",
      "train loss:0.03870723818318485\n",
      "train loss:0.06223520975027066\n",
      "train loss:0.2886491865322577\n",
      "train loss:0.07952847781463454\n",
      "train loss:0.12864701593510774\n",
      "train loss:0.10790036791518356\n",
      "train loss:0.41132604284492585\n",
      "train loss:0.2053179254596419\n",
      "train loss:0.10404423447222673\n",
      "train loss:0.16247909681097022\n",
      "train loss:0.1573507718869182\n",
      "train loss:0.07748152269527264\n",
      "train loss:0.05342999774807538\n",
      "train loss:0.27212376306398023\n",
      "train loss:0.1028158127093779\n",
      "train loss:0.16162757492254703\n",
      "train loss:0.10950985604267302\n",
      "train loss:0.2339174860552945\n",
      "train loss:0.10096519741612285\n",
      "train loss:0.12420628049291585\n",
      "train loss:0.4587212793855425\n",
      "train loss:0.39978747415989885\n",
      "train loss:0.11222857306680145\n",
      "train loss:0.07729999503984289\n",
      "train loss:0.09876257005122507\n",
      "train loss:0.48506744979788574\n",
      "train loss:0.1210724408461916\n",
      "train loss:0.13132830126850542\n",
      "train loss:0.14952413533288364\n",
      "train loss:0.30612916371374355\n",
      "train loss:0.43749543802512153\n",
      "train loss:0.30609011697035504\n",
      "train loss:0.10175832470970911\n",
      "train loss:0.12491815532029119\n",
      "train loss:0.30263338857106103\n",
      "train loss:0.4813410252960882\n",
      "train loss:0.6764825404218439\n",
      "train loss:0.09213969225543167\n",
      "train loss:0.13298578815111609\n",
      "train loss:0.25686358722252045\n",
      "train loss:0.16415429609938417\n",
      "train loss:0.2309723275483354\n",
      "train loss:0.204360758961631\n",
      "train loss:0.26648980630257624\n",
      "train loss:0.11258080840734987\n",
      "train loss:0.10212325507750393\n",
      "train loss:0.137689060829955\n",
      "train loss:0.6241398324041016\n",
      "train loss:0.10821537783171213\n",
      "train loss:0.11609445004701607\n",
      "train loss:0.26326858389559693\n",
      "train loss:0.11748099165215045\n",
      "train loss:0.21717512285665377\n",
      "train loss:0.09376982699839667\n",
      "train loss:0.37052413971532006\n",
      "train loss:0.410733758528502\n",
      "train loss:0.09050056785298746\n",
      "train loss:0.14228008120610655\n",
      "train loss:0.2712230195181804\n",
      "train loss:0.10028053117922121\n",
      "train loss:0.14318654157907287\n",
      "train loss:0.3854632546945209\n",
      "train loss:0.36716638929998235\n",
      "train loss:0.23478013829573635\n",
      "train loss:0.30128772938291426\n",
      "train loss:0.2598594741766266\n",
      "train loss:0.2600310770744777\n",
      "train loss:0.4915394996514123\n",
      "train loss:0.06928451465095542\n",
      "train loss:0.0344078687394756\n",
      "train loss:0.12403818937768747\n",
      "train loss:0.089808203104279\n",
      "train loss:0.10225372453783921\n",
      "train loss:0.3598411446743724\n",
      "train loss:0.19571060783761288\n",
      "train loss:0.35812518793266324\n",
      "train loss:0.09443502946284464\n",
      "train loss:0.2260631341386824\n",
      "train loss:0.15441338842089247\n",
      "train loss:0.5664795882261755\n",
      "train loss:0.06932205926604651\n",
      "train loss:0.3479946260082909\n",
      "train loss:0.16415416837834534\n",
      "train loss:0.1839490960518016\n",
      "train loss:0.6073208640359898\n",
      "train loss:0.1593542304403029\n",
      "train loss:0.14811598984128477\n",
      "train loss:0.09435357874006714\n",
      "train loss:0.586002431432026\n",
      "train loss:0.5765380988846186\n",
      "train loss:0.49009151558460073\n",
      "train loss:0.1135537186582775\n",
      "train loss:0.15614670960816499\n",
      "train loss:0.25362462343205944\n",
      "train loss:0.18974249387343053\n",
      "train loss:0.09739853466407282\n",
      "train loss:0.10066401501733219\n",
      "train loss:0.23977846651086662\n",
      "train loss:0.09638893937724903\n",
      "train loss:0.14986397391298178\n",
      "train loss:0.160057334557841\n",
      "train loss:0.10033392062418195\n",
      "train loss:0.10398355826323225\n",
      "train loss:0.15119065509022128\n",
      "train loss:0.10034070998516303\n",
      "train loss:0.3334396738002665\n",
      "train loss:0.17707572438624472\n",
      "train loss:0.10254875370889044\n",
      "train loss:0.2486500920431485\n",
      "train loss:0.1495929887566087\n",
      "train loss:0.1299074262219464\n",
      "train loss:0.13061933058874525\n",
      "train loss:0.3088860437870698\n",
      "train loss:0.19885808454568416\n",
      "train loss:0.16292680416554306\n",
      "train loss:0.08401978359070952\n",
      "train loss:0.11008541377054132\n",
      "train loss:0.4050589904201913\n",
      "train loss:0.26411713916304874\n",
      "train loss:0.15690331511010133\n",
      "train loss:0.11935360551009824\n",
      "train loss:0.07524095462567024\n",
      "train loss:0.06616868090738957\n",
      "train loss:0.06993514409327288\n",
      "train loss:0.5870847907577051\n",
      "train loss:0.1260580824435002\n",
      "train loss:0.10347678109155788\n",
      "train loss:0.046939561410325445\n",
      "train loss:0.19048620819563272\n",
      "train loss:0.07049724255585046\n",
      "train loss:0.2414205577998119\n",
      "train loss:0.2324222040503101\n",
      "train loss:0.2660032474947026\n",
      "train loss:0.10454543088571619\n",
      "train loss:0.07122931956367165\n",
      "train loss:0.13393851933819212\n",
      "train loss:0.3050197857364536\n",
      "train loss:0.3385525805992091\n",
      "train loss:0.22633602790204277\n",
      "train loss:0.4304272766889888\n",
      "train loss:0.15670052356868353\n",
      "train loss:0.18413634681147784\n",
      "train loss:0.1570807276772678\n",
      "train loss:0.1475015014175486\n",
      "train loss:0.2476475902883431\n",
      "train loss:0.1726197279563298\n",
      "train loss:0.22202294630675684\n",
      "train loss:0.466860615292279\n",
      "train loss:0.18713643391108642\n",
      "train loss:0.4710725062811839\n",
      "train loss:0.07414990110338432\n",
      "train loss:0.16415400329867022\n",
      "train loss:0.15009705289203126\n",
      "train loss:0.10014417865373994\n",
      "train loss:0.2682341982935027\n",
      "train loss:0.17376185278190065\n",
      "train loss:0.27613593059137426\n",
      "train loss:0.0701174290324981\n",
      "train loss:0.4677923503471657\n",
      "train loss:0.14875108271534823\n",
      "train loss:0.2597671503834872\n",
      "train loss:0.24787733575052934\n",
      "train loss:0.15333038792909517\n",
      "train loss:0.10085958092959936\n",
      "train loss:0.46047495056216214\n",
      "train loss:0.28904518296081044\n",
      "train loss:0.08215193883509941\n",
      "train loss:0.4137942893880934\n",
      "train loss:0.20744887470005374\n",
      "train loss:0.08322536578078739\n",
      "train loss:0.05709598672597333\n",
      "train loss:0.24931976735948713\n",
      "train loss:0.2314504808663033\n",
      "train loss:0.08553520146679783\n",
      "train loss:0.11521198671500141\n",
      "train loss:0.10209689503527329\n",
      "train loss:0.09886592315846252\n",
      "train loss:0.13776699122024727\n",
      "train loss:0.2802202714261344\n",
      "train loss:0.22641100679329218\n",
      "train loss:0.1076668193606009\n",
      "train loss:0.32441984251389033\n",
      "train loss:0.21226263004795926\n",
      "train loss:0.15654662231469385\n",
      "train loss:0.12141999425512122\n",
      "train loss:0.06597188355363955\n",
      "train loss:0.17965948816320204\n",
      "train loss:0.4586742621323436\n",
      "train loss:0.14637512366349434\n",
      "train loss:0.1434519543609038\n",
      "train loss:0.2769869512083082\n",
      "train loss:0.16934391359696305\n",
      "train loss:0.12051374128242712\n",
      "train loss:0.09005893920752525\n",
      "train loss:0.2378821953799119\n",
      "train loss:0.33990054049196083\n",
      "train loss:0.11170009255616069\n",
      "train loss:0.19066443242526637\n",
      "train loss:0.08486733489847963\n",
      "train loss:0.08060879039494025\n",
      "train loss:0.3756919495331691\n",
      "train loss:0.15184404034880755\n",
      "train loss:0.06019791573595427\n",
      "train loss:0.06126126963235903\n",
      "train loss:0.06541902222118232\n",
      "train loss:0.434179079298631\n",
      "train loss:0.08446405782380048\n",
      "train loss:0.24861685900003047\n",
      "train loss:0.06250771104467792\n",
      "train loss:0.03234833285392089\n",
      "train loss:0.10959008088428555\n",
      "train loss:0.042496528667219424\n",
      "train loss:0.34600423081730075\n",
      "train loss:0.22903907323363762\n",
      "train loss:0.11322682250347796\n",
      "train loss:0.08502003309562363\n",
      "train loss:0.19299261525930778\n",
      "train loss:0.21229499339310812\n",
      "train loss:0.2615379544804308\n",
      "train loss:0.11062883709576207\n",
      "train loss:0.05618947858575542\n",
      "train loss:0.1713461794046614\n",
      "train loss:0.2431788262367846\n",
      "train loss:0.20717741745403376\n",
      "train loss:0.2195557394966861\n",
      "train loss:0.1999980429981928\n",
      "train loss:0.1257369445966455\n",
      "train loss:0.16038698172168847\n",
      "train loss:0.26301947245215296\n",
      "train loss:0.02542254449121457\n",
      "train loss:0.0919478335559726\n",
      "train loss:0.2747587071425536\n",
      "train loss:0.16912325408319578\n",
      "train loss:0.28767673005246963\n",
      "train loss:0.18738843939041663\n",
      "train loss:0.2289271023446325\n",
      "train loss:0.1362231640927164\n",
      "train loss:0.1925509853347008\n",
      "train loss:0.09194643708358083\n",
      "train loss:0.0908145801026241\n",
      "train loss:0.07847906348827831\n",
      "train loss:0.15266390501587979\n",
      "train loss:0.18456047546173276\n",
      "train loss:0.08281787020721977\n",
      "train loss:0.278413498531332\n",
      "train loss:0.11890458961159615\n",
      "train loss:0.0628366212117559\n",
      "train loss:0.23470800006006465\n",
      "train loss:0.03867790810926449\n",
      "train loss:1.0342725887884074\n",
      "train loss:0.39390925897516527\n",
      "train loss:0.34160598662435643\n",
      "train loss:0.15157986064547707\n",
      "train loss:0.07400092494571969\n",
      "train loss:0.1375042586793413\n",
      "train loss:0.07922517682442058\n",
      "train loss:0.06221064918637449\n",
      "train loss:0.07755215956262267\n",
      "train loss:0.19146545537984488\n",
      "train loss:0.5407509398525158\n",
      "train loss:0.26900283172506423\n",
      "train loss:0.611997392883227\n",
      "train loss:0.1363075204305149\n",
      "train loss:0.6757448867680833\n",
      "train loss:0.13131754786303956\n",
      "train loss:0.17685845148623036\n",
      "train loss:0.12327439723371206\n",
      "train loss:0.1228316992211956\n",
      "train loss:0.07609840394567935\n",
      "train loss:0.06218901501465318\n",
      "train loss:0.1381937981951063\n",
      "train loss:0.09552835460455525\n",
      "train loss:0.23492208571442608\n",
      "train loss:0.07751149909760252\n",
      "train loss:0.12904364082509406\n",
      "train loss:0.10793435949895608\n",
      "train loss:0.056250371079454096\n",
      "train loss:0.08969124935206621\n",
      "train loss:0.14573325123124356\n",
      "train loss:0.14709219673800686\n",
      "train loss:0.06155693556188366\n",
      "train loss:0.11519834303211321\n",
      "train loss:0.14116546899320767\n",
      "train loss:0.2072735213145211\n",
      "train loss:0.1705989283516993\n",
      "train loss:0.10986991266145543\n",
      "train loss:0.23437731890266256\n",
      "train loss:0.26567269573026187\n",
      "train loss:0.14312917853836504\n",
      "train loss:0.1380624035860031\n",
      "train loss:0.025989891613059587\n",
      "train loss:0.11121757416375513\n",
      "train loss:0.0806725155120834\n",
      "train loss:0.7210685278851732\n",
      "train loss:0.12294272978631704\n",
      "train loss:0.2536484405245586\n",
      "train loss:0.23208342786502748\n",
      "train loss:0.08457808438122236\n",
      "train loss:0.2040339981182623\n",
      "train loss:0.10373338002323601\n",
      "train loss:0.15422467360586423\n",
      "train loss:0.0725209118782961\n",
      "train loss:0.2901184176489082\n",
      "train loss:0.2420952796450297\n",
      "train loss:0.37459275119279856\n",
      "train loss:0.05554939606754189\n",
      "train loss:0.2635196724595444\n",
      "train loss:0.15007028278831014\n",
      "train loss:0.11721822421226799\n",
      "train loss:0.07067701807691318\n",
      "train loss:0.12245508417486348\n",
      "train loss:0.25155855203082417\n",
      "train loss:0.0883055883816502\n",
      "train loss:0.43012728345355045\n",
      "train loss:0.0611096324270832\n",
      "train loss:0.08424921567913163\n",
      "train loss:0.07730040157551557\n",
      "train loss:0.14429574149253055\n",
      "train loss:0.30042309847595744\n",
      "train loss:0.07204954743722475\n",
      "train loss:0.16168384744879608\n",
      "train loss:0.25976348632283564\n",
      "train loss:0.27421577415545695\n",
      "train loss:0.11988888349113078\n",
      "train loss:0.39382236972129736\n",
      "train loss:0.10034274172338753\n",
      "train loss:0.18738880797507243\n",
      "train loss:0.47021074092811155\n",
      "train loss:0.0832383587523145\n",
      "train loss:0.15813076988509578\n",
      "train loss:0.09352211697868218\n",
      "train loss:0.11327190268487865\n",
      "train loss:0.1952438293577337\n",
      "train loss:0.15759638369261042\n",
      "train loss:0.270148207386395\n",
      "train loss:0.2169430117003801\n",
      "train loss:0.0692744058996822\n",
      "train loss:0.12722728594495403\n",
      "train loss:0.14615760315664397\n",
      "train loss:0.5959151352364399\n",
      "train loss:0.1724746656175177\n",
      "train loss:0.19398289918230438\n",
      "train loss:0.3124955700100377\n",
      "train loss:0.1276925235540553\n",
      "train loss:0.09351558273856551\n",
      "train loss:0.13710108143977487\n",
      "train loss:0.09751645802759117\n",
      "train loss:0.09679582074302809\n",
      "train loss:0.21551390260420594\n",
      "train loss:0.2713136251674946\n",
      "train loss:0.2107491918418153\n",
      "train loss:0.24398595587537703\n",
      "train loss:0.24827974619275378\n",
      "train loss:0.20748992154228485\n",
      "train loss:0.4243119326205289\n",
      "train loss:0.15541186195706547\n",
      "train loss:0.1762534568973924\n",
      "train loss:0.4154629420650511\n",
      "train loss:0.07130678129056645\n",
      "train loss:0.26347859039356514\n",
      "train loss:0.049911306364945396\n",
      "train loss:0.12192025293989817\n",
      "train loss:0.3248851192752806\n",
      "train loss:0.16994942579310768\n",
      "train loss:0.1784942766003627\n",
      "train loss:0.05980815240857919\n",
      "train loss:0.07141155641286584\n",
      "train loss:0.1869960583813873\n",
      "train loss:0.0613205432841955\n",
      "train loss:0.20605248446863866\n",
      "train loss:0.3510779847044706\n",
      "train loss:0.15407126102863142\n",
      "train loss:0.5005334384303779\n",
      "train loss:0.20825343332759355\n",
      "train loss:0.08194401348163019\n",
      "train loss:0.06611126033846942\n",
      "train loss:0.16296501980154643\n",
      "train loss:0.1823688221434015\n",
      "train loss:0.07155871561992258\n",
      "train loss:0.17662572194022888\n",
      "train loss:0.19487762858774965\n",
      "train loss:0.09984398828707687\n",
      "train loss:0.44305939914974507\n",
      "train loss:0.28024129796506225\n",
      "train loss:0.2070580492448778\n",
      "train loss:0.15054105825733438\n",
      "train loss:0.3684659623434574\n",
      "train loss:0.152024392120931\n",
      "train loss:0.33931486937849453\n",
      "train loss:0.13365234450407482\n",
      "train loss:0.09533785613621726\n",
      "train loss:0.09233208739043745\n",
      "train loss:0.06760091910232882\n",
      "train loss:0.0883180008647268\n",
      "train loss:0.10943857220850348\n",
      "train loss:0.1417445970092635\n",
      "train loss:0.2548468965227703\n",
      "train loss:0.09423696816661255\n",
      "train loss:0.05447723588659061\n",
      "train loss:0.0679628426014125\n",
      "train loss:0.3152097549395684\n",
      "train loss:0.06373125097650863\n",
      "train loss:0.05582048002976787\n",
      "train loss:0.3094932165025266\n",
      "train loss:0.23378304485905907\n",
      "train loss:0.3377134506454534\n",
      "train loss:0.07545498058430791\n",
      "train loss:0.12198592503900571\n",
      "train loss:0.42661503941379103\n",
      "train loss:0.21068202114058332\n",
      "train loss:0.18308050506214807\n",
      "train loss:0.30570933145881646\n",
      "train loss:0.10344466222072461\n",
      "train loss:0.07953297879872984\n",
      "train loss:0.39495483396767594\n",
      "train loss:0.08308657312887473\n",
      "train loss:0.07822004724027619\n",
      "train loss:0.1251647970622747\n",
      "train loss:0.1458032487335723\n",
      "train loss:0.0540073246335591\n",
      "train loss:0.044105095803970255\n",
      "train loss:0.3608868984321346\n",
      "train loss:0.09361745257010459\n",
      "train loss:0.05033947621389453\n",
      "train loss:0.05670416661387247\n",
      "train loss:0.48273154703867227\n",
      "train loss:0.12546222553177644\n",
      "train loss:0.11415368320299066\n",
      "train loss:0.061600531074367464\n",
      "train loss:0.14831465275451977\n",
      "train loss:0.12580545128585566\n",
      "train loss:0.23081091288443484\n",
      "train loss:0.08310899110977166\n",
      "train loss:0.09870011789550426\n",
      "train loss:0.13647059660060595\n",
      "train loss:0.0834819075891125\n",
      "train loss:0.10618723323094167\n",
      "train loss:0.31245181934854616\n",
      "train loss:0.39821132471026066\n",
      "train loss:0.2642613031056247\n",
      "train loss:0.15345036011085997\n",
      "train loss:0.12360220156610009\n",
      "train loss:0.09441418843151922\n",
      "train loss:0.0707878804062102\n",
      "train loss:0.12034800450509377\n",
      "train loss:0.09946393775942419\n",
      "train loss:0.14124037598946618\n",
      "train loss:0.32433814449631754\n",
      "train loss:0.050661299926521915\n",
      "train loss:0.06557299796592957\n",
      "train loss:0.13351632774578828\n",
      "train loss:0.1854025482897564\n",
      "train loss:0.17575971119379408\n",
      "train loss:0.10210426180919004\n",
      "train loss:0.2727304225883974\n",
      "train loss:0.08301479445417395\n",
      "train loss:0.1762260342924136\n",
      "train loss:0.07347214047720538\n",
      "train loss:0.2614592297364138\n",
      "train loss:0.413624307028586\n",
      "train loss:0.15529156648857612\n",
      "train loss:0.04743060151418187\n",
      "train loss:0.2564744286749753\n",
      "train loss:0.25387426893290177\n",
      "train loss:0.1314804234268816\n",
      "train loss:0.22272542807355694\n",
      "train loss:0.29262360519192177\n",
      "train loss:0.2480433945140852\n",
      "train loss:0.16990273636049202\n",
      "train loss:0.041503123558701144\n",
      "train loss:0.07029291101660685\n",
      "train loss:0.16185566220766354\n",
      "train loss:0.05059302927542193\n",
      "train loss:0.22836392538894162\n",
      "train loss:0.28378197587939535\n",
      "train loss:0.22385489940517142\n",
      "train loss:0.24819298593045253\n",
      "train loss:0.10848605868957553\n",
      "train loss:0.33760433445796134\n",
      "train loss:0.03922711293591477\n",
      "train loss:0.30142879946544876\n",
      "train loss:0.2731899491402756\n",
      "train loss:0.029025162515916057\n",
      "train loss:0.2804479432654174\n",
      "train loss:0.12942285952596716\n",
      "train loss:0.0804115043761981\n",
      "train loss:0.12330666569302248\n",
      "train loss:0.05418765477144675\n",
      "train loss:0.029014297653954607\n",
      "train loss:0.07559366131544225\n",
      "train loss:0.05649533178331266\n",
      "train loss:0.09488700668185825\n",
      "train loss:0.07093404464923914\n",
      "train loss:0.11979773365716327\n",
      "train loss:0.3586730664082534\n",
      "train loss:0.5539647120856309\n",
      "train loss:0.13890532980008588\n",
      "train loss:0.15038846146558124\n",
      "train loss:0.20645019132903697\n",
      "train loss:0.06174223841724466\n",
      "train loss:0.11476529135597059\n",
      "train loss:0.21019355592017924\n",
      "train loss:0.15874078003956368\n",
      "train loss:0.0906216595819594\n",
      "train loss:0.05775182712751066\n",
      "train loss:0.0708216939126701\n",
      "train loss:0.31872857338919364\n",
      "train loss:0.1980725396881421\n",
      "train loss:0.19246926964287298\n",
      "train loss:0.08480722511200664\n",
      "train loss:0.1778987411458721\n",
      "train loss:0.0893216603623711\n",
      "train loss:0.07274449359521572\n",
      "train loss:0.3395275743301579\n",
      "train loss:0.36583857498391903\n",
      "train loss:0.9508329042844084\n",
      "train loss:0.5890411107782069\n",
      "train loss:0.09015267667795113\n",
      "train loss:0.12753853980810426\n",
      "train loss:0.3202090158793762\n",
      "train loss:0.07460240876172061\n",
      "train loss:0.07726346950760118\n",
      "train loss:0.07852549421678322\n",
      "train loss:0.17791177528412122\n",
      "train loss:0.11923076322734469\n",
      "train loss:0.11058449093969913\n",
      "train loss:0.3921882229259571\n",
      "train loss:0.09628878831709152\n",
      "train loss:0.03493887014415173\n",
      "train loss:0.07892813942208529\n",
      "train loss:0.15675488332415205\n",
      "train loss:0.04666208149030467\n",
      "train loss:0.343471394831981\n",
      "train loss:0.08857622471824081\n",
      "train loss:0.07987779136889112\n",
      "train loss:0.09326869142638333\n",
      "train loss:0.3188752331355188\n",
      "train loss:0.10124019932462605\n",
      "train loss:0.2128839428883591\n",
      "train loss:0.17355216509956478\n",
      "train loss:0.07180107124196869\n",
      "train loss:0.26078996426159495\n",
      "train loss:0.1755127118508142\n",
      "train loss:0.15347197363244203\n",
      "train loss:0.22245041336897897\n",
      "train loss:0.061824839122350664\n",
      "train loss:0.09634287905026473\n",
      "train loss:0.15707189108226477\n",
      "train loss:0.02066973838376464\n",
      "train loss:0.1500129473876265\n",
      "train loss:0.08755036107888861\n",
      "train loss:0.19423621166201024\n",
      "train loss:0.29870613112084743\n",
      "train loss:0.10236274414169118\n",
      "train loss:0.07236664507740245\n",
      "train loss:0.07198953978060675\n",
      "train loss:0.18663192598048145\n",
      "train loss:0.09911568493296943\n",
      "train loss:0.08069887663633143\n",
      "train loss:0.2602223749845093\n",
      "train loss:0.20974680899311837\n",
      "train loss:0.03415364549554315\n",
      "train loss:0.44649704141502966\n",
      "train loss:0.32384268442080255\n",
      "train loss:0.25356700169732316\n",
      "train loss:0.15966312835185034\n",
      "train loss:0.2349815710310232\n",
      "train loss:0.13638676535379238\n",
      "train loss:0.07084533284479923\n",
      "train loss:0.08768615113619115\n",
      "train loss:0.20283697338288464\n",
      "train loss:0.2662119780637482\n",
      "train loss:0.22795559646690602\n",
      "train loss:0.1922326984195581\n",
      "train loss:0.4554607533784727\n",
      "train loss:0.3732849823437767\n",
      "train loss:0.08831767273576581\n",
      "train loss:0.07102064088283735\n",
      "train loss:0.14855342755537104\n",
      "train loss:0.044489353535098586\n",
      "train loss:0.14435634386647578\n",
      "train loss:0.11292086806771887\n",
      "train loss:0.12094278584855916\n",
      "train loss:0.1218704718594826\n",
      "train loss:0.06342180508235469\n",
      "train loss:0.12825753944357066\n",
      "train loss:0.15382458719201914\n",
      "train loss:0.30993827548534514\n",
      "train loss:0.09292539532123718\n",
      "train loss:0.15043525941570884\n",
      "train loss:0.16941066054851306\n",
      "train loss:0.23162005240232164\n",
      "train loss:0.3821770639180691\n",
      "train loss:0.3645657430189995\n",
      "train loss:0.21359583258177056\n",
      "train loss:0.43644279593827184\n",
      "train loss:0.053208489118368923\n",
      "train loss:0.19366493029065937\n",
      "train loss:0.08014052940147423\n",
      "train loss:0.1067913141148096\n",
      "train loss:0.254243601451903\n",
      "train loss:0.07092868352378663\n",
      "train loss:0.5076365719820926\n",
      "train loss:0.10016423409839065\n",
      "train loss:0.315711441991917\n",
      "train loss:0.09411322046621066\n",
      "train loss:0.13854192706122279\n",
      "train loss:0.13978230877214193\n",
      "train loss:0.16209912079561795\n",
      "train loss:0.1735088089510272\n",
      "train loss:0.06742408523318433\n",
      "train loss:0.21050592387310446\n",
      "train loss:0.11725543793388983\n",
      "train loss:0.16219461060708174\n",
      "train loss:0.18659285736498532\n",
      "train loss:0.15790168665908477\n",
      "train loss:0.21953536084659123\n",
      "train loss:0.08687424667218246\n",
      "train loss:0.04398036664451442\n",
      "train loss:0.10951245004220726\n",
      "train loss:0.2717986144278539\n",
      "train loss:0.19517317575861576\n",
      "train loss:0.047117743262518955\n",
      "train loss:0.20777284799920792\n",
      "train loss:0.3172121694191644\n",
      "train loss:0.24061351230449865\n",
      "train loss:0.50821679394831\n",
      "train loss:0.029904711018032444\n",
      "train loss:0.11314158142596725\n",
      "train loss:0.1172777859264453\n",
      "train loss:0.3253552734130053\n",
      "train loss:0.102583666891201\n",
      "train loss:0.47530621590887917\n",
      "train loss:0.10087747730565191\n",
      "train loss:0.07295126305124128\n",
      "train loss:0.10686308416523613\n",
      "train loss:0.1196460524501875\n",
      "train loss:0.13713518406830336\n",
      "train loss:0.11769948807898634\n",
      "train loss:0.062218262934806735\n",
      "train loss:0.19349775504660205\n",
      "train loss:0.3685488931868266\n",
      "train loss:0.07768893074784697\n",
      "train loss:0.04163615055625691\n",
      "train loss:0.07657927519009705\n",
      "train loss:0.5145189808006595\n",
      "train loss:0.04573066293108117\n",
      "train loss:0.06736273778175808\n",
      "train loss:0.2587708067485094\n",
      "train loss:0.12667241666287768\n",
      "train loss:0.0923139592707328\n",
      "train loss:0.14391454496709333\n",
      "train loss:0.2184573628133023\n",
      "train loss:0.07543635405583529\n",
      "train loss:0.07580766675875783\n",
      "train loss:0.5734051698206121\n",
      "train loss:0.05888672801321468\n",
      "train loss:0.12103361850360973\n",
      "train loss:0.02409585635306512\n",
      "train loss:0.02678945262876205\n",
      "train loss:0.08506919807047171\n",
      "train loss:0.11972286701342032\n",
      "train loss:0.060066665899889064\n",
      "train loss:0.30107274694044267\n",
      "train loss:0.22768196353630016\n",
      "train loss:0.09761168260646316\n",
      "train loss:0.06959855633559404\n",
      "train loss:0.20635396323769006\n",
      "train loss:0.07315179005389416\n",
      "train loss:0.1241100688575196\n",
      "train loss:0.08616894211661719\n",
      "train loss:0.16615875711731623\n",
      "train loss:0.11698122557512171\n",
      "train loss:0.07894313795920513\n",
      "train loss:0.0792958283155413\n",
      "train loss:0.18305902974477634\n",
      "train loss:0.2175893417663331\n",
      "train loss:0.2005652448820725\n",
      "train loss:0.05326213250091661\n",
      "train loss:0.3051405340910919\n",
      "train loss:0.11801466863007673\n",
      "train loss:0.021950185316798824\n",
      "train loss:0.22811494500055052\n",
      "train loss:0.22011751532221568\n",
      "train loss:0.07338588651332242\n",
      "train loss:0.08730336243375353\n",
      "train loss:0.301581246464694\n",
      "train loss:0.201912959013025\n",
      "train loss:0.05645192486492609\n",
      "train loss:0.6392298612582745\n",
      "train loss:0.7189033660031597\n",
      "train loss:0.22664855990909782\n",
      "train loss:0.2438424992960268\n",
      "train loss:0.1742323950789526\n",
      "train loss:0.13687352788886664\n",
      "train loss:0.11572346171887277\n",
      "train loss:0.15252563388218843\n",
      "train loss:0.02773780015460881\n",
      "train loss:0.2526909055631911\n",
      "train loss:0.1044456531458547\n",
      "train loss:0.19833746864750135\n",
      "train loss:0.13142067624504428\n",
      "train loss:0.045260264610702594\n",
      "train loss:0.040932100392558846\n",
      "train loss:0.04295873024906736\n",
      "train loss:0.13100267785514622\n",
      "train loss:0.18905300416778964\n",
      "train loss:0.11943763175104936\n",
      "train loss:0.10935546692408069\n",
      "train loss:0.20206771546455024\n",
      "train loss:0.1615891187572373\n",
      "train loss:0.5874669908484375\n",
      "train loss:0.1290637007762749\n",
      "train loss:0.06600418946818835\n",
      "train loss:0.1211724521031507\n",
      "train loss:0.07642753421275361\n",
      "train loss:0.4442175473933787\n",
      "train loss:0.5449161561340428\n",
      "train loss:0.7151092377641354\n",
      "train loss:0.08337605954917987\n",
      "train loss:0.0949502939990358\n",
      "train loss:0.21127981859518646\n",
      "train loss:0.03696219554350891\n",
      "train loss:0.20807720931694484\n",
      "train loss:0.7524378230121104\n",
      "train loss:0.03133796811563438\n",
      "train loss:0.11100796161801371\n",
      "train loss:0.10736318655930682\n",
      "train loss:0.12065043944752596\n",
      "train loss:0.03446002408953211\n",
      "train loss:0.09519937869522574\n",
      "train loss:0.1676671049650454\n",
      "train loss:0.09175027040765965\n",
      "train loss:0.10948173826985973\n",
      "train loss:0.7046436142108626\n",
      "train loss:0.14550416030667568\n",
      "train loss:0.10156643009159859\n",
      "train loss:0.1173249359408355\n",
      "train loss:0.056053094527760784\n",
      "train loss:0.1087294091821243\n",
      "train loss:0.31067746144849173\n",
      "train loss:0.0633677321744663\n",
      "train loss:0.15154146320390188\n",
      "train loss:0.09277195603163449\n",
      "train loss:0.06434969683261703\n",
      "train loss:0.13801073686363988\n",
      "train loss:0.180239344897146\n",
      "train loss:0.12301072620710701\n",
      "train loss:0.04696385881635598\n",
      "train loss:0.10835058947493405\n",
      "train loss:0.19922761550657722\n",
      "train loss:0.10502987332979172\n",
      "train loss:0.33570982790820375\n",
      "train loss:0.292249700551428\n",
      "train loss:0.12590568174993727\n",
      "train loss:0.10185255373934841\n",
      "train loss:0.04863348106344977\n",
      "train loss:0.07405002355050876\n",
      "train loss:0.11124311772093746\n",
      "train loss:0.06514681530946678\n",
      "train loss:0.24415086411619336\n",
      "train loss:0.21851643144400634\n",
      "train loss:0.07681106645482563\n",
      "train loss:0.14739319497630782\n",
      "train loss:0.0684240447989388\n",
      "train loss:0.10309936485090017\n",
      "train loss:0.3668906607452604\n",
      "train loss:0.05555969942915505\n",
      "train loss:0.1583974698986914\n",
      "train loss:0.05513190227332058\n",
      "train loss:0.2208621350654916\n",
      "train loss:0.07589148292204732\n",
      "train loss:0.0759457653022732\n",
      "train loss:0.07839597512206918\n",
      "train loss:0.1512555203991119\n",
      "train loss:0.062313890152161176\n",
      "train loss:0.09577847945432354\n",
      "train loss:0.20624698247610707\n",
      "train loss:0.13749689643868768\n",
      "train loss:0.11144724311031336\n",
      "train loss:0.21594331440354958\n",
      "train loss:0.10378155770638026\n",
      "train loss:0.08236526955202225\n",
      "train loss:0.1904884715009459\n",
      "train loss:0.07119135773383889\n",
      "train loss:0.07605647144370467\n",
      "train loss:0.11549082488747199\n",
      "train loss:0.30726680412072943\n",
      "train loss:0.11122991787623347\n",
      "train loss:0.1574996132628758\n",
      "train loss:0.04883315069544433\n",
      "train loss:0.18689314382768418\n",
      "train loss:0.11906395654683616\n",
      "train loss:0.0813996074448489\n",
      "train loss:0.04378743006849793\n",
      "train loss:0.19084836028405577\n",
      "train loss:0.1271394507514602\n",
      "train loss:0.20659077827005617\n",
      "train loss:0.23948819022674075\n",
      "train loss:0.10197721086886609\n",
      "train loss:0.12227020359881018\n",
      "train loss:0.16968762591970143\n",
      "train loss:0.07964175306346025\n",
      "train loss:0.375753383345189\n",
      "train loss:0.07816061955789444\n",
      "train loss:0.0519507460181724\n",
      "train loss:0.13361796971892853\n",
      "train loss:0.10013844083583984\n",
      "train loss:0.04039165639510167\n",
      "train loss:0.1382254417476792\n",
      "train loss:0.2053750075515032\n",
      "train loss:0.1251035495173913\n",
      "train loss:0.0339006946518333\n",
      "train loss:0.12589131413014334\n",
      "train loss:0.03843579005506621\n",
      "train loss:0.30085924533926184\n",
      "train loss:0.18444926626371674\n",
      "train loss:0.06341665994988419\n",
      "train loss:0.051535154494704555\n",
      "train loss:0.13735038598443555\n",
      "train loss:0.08186531705189883\n",
      "train loss:0.575890030148041\n",
      "train loss:0.32089927281475716\n",
      "train loss:0.09391580600048922\n",
      "train loss:0.20181596393235163\n",
      "train loss:0.19968413308113436\n",
      "train loss:0.46991618289599835\n",
      "train loss:0.1928970206889892\n",
      "train loss:0.1495578136552748\n",
      "train loss:0.09025607161834004\n",
      "train loss:0.10604366895327295\n",
      "train loss:0.04592916737666559\n",
      "train loss:0.1562336701277599\n",
      "train loss:0.1410081944749295\n",
      "train loss:0.28298331844195285\n",
      "train loss:0.08923317337113298\n",
      "train loss:0.37300682825090814\n",
      "train loss:0.1520951790783915\n",
      "train loss:0.07469187174038755\n",
      "train loss:0.12450001517033851\n",
      "train loss:0.048828926300376746\n",
      "train loss:0.1507783195110738\n",
      "train loss:0.10091838375041906\n",
      "train loss:0.2576510751949208\n",
      "train loss:0.297794390911846\n",
      "train loss:0.49028786516073286\n",
      "train loss:0.1603652115220936\n",
      "train loss:0.1396759731646558\n",
      "train loss:0.10542794722159167\n",
      "train loss:0.1165135132568319\n",
      "train loss:0.15510098391348853\n",
      "train loss:0.1968911249460689\n",
      "train loss:0.16093961134789556\n",
      "train loss:0.11423092568749019\n",
      "train loss:0.3609393548510345\n",
      "train loss:0.2810649038136407\n",
      "train loss:0.1699473057793452\n",
      "train loss:0.20977743477944616\n",
      "train loss:0.07569888981064489\n",
      "train loss:0.10915036720139991\n",
      "train loss:0.1490940063863972\n",
      "train loss:0.1573615784128039\n",
      "train loss:0.04745229557265941\n",
      "train loss:0.27634366744476224\n",
      "train loss:0.11729302953857482\n",
      "train loss:0.0413630017420305\n",
      "train loss:0.18537143758458813\n",
      "train loss:0.10933775015333647\n",
      "train loss:0.07479175273745439\n",
      "train loss:0.040578849286181315\n",
      "train loss:0.0983084778654239\n",
      "train loss:0.05476972874077357\n",
      "train loss:0.15118984010835462\n",
      "train loss:0.07575592479928536\n",
      "train loss:0.05730840164023589\n",
      "train loss:0.2738913272690351\n",
      "train loss:0.21168788371297798\n",
      "train loss:0.1178949174655294\n",
      "train loss:0.07366751367260234\n",
      "train loss:0.09755043671162933\n",
      "train loss:0.09543654386316114\n",
      "train loss:0.18296377295362323\n",
      "train loss:0.09586182284133322\n",
      "train loss:0.09914584229551686\n",
      "train loss:0.12514536813926397\n",
      "train loss:0.05036062349846983\n",
      "train loss:0.5926173634720182\n",
      "train loss:0.10458108410322163\n",
      "train loss:0.25586597836410374\n",
      "train loss:0.19644470300962003\n",
      "train loss:0.10232126098852262\n",
      "train loss:0.029138171085384384\n",
      "train loss:0.04450288120093136\n",
      "train loss:0.1457608899707409\n",
      "train loss:0.048415660668435626\n",
      "train loss:0.32621732820093774\n",
      "train loss:0.06762280324976772\n",
      "train loss:0.25335611118269336\n",
      "train loss:0.0410034569014075\n",
      "train loss:0.55162016004433\n",
      "train loss:0.055765884545677664\n",
      "train loss:0.12097008738696063\n",
      "train loss:0.19395698011128326\n",
      "train loss:0.08803628514663256\n",
      "train loss:0.11424865162874756\n",
      "train loss:0.17632916111867886\n",
      "train loss:0.0813441872476345\n",
      "train loss:0.28365797631208156\n",
      "train loss:0.27385526128816806\n",
      "train loss:0.10990963465752622\n",
      "train loss:0.3126689653863683\n",
      "train loss:0.5484114494311971\n",
      "train loss:0.11823923188932459\n",
      "train loss:0.1192138691420367\n",
      "train loss:0.014143987387614783\n",
      "train loss:0.07826980639558553\n",
      "train loss:0.12305674703580181\n",
      "train loss:0.05363935357055852\n",
      "train loss:0.18326253922617328\n",
      "train loss:0.5530463575199472\n",
      "train loss:0.10448647689171697\n",
      "train loss:0.15417264025435629\n",
      "train loss:0.11230037099302452\n",
      "train loss:0.06204083268014519\n",
      "train loss:0.32279527006573433\n",
      "train loss:0.066497551484006\n",
      "train loss:0.11403903744909652\n",
      "train loss:0.09493386076398963\n",
      "train loss:0.10715671272564142\n",
      "train loss:0.09495108399752646\n",
      "train loss:0.0850683534432315\n",
      "train loss:0.1327887008848042\n",
      "train loss:0.07566505908106434\n",
      "train loss:0.06459624965099418\n",
      "train loss:0.10440725808710714\n",
      "train loss:0.24842663309608343\n",
      "train loss:0.11142491760415389\n",
      "train loss:0.14426707051105891\n",
      "train loss:0.1292873973281189\n",
      "train loss:0.38849463102102466\n",
      "train loss:0.036487136191953026\n",
      "train loss:0.22143197260726746\n",
      "train loss:0.10813167338479962\n",
      "train loss:0.06789406569948979\n",
      "train loss:0.1688104132289485\n",
      "train loss:0.09080380293909225\n",
      "train loss:0.10623561572047714\n",
      "train loss:0.2569700839822937\n",
      "train loss:0.13738691224848862\n",
      "train loss:0.08434098345825061\n",
      "train loss:0.09736457975728378\n",
      "train loss:0.15244559441433156\n",
      "train loss:0.02291798190161079\n",
      "train loss:0.03649410908432327\n",
      "train loss:0.142891046412616\n",
      "train loss:0.09907349576966544\n",
      "train loss:0.17234738326089064\n",
      "train loss:0.14655766854545987\n",
      "train loss:0.04464267708916313\n",
      "train loss:0.07890977473657904\n",
      "train loss:0.34872559061132247\n",
      "train loss:0.16931035223037033\n",
      "train loss:0.12976775604766358\n",
      "train loss:0.06027391092124518\n",
      "train loss:0.0767875922898619\n",
      "train loss:0.10065879888545853\n",
      "train loss:0.3443288905437971\n",
      "train loss:0.037840668147411216\n",
      "train loss:0.1258587991736801\n",
      "train loss:0.1923190199377175\n",
      "train loss:0.10243275331081741\n",
      "train loss:0.3701527784198829\n",
      "train loss:0.1328059541078995\n",
      "train loss:0.3439682157541565\n",
      "train loss:0.06796269441195797\n",
      "train loss:0.060220397844481736\n",
      "train loss:0.23234988139070192\n",
      "train loss:0.1376421998854923\n",
      "train loss:0.18232157505968657\n",
      "train loss:0.2219793351855693\n",
      "train loss:0.09689942551558539\n",
      "train loss:0.23019628549945215\n",
      "train loss:0.36332643906845596\n",
      "train loss:0.06454682991417432\n",
      "train loss:0.18829645636751696\n",
      "train loss:0.3997890293167385\n",
      "train loss:0.15275485722437704\n",
      "train loss:0.06501590387218228\n",
      "train loss:0.09577558211266077\n",
      "train loss:0.07350862943837147\n",
      "train loss:0.29554265295862703\n",
      "train loss:0.03007673673660658\n",
      "train loss:0.0631953423124059\n",
      "train loss:0.43810992495550205\n",
      "train loss:0.3119106064039865\n",
      "train loss:0.19790299166534536\n",
      "train loss:0.3967916780227113\n",
      "train loss:0.09890414482492269\n",
      "train loss:0.1841481029720719\n",
      "train loss:0.11331664922647303\n",
      "train loss:0.06839779461089839\n",
      "train loss:0.15356261036828817\n",
      "train loss:0.14558807201357174\n",
      "train loss:0.9386348499294481\n",
      "train loss:0.1700054944833692\n",
      "train loss:0.23410727949349475\n",
      "train loss:0.2154785472229299\n",
      "train loss:0.06747948024352737\n",
      "train loss:0.5424252832203494\n",
      "train loss:0.2241160044219832\n",
      "train loss:0.08914573331783339\n",
      "train loss:0.1011239136858182\n",
      "train loss:0.1602915571425263\n",
      "train loss:0.09196014248144536\n",
      "train loss:0.2553954132952563\n",
      "train loss:0.23879089553059346\n",
      "train loss:0.32189461746002535\n",
      "train loss:0.20728142423014995\n",
      "train loss:0.25607202064272383\n",
      "train loss:0.19944434794341723\n",
      "train loss:0.34303308833915963\n",
      "train loss:0.2761310738924388\n",
      "train loss:0.2646440596922831\n",
      "train loss:0.1542082713236418\n",
      "train loss:0.029452980141672927\n",
      "train loss:0.2173426556807541\n",
      "train loss:0.3405121605990929\n",
      "train loss:0.11161071825694852\n",
      "train loss:0.10826398619668368\n",
      "train loss:0.14269341313717127\n",
      "train loss:0.19321077760277316\n",
      "train loss:0.1331878046506847\n",
      "train loss:0.15995102816548584\n",
      "train loss:0.2670293785406126\n",
      "train loss:0.05131459212413561\n",
      "train loss:0.19855833896060177\n",
      "train loss:0.06878762335790034\n",
      "train loss:0.06901063638613093\n",
      "train loss:0.06050855178286871\n",
      "train loss:0.3056704714386552\n",
      "train loss:0.08131742816945656\n",
      "train loss:0.3119831433747403\n",
      "train loss:0.17344511459406067\n",
      "train loss:0.14962171660291326\n",
      "train loss:0.14552272402210872\n",
      "train loss:0.1494342688154902\n",
      "train loss:0.11775855016867019\n",
      "train loss:0.04007623461340954\n",
      "train loss:0.09228768174231172\n",
      "train loss:0.14418132854687624\n",
      "train loss:0.0928947758381154\n",
      "train loss:0.10515536277683096\n",
      "train loss:0.17840634987236137\n",
      "train loss:0.017793084911837648\n",
      "train loss:0.18664621340159637\n",
      "train loss:0.09068780867857325\n",
      "train loss:0.1416802049610023\n",
      "train loss:0.22217288068741126\n",
      "train loss:0.10269725826410286\n",
      "train loss:0.20087185209885428\n",
      "train loss:0.0574881475462704\n",
      "train loss:0.18048029424679413\n",
      "train loss:0.8495622585739028\n",
      "train loss:0.03213651795918322\n",
      "train loss:0.30764041259990105\n",
      "train loss:0.11600088112888345\n",
      "train loss:0.12918381124297992\n",
      "train loss:0.26260087431773693\n",
      "train loss:0.05626582866208249\n",
      "train loss:0.06675101522783937\n",
      "train loss:0.03181308453605481\n",
      "train loss:0.1401935019962056\n",
      "train loss:0.2149004666787372\n",
      "train loss:0.09881257175250076\n",
      "train loss:0.0814928659800696\n",
      "train loss:0.18609922279068536\n",
      "train loss:0.10811889452539816\n",
      "train loss:0.05468671142479248\n",
      "train loss:0.17925000688360454\n",
      "train loss:0.09130010205421218\n",
      "train loss:0.27135868134326696\n",
      "train loss:0.19677956267284424\n",
      "train loss:0.3014085623847277\n",
      "train loss:0.07817421234213717\n",
      "train loss:0.28671812532514607\n",
      "train loss:0.5241520100968258\n",
      "train loss:0.13145433986091618\n",
      "train loss:0.38695633320346845\n",
      "train loss:0.06393529205745362\n",
      "train loss:0.21129912479559385\n",
      "train loss:0.26783251411650016\n",
      "train loss:0.09951406394262335\n",
      "train loss:0.1438873736211287\n",
      "train loss:0.30826905329393745\n",
      "train loss:0.17828666516334357\n",
      "train loss:0.2969288144894948\n",
      "train loss:0.2550366543510181\n",
      "train loss:0.020968839299947934\n",
      "train loss:0.0798833553680625\n",
      "train loss:0.08886372929644967\n",
      "train loss:0.1012876734664355\n",
      "train loss:0.16598866993357922\n",
      "train loss:0.17848284724156505\n",
      "train loss:0.1675230020913868\n",
      "train loss:0.05200690918879772\n",
      "train loss:0.06586111325291019\n",
      "train loss:0.052755785137717406\n",
      "train loss:0.16425702514990304\n",
      "train loss:0.17938069930198886\n",
      "train loss:0.18185661981445278\n",
      "train loss:0.10452668113414479\n",
      "train loss:0.17642821096115757\n",
      "train loss:0.3689051883901025\n",
      "train loss:0.04922321971602162\n",
      "train loss:0.30455440027114455\n",
      "train loss:0.051154661805551144\n",
      "train loss:0.12984529963599278\n",
      "train loss:0.5373549039343244\n",
      "train loss:0.12257478825939437\n",
      "train loss:0.09402966208210913\n",
      "train loss:0.11559617600981713\n",
      "train loss:0.04822215112142477\n",
      "train loss:0.11080122868839043\n",
      "train loss:0.05842708018375302\n",
      "train loss:0.07520711795613196\n",
      "train loss:0.03927509310331668\n",
      "train loss:0.20678994893631542\n",
      "train loss:0.1339118589008227\n",
      "train loss:0.08668761519391109\n",
      "train loss:0.20438377089278398\n",
      "train loss:0.3287277710920917\n",
      "train loss:0.07654370567798921\n",
      "train loss:0.16978212742311619\n",
      "train loss:0.11254837607410087\n",
      "train loss:0.21709739616777085\n",
      "train loss:0.05036601610395598\n",
      "train loss:0.15749369539985233\n",
      "train loss:0.07427600956419336\n",
      "train loss:0.05576129295377246\n",
      "train loss:0.07573038295291981\n",
      "train loss:0.21908553907356448\n",
      "train loss:0.09914960288380936\n",
      "train loss:1.1491705673362504\n",
      "train loss:0.04249136689369858\n",
      "train loss:0.09701842128960703\n",
      "train loss:0.06163814946026498\n",
      "train loss:0.03248583860250866\n",
      "train loss:0.28449493099985257\n",
      "train loss:0.14494596624496847\n",
      "train loss:0.0791276629222486\n",
      "train loss:0.05239201040694929\n",
      "train loss:0.07498415907776591\n",
      "train loss:0.16247930122664428\n",
      "train loss:0.15284011120967678\n",
      "train loss:0.3020284948450195\n",
      "train loss:0.168254044259361\n",
      "train loss:0.12851050917558327\n",
      "train loss:0.35383711456702827\n",
      "train loss:0.3255730750101996\n",
      "train loss:0.12440697650823465\n",
      "train loss:0.4492709835534579\n",
      "train loss:0.10936267091437715\n",
      "train loss:0.06782315358315998\n",
      "train loss:0.0639252872942104\n",
      "train loss:0.20931260972306723\n",
      "train loss:0.13936130130608615\n",
      "train loss:0.11357594068254596\n",
      "train loss:0.09731221014100708\n",
      "train loss:0.6057331310442893\n",
      "train loss:0.06685207086099101\n",
      "train loss:0.1597361707723872\n",
      "train loss:0.627220335814295\n",
      "train loss:0.21092946062006157\n",
      "train loss:0.14570291949519365\n",
      "train loss:0.9419690552629434\n",
      "train loss:0.14745683624785308\n",
      "train loss:0.12343534712485937\n",
      "train loss:0.10516073895180128\n",
      "train loss:0.05106872673269629\n",
      "train loss:0.28986592744221695\n",
      "train loss:0.12983203031201043\n",
      "train loss:0.148594070597172\n",
      "train loss:0.26410114429063475\n",
      "train loss:0.26428103882093873\n",
      "train loss:0.1406262839926912\n",
      "train loss:0.077722939798899\n",
      "train loss:0.09497703700695094\n",
      "train loss:0.35842422460875634\n",
      "train loss:0.3888081159148449\n",
      "train loss:0.29651043286417017\n",
      "train loss:0.3686743953746781\n",
      "train loss:0.10101770407318453\n",
      "train loss:0.07797435256152026\n",
      "train loss:0.41458541868291776\n",
      "train loss:0.2886081988236563\n",
      "train loss:0.4116294357937259\n",
      "train loss:0.10780978420766862\n",
      "train loss:0.01880577180287457\n",
      "train loss:0.14790602661704155\n",
      "train loss:0.2468182229924777\n",
      "train loss:0.18148241171817883\n",
      "train loss:0.3120351968959232\n",
      "train loss:0.05702023774793448\n",
      "train loss:0.5836126539463253\n",
      "train loss:0.15619356672553555\n",
      "train loss:0.2718272029579744\n",
      "train loss:0.07572356926164228\n",
      "train loss:0.24800395892108296\n",
      "train loss:0.13416254958582946\n",
      "train loss:0.16966695244485253\n",
      "train loss:0.208117688428338\n",
      "train loss:0.10561194809448182\n",
      "train loss:0.16310277055599812\n",
      "train loss:0.19034087152683568\n",
      "train loss:0.5539567745903067\n",
      "train loss:0.16942208418346374\n",
      "train loss:0.16037317605860418\n",
      "train loss:0.022801418077273664\n",
      "train loss:0.09755049508094436\n",
      "train loss:0.314383608270018\n",
      "train loss:0.17071960556230698\n",
      "train loss:0.14108543015439348\n",
      "train loss:0.045127629474767686\n",
      "train loss:0.07983090249977162\n",
      "train loss:0.20317153599372764\n",
      "train loss:0.16571912943218778\n",
      "train loss:0.09705583995614811\n",
      "train loss:0.08012762174383418\n",
      "train loss:0.16413346967867107\n",
      "train loss:0.34457622324123005\n",
      "train loss:0.13207149665731996\n",
      "train loss:0.11181707794641746\n",
      "train loss:0.12133070437411726\n",
      "train loss:0.12373902118560429\n",
      "train loss:0.20425485204674076\n",
      "train loss:0.0801023090318804\n",
      "train loss:0.10521746100883189\n",
      "train loss:0.11375132829671537\n",
      "train loss:0.09344637414256945\n",
      "train loss:0.26882510715539926\n",
      "train loss:0.3054594127103658\n",
      "train loss:0.37232079014972547\n",
      "train loss:0.07499925335762744\n",
      "train loss:0.02922602107404499\n",
      "train loss:0.3858793422371114\n",
      "train loss:0.33532773160299956\n",
      "train loss:0.22129152085536652\n",
      "train loss:0.32396349084837756\n",
      "train loss:0.1036691786932862\n",
      "train loss:0.05462441203060033\n",
      "train loss:0.4214767159392482\n",
      "train loss:0.11803760114763684\n",
      "train loss:0.15149733393227288\n",
      "train loss:0.5961744618400673\n",
      "train loss:0.11578830403885057\n",
      "train loss:0.03361616992475756\n",
      "train loss:0.4669974944157852\n",
      "train loss:0.15710309608842415\n",
      "train loss:0.3813030483528057\n",
      "train loss:0.14805984154739005\n",
      "train loss:0.0818034531138429\n",
      "train loss:0.31342773843560295\n",
      "train loss:0.022479245787810145\n",
      "train loss:0.3114554972557836\n",
      "train loss:0.16825731719155007\n",
      "train loss:0.37501466297324815\n",
      "train loss:0.12538848209763456\n",
      "train loss:0.03848287719548873\n",
      "train loss:0.4606260540557971\n",
      "train loss:0.1525798761807577\n",
      "train loss:0.16741223540333336\n",
      "train loss:0.27009021148822515\n",
      "train loss:0.07640161822025467\n",
      "train loss:0.0985789547387318\n",
      "train loss:0.16937922736934496\n",
      "train loss:0.12441912917387513\n",
      "train loss:0.11342481463027504\n",
      "train loss:0.16539066777784617\n",
      "train loss:0.0660587787357623\n",
      "train loss:0.2892028171730712\n",
      "train loss:0.24250504239116916\n",
      "train loss:0.07591597451062651\n",
      "train loss:0.06527386855269268\n",
      "train loss:0.14283275782058644\n",
      "train loss:0.09990302785940586\n",
      "train loss:0.04230463265413212\n",
      "train loss:0.08345682906192449\n",
      "train loss:0.220012004406396\n",
      "train loss:0.13514692008856827\n",
      "train loss:0.1036438055606555\n",
      "train loss:0.15770446471898977\n",
      "train loss:0.3098735324367272\n",
      "train loss:0.1082979478118761\n",
      "train loss:0.3615020743339673\n",
      "train loss:0.0984034611693291\n",
      "train loss:0.08328022967337295\n",
      "train loss:0.08313363259395484\n",
      "train loss:0.5227599933053776\n",
      "train loss:0.15642461871414315\n",
      "train loss:0.09975162394533996\n",
      "train loss:0.11360455998264987\n",
      "train loss:0.10934395133356543\n",
      "train loss:0.11672807713414497\n",
      "train loss:0.6417908989079879\n",
      "train loss:0.0706039182528973\n",
      "train loss:0.10219450439005197\n",
      "train loss:0.40178282542705857\n",
      "train loss:0.2412925877193487\n",
      "train loss:0.17015294769483422\n",
      "train loss:0.0655173984497221\n",
      "train loss:0.11326211962054655\n",
      "train loss:0.1347371641726659\n",
      "train loss:0.06774362026328508\n",
      "train loss:0.23344271738683325\n",
      "train loss:0.17746996460081577\n",
      "train loss:0.08871818904576872\n",
      "train loss:0.2836212588601106\n",
      "train loss:0.08467103493442278\n",
      "train loss:0.040455355357938094\n",
      "train loss:0.11852547544092432\n",
      "train loss:0.10618208187376421\n",
      "train loss:0.2458113822211074\n",
      "train loss:0.2462941463394086\n",
      "train loss:0.18297852274532583\n",
      "train loss:0.1316739589573719\n",
      "train loss:0.15033265226763792\n",
      "train loss:0.15875645127188054\n",
      "train loss:0.09736316406600956\n",
      "train loss:0.24401335833876867\n",
      "train loss:0.1952097867585083\n",
      "train loss:0.15477178788098642\n",
      "train loss:0.1504922740848632\n",
      "train loss:0.3342621885596064\n",
      "train loss:0.10565523933747978\n",
      "train loss:0.21197997003653307\n",
      "train loss:0.042387970143638265\n",
      "train loss:0.30475885011733544\n",
      "train loss:0.36529473546293917\n",
      "train loss:0.2171066783506014\n",
      "train loss:0.053285320681896384\n",
      "train loss:0.17307697826157653\n",
      "train loss:0.05015453564223519\n",
      "train loss:0.44633065771603314\n",
      "train loss:0.03096026916025735\n",
      "train loss:0.11093034218299216\n",
      "train loss:0.03356678749627259\n",
      "train loss:0.08587842327650617\n",
      "train loss:0.08018573122276974\n",
      "train loss:0.3497288358802305\n",
      "train loss:0.15798992470605464\n",
      "train loss:0.0667977725894287\n",
      "train loss:0.11318098032976243\n",
      "train loss:0.05403580013027057\n",
      "train loss:0.1773808354547703\n",
      "train loss:0.4126907393137922\n",
      "train loss:0.08488039735897487\n",
      "train loss:0.17246677778180286\n",
      "train loss:0.19891130657065076\n",
      "train loss:0.11102379517343995\n",
      "train loss:0.14711461937376896\n",
      "train loss:0.029711440800042132\n",
      "train loss:0.07066588504870638\n",
      "train loss:0.10264316625810313\n",
      "train loss:0.057513079019284055\n",
      "train loss:0.08065046790729338\n",
      "train loss:0.07583843387486915\n",
      "train loss:0.04064475159926386\n",
      "train loss:0.1588296251409252\n",
      "train loss:0.06892723081145727\n",
      "train loss:0.046042670704430494\n",
      "train loss:0.09585319398265803\n",
      "train loss:0.07572879058403076\n",
      "train loss:0.03571734627651382\n",
      "train loss:0.06672987795717775\n",
      "train loss:0.03211486238474901\n",
      "train loss:0.25139586187347623\n",
      "train loss:0.06923414068405273\n",
      "train loss:0.18661805494079023\n",
      "train loss:0.22971116607559863\n",
      "train loss:0.07390279486653156\n",
      "train loss:0.0973604208986871\n",
      "train loss:0.0920398894695196\n",
      "train loss:0.26177681074181053\n",
      "train loss:0.049076325232403\n",
      "train loss:0.4912847946235416\n",
      "train loss:0.19628182065942726\n",
      "train loss:0.054474607767514366\n",
      "train loss:0.08936583943031291\n",
      "train loss:0.0835603583641123\n",
      "train loss:0.06134776784931572\n",
      "train loss:0.3499579032645098\n",
      "train loss:0.5788586183417948\n",
      "train loss:0.20751372736955292\n",
      "train loss:0.04435995921026184\n",
      "train loss:0.29935946396909274\n",
      "train loss:0.06035331446137439\n",
      "train loss:0.25752190452554463\n",
      "train loss:0.16227279891272733\n",
      "train loss:0.10534303659902294\n",
      "train loss:0.20614594332578978\n",
      "train loss:0.1563505661695392\n",
      "train loss:0.33231193821035143\n",
      "train loss:0.19364273703494897\n",
      "train loss:0.14262764793516797\n",
      "train loss:0.05383849643103545\n",
      "train loss:0.13134669573987245\n",
      "train loss:0.10964711648172537\n",
      "train loss:0.27596763403573626\n",
      "train loss:0.1173023324710103\n",
      "train loss:0.024327742741166142\n",
      "train loss:0.031378717753856256\n",
      "train loss:0.3328973138888797\n",
      "train loss:0.06681159815843836\n",
      "train loss:0.04414389755535222\n",
      "train loss:0.02897840091442882\n",
      "train loss:0.34930172477660726\n",
      "train loss:0.24889633406247155\n",
      "train loss:0.04162852841374299\n",
      "train loss:0.06988867248417567\n",
      "train loss:0.1021561196159988\n",
      "train loss:0.43261553789362484\n",
      "train loss:0.13131042705488408\n",
      "train loss:0.10792992369767668\n",
      "train loss:0.12230083095168855\n",
      "train loss:0.13337758445549214\n",
      "train loss:0.051755672877249234\n",
      "train loss:0.05227809103255922\n",
      "train loss:0.15836130414435037\n",
      "train loss:0.10994643448127389\n",
      "train loss:0.17458224653624918\n",
      "train loss:0.14055351446532416\n",
      "train loss:0.19521845628654386\n",
      "train loss:0.09392748581679598\n",
      "train loss:0.1485521399397349\n",
      "train loss:0.12299714563837029\n",
      "train loss:0.22718377432027514\n",
      "train loss:0.10157295967742386\n",
      "train loss:0.1125131620989808\n",
      "train loss:0.1893906953516702\n",
      "train loss:0.1470042077296903\n",
      "train loss:0.1769202310970225\n",
      "train loss:0.09692408805792517\n",
      "train loss:0.16829275974765007\n",
      "train loss:0.13925007756110397\n",
      "train loss:0.09674738952732917\n",
      "train loss:0.07568900185298877\n",
      "train loss:0.02482074946215282\n",
      "train loss:0.05859674942994161\n",
      "train loss:0.28384156644112574\n",
      "train loss:0.1393108861346916\n",
      "train loss:0.08791079980192368\n",
      "train loss:0.09351070622736293\n",
      "train loss:0.12945620698420973\n",
      "train loss:0.03514342050187588\n",
      "train loss:0.20893958535991472\n",
      "train loss:0.13609563664359678\n",
      "train loss:0.13146285831955862\n",
      "train loss:0.19254416749509629\n",
      "train loss:0.07245137234128157\n",
      "train loss:0.043597410142496386\n",
      "train loss:0.044167173363413956\n",
      "train loss:0.0571952492463774\n",
      "train loss:0.13829522806105302\n",
      "train loss:0.3452418595971297\n",
      "train loss:0.04052577728179543\n",
      "train loss:0.09028980965653897\n",
      "train loss:0.24836571214344288\n",
      "train loss:0.18670496233115833\n",
      "train loss:0.13812899335747025\n",
      "train loss:0.06535959313504577\n",
      "train loss:0.1103715381689328\n",
      "train loss:0.25103121320973765\n",
      "train loss:0.03341814983388834\n",
      "train loss:0.1444303321280733\n",
      "train loss:0.3047452859419856\n",
      "train loss:0.08590075148959053\n",
      "train loss:0.19972522697002548\n",
      "train loss:0.1671136457564345\n",
      "train loss:0.41102551303929136\n",
      "train loss:0.05375229114473597\n",
      "train loss:0.5004325854245918\n",
      "train loss:0.16973725589952163\n",
      "train loss:0.44442857872712466\n",
      "train loss:0.1306053242160632\n",
      "train loss:0.1043967399639312\n",
      "train loss:0.20342071099889855\n",
      "train loss:0.07075892709493935\n",
      "train loss:0.2864509571207702\n",
      "train loss:0.07229267871055126\n",
      "train loss:0.2553486101746267\n",
      "train loss:0.16106419165250807\n",
      "train loss:0.1919378431403835\n",
      "train loss:0.15012537940593645\n",
      "train loss:0.09026149167965658\n",
      "train loss:0.0279702928616195\n",
      "train loss:0.2549748405990185\n",
      "train loss:0.06852044129655929\n",
      "train loss:0.16581246429851212\n",
      "train loss:0.08553158564069555\n",
      "train loss:0.11092182395731807\n",
      "train loss:0.11763333622737521\n",
      "train loss:0.1261710333674803\n",
      "train loss:0.07589730066596825\n",
      "train loss:0.30817536237516346\n",
      "train loss:0.12798692822736563\n",
      "train loss:0.37796244044828253\n",
      "train loss:0.08852520794248465\n",
      "train loss:0.34360137363793986\n",
      "train loss:0.07333082299720972\n",
      "train loss:0.2518777050137899\n",
      "train loss:0.7578146965679372\n",
      "train loss:0.5018324354356696\n",
      "train loss:0.054221443655629385\n",
      "train loss:0.19638857629417159\n",
      "train loss:0.22733795531425982\n",
      "train loss:0.3813925285526033\n",
      "train loss:0.09197261267150622\n",
      "train loss:0.46810865324900236\n",
      "train loss:0.035146717081385934\n",
      "train loss:0.10337579387828419\n",
      "train loss:0.05446255167343843\n",
      "train loss:0.17096657356085895\n",
      "train loss:0.20151602686039025\n",
      "train loss:0.2465338726459594\n",
      "train loss:0.14043630163752205\n",
      "train loss:0.08186074249207916\n",
      "train loss:0.12856214722458953\n",
      "train loss:0.1419028591537907\n",
      "train loss:0.1631285759535983\n",
      "train loss:0.26142419670175027\n",
      "train loss:0.1792369051896705\n",
      "train loss:0.023474215800681403\n",
      "train loss:0.0549334861499232\n",
      "train loss:0.17308232411506794\n",
      "train loss:0.026562190420453297\n",
      "train loss:0.10333920174649641\n",
      "train loss:0.11788556914557666\n",
      "train loss:0.20276971736004235\n",
      "train loss:0.19799296624598486\n",
      "train loss:0.08235069416891674\n",
      "train loss:0.07231892949913415\n",
      "train loss:0.32315994846752577\n",
      "train loss:0.05451577523621986\n",
      "train loss:0.21548021544286236\n",
      "train loss:0.12700378854829325\n",
      "train loss:0.1594310839522463\n",
      "train loss:0.14537231817539656\n",
      "train loss:0.04816731798593786\n",
      "train loss:0.14883732546274242\n",
      "train loss:0.16302382259802478\n",
      "train loss:0.17152093572969856\n",
      "train loss:0.12071317439400393\n",
      "train loss:0.3084483946086728\n",
      "train loss:0.09481303328263926\n",
      "train loss:0.5321021880072568\n",
      "train loss:0.31329023082862706\n",
      "train loss:0.03385168720147323\n",
      "train loss:0.2438238896270551\n",
      "train loss:0.10547208646683437\n",
      "train loss:0.1580157914174446\n",
      "train loss:0.14554257601870976\n",
      "train loss:0.1218896585686661\n",
      "train loss:0.12956419076268333\n",
      "train loss:0.15922525201338883\n",
      "train loss:0.05750275892209013\n",
      "train loss:0.16236631023742285\n",
      "train loss:0.1104194998024945\n",
      "train loss:0.15474429428739694\n",
      "train loss:0.03961945855238315\n",
      "train loss:0.5849596305346941\n",
      "train loss:0.18970830219794885\n",
      "train loss:0.1854200551772866\n",
      "train loss:0.20858431627656393\n",
      "train loss:0.05374664744283876\n",
      "train loss:0.13016847550737132\n",
      "train loss:0.03590008355282563\n",
      "train loss:0.17744180204839094\n",
      "train loss:0.22595025851264988\n",
      "train loss:0.06289347396171009\n",
      "train loss:0.059655876957887016\n",
      "train loss:0.09268040050391278\n",
      "train loss:0.09345157317623765\n",
      "train loss:0.31356533729694036\n",
      "train loss:0.12498669745428083\n",
      "train loss:0.20358631875814936\n",
      "train loss:0.07733238483878367\n",
      "train loss:0.05301505856751297\n",
      "train loss:0.3612287418885859\n",
      "train loss:0.18397017284302608\n",
      "train loss:0.12030778317548889\n",
      "train loss:0.14118521914648313\n",
      "train loss:0.10998102877535479\n",
      "train loss:0.10529771956077358\n",
      "train loss:0.44732733781655537\n",
      "train loss:0.314799787512422\n",
      "train loss:0.06354308933187569\n",
      "train loss:0.08616973550038468\n",
      "train loss:0.05060461243782813\n",
      "train loss:0.25946629868787413\n",
      "train loss:0.04939701603381642\n",
      "train loss:0.15721430276456788\n",
      "train loss:0.162178621172049\n",
      "train loss:0.06019878164217815\n",
      "train loss:0.2590701955722012\n",
      "train loss:0.119459576038799\n",
      "train loss:0.47018754020088116\n",
      "train loss:0.0851185031680992\n",
      "train loss:0.12790832631702986\n",
      "train loss:0.12721779598791177\n",
      "train loss:0.11507412011013017\n",
      "train loss:0.21796533288801104\n",
      "train loss:0.40424725393155714\n",
      "train loss:0.0873727524540579\n",
      "train loss:0.1099075396083583\n",
      "train loss:0.17457426908986762\n",
      "train loss:0.2064844349143115\n",
      "train loss:0.057599688115192164\n",
      "train loss:0.05910482816671179\n",
      "train loss:0.28110318243660704\n",
      "train loss:0.06380832194373398\n",
      "train loss:0.169388043157265\n",
      "train loss:0.06316040895980957\n",
      "train loss:0.2574199708659481\n",
      "train loss:0.1327587957529069\n",
      "train loss:0.26852986724027683\n",
      "train loss:0.20279147815519444\n",
      "train loss:0.3632777668281931\n",
      "train loss:0.10336926240824423\n",
      "train loss:0.09552000594370015\n",
      "train loss:0.1072553361148063\n",
      "train loss:0.4392181673984495\n",
      "train loss:0.3211061269806556\n",
      "train loss:0.31472229957962766\n",
      "train loss:0.11516193615057013\n",
      "train loss:0.15719895871145623\n",
      "train loss:0.037060531763253676\n",
      "train loss:0.1530601702319604\n",
      "train loss:0.3123217339056128\n",
      "train loss:0.30162810603004964\n",
      "train loss:0.11931530202513976\n",
      "train loss:0.09056723087264468\n",
      "train loss:0.15864983199369828\n",
      "train loss:0.27476316285593694\n",
      "train loss:0.1645776630845942\n",
      "train loss:0.04403757850452543\n",
      "train loss:0.08446458312260491\n",
      "train loss:0.06486362465731382\n",
      "train loss:0.13170860370602833\n",
      "train loss:0.085162817698443\n",
      "train loss:0.13996101479785095\n",
      "train loss:0.21990288404224256\n",
      "train loss:0.10737652504275975\n",
      "train loss:0.0195438570766365\n",
      "train loss:0.15378846583872194\n",
      "train loss:0.0654061614436825\n",
      "train loss:0.09749476102469035\n",
      "train loss:0.17917057574411718\n",
      "train loss:0.10663799427797005\n",
      "train loss:0.48220045368643255\n",
      "train loss:0.25828398467031094\n",
      "train loss:0.07675295561921347\n",
      "train loss:0.04775804124687078\n",
      "train loss:0.2326307243462602\n",
      "train loss:0.16513088123833647\n",
      "train loss:0.166454550283345\n",
      "train loss:0.313796463262229\n",
      "train loss:0.13829359396642382\n",
      "train loss:0.10334221232480117\n",
      "train loss:0.05457417068244371\n",
      "train loss:0.1734814132498113\n",
      "train loss:0.04900985343339394\n",
      "train loss:0.10680950695223028\n",
      "train loss:0.09549367754762882\n",
      "train loss:0.3767818740201773\n",
      "train loss:0.07181538984350344\n",
      "train loss:0.47973370418570205\n",
      "train loss:0.04243566939545282\n",
      "train loss:0.3210804119413614\n",
      "train loss:0.24928998539495145\n",
      "train loss:0.10414243422301488\n",
      "train loss:0.03358744756909423\n",
      "train loss:0.04323452764231262\n",
      "train loss:0.055755878199041944\n",
      "train loss:0.15726177132721927\n",
      "train loss:0.025845353322080176\n",
      "train loss:0.08017508668692858\n",
      "train loss:0.05450262150907795\n",
      "train loss:0.10384030068632033\n",
      "train loss:0.05130771195473341\n",
      "train loss:0.07256324647678421\n",
      "train loss:0.08190769002958632\n",
      "train loss:0.022627101686669846\n",
      "train loss:0.22578309512180145\n",
      "train loss:0.21106481652278603\n",
      "train loss:0.1141654684886971\n",
      "train loss:0.09344611240055102\n",
      "train loss:0.08693376252952772\n",
      "train loss:0.11559981162855093\n",
      "train loss:0.37065572386674633\n",
      "train loss:0.07389281514332127\n",
      "train loss:0.08480324922192023\n",
      "train loss:0.09849753550955034\n",
      "train loss:0.12586050444678268\n",
      "train loss:0.22082908015757666\n",
      "train loss:0.08429839257647101\n",
      "train loss:0.05420310068956453\n",
      "train loss:0.2504956835772561\n",
      "train loss:0.04311605074474062\n",
      "train loss:0.11541386600895417\n",
      "train loss:0.6727023088930697\n",
      "train loss:0.03318573075711462\n",
      "train loss:0.1568829419985118\n",
      "train loss:0.04686396598421638\n",
      "train loss:0.2750244163492112\n",
      "train loss:0.5332198363436532\n",
      "train loss:0.0961528269628292\n",
      "train loss:0.14528002585204808\n",
      "train loss:0.0265248346360917\n",
      "train loss:0.25564213529010543\n",
      "train loss:0.2933554417328359\n",
      "train loss:0.09845280017747153\n",
      "train loss:0.11502961488240343\n",
      "train loss:0.4608293921780734\n",
      "train loss:0.07388963568093974\n",
      "train loss:0.20764699813851237\n",
      "train loss:0.019768410137891795\n",
      "train loss:0.5082013073779561\n",
      "train loss:0.03309424746155112\n",
      "train loss:0.09682810549682247\n",
      "train loss:0.34247510525962865\n",
      "train loss:0.06318413921875102\n",
      "train loss:0.1494506043834194\n",
      "train loss:0.2043516518911757\n",
      "train loss:0.07707098917419197\n",
      "train loss:0.12808100855996243\n",
      "train loss:0.07081575895816758\n",
      "train loss:0.044596847446164094\n",
      "train loss:0.15986736793808554\n",
      "train loss:0.06415202157143196\n",
      "train loss:0.13817278497245777\n",
      "train loss:0.05402312236970591\n",
      "train loss:0.18705675026060498\n",
      "train loss:0.2692476116514162\n",
      "train loss:0.12389486537271753\n",
      "train loss:0.15626992761151967\n",
      "train loss:0.5129430512661123\n",
      "train loss:0.06264740641551889\n",
      "train loss:0.2858562513369649\n",
      "train loss:0.07115869013378205\n",
      "train loss:0.19318901958067414\n",
      "train loss:0.12307579962611422\n",
      "train loss:0.15977830252801914\n",
      "train loss:0.1696128123389534\n",
      "train loss:0.13905393445437936\n",
      "train loss:0.08974401576000081\n",
      "train loss:0.04911023020760739\n",
      "train loss:0.03645190100299103\n",
      "train loss:0.5250267492920178\n",
      "train loss:0.1543917741060685\n",
      "train loss:0.16512107109891233\n",
      "train loss:0.04393189088987839\n",
      "train loss:0.06611877358641391\n",
      "train loss:0.06304312618630639\n",
      "train loss:0.13748818719045067\n",
      "train loss:0.13160741296253614\n",
      "train loss:0.04362191463530067\n",
      "train loss:0.07437370059584285\n",
      "train loss:0.09965049717608568\n",
      "train loss:0.11814588689655589\n",
      "train loss:0.10822591783911686\n",
      "train loss:0.14454670618688312\n",
      "train loss:0.1370095707909092\n",
      "train loss:0.07482601173238346\n",
      "train loss:0.11244262957453627\n",
      "train loss:0.20140992299747942\n",
      "train loss:0.05425666330704966\n",
      "train loss:0.09636847908701614\n",
      "train loss:0.09716496537766325\n",
      "train loss:0.09185107909018653\n",
      "train loss:0.020687472784885155\n",
      "train loss:0.04369083469994937\n",
      "train loss:0.8318221296312724\n",
      "train loss:0.4296933731676496\n",
      "train loss:0.12330953561108553\n",
      "train loss:0.16530073947125007\n",
      "train loss:0.1327175899371252\n",
      "train loss:0.10224456421848034\n",
      "train loss:0.19641897206847328\n",
      "train loss:0.18731858496819204\n",
      "train loss:0.4824287309201851\n",
      "train loss:0.04382689687887979\n",
      "train loss:0.13092980943650168\n",
      "train loss:0.06885249084339819\n",
      "train loss:0.1535087961043885\n",
      "train loss:0.06340625367304031\n",
      "train loss:0.15654166028286082\n",
      "train loss:0.32615754413083126\n",
      "train loss:0.28585321777170364\n",
      "train loss:0.05897302073653632\n",
      "train loss:0.05166510997839335\n",
      "train loss:0.11813613084114934\n",
      "train loss:0.10849900137381256\n",
      "train loss:0.04752157422971759\n",
      "train loss:0.07111106661788663\n",
      "train loss:0.35927841948081896\n",
      "train loss:0.2985288993284955\n",
      "train loss:0.1233304311187586\n",
      "train loss:0.11856716094063813\n",
      "train loss:0.29983943300541854\n",
      "train loss:0.09479525822035159\n",
      "train loss:0.19824684773190374\n",
      "train loss:0.05513783934557263\n",
      "train loss:0.11747124925383859\n",
      "train loss:0.0642653697649353\n",
      "train loss:0.279430811076045\n",
      "train loss:0.08158390336233254\n",
      "train loss:0.17700545370789925\n",
      "train loss:0.05731177512895454\n",
      "train loss:0.04548855411428745\n",
      "train loss:0.0288001500766094\n",
      "train loss:0.34793878178562804\n",
      "train loss:0.11043133653744139\n",
      "train loss:0.2505583240855676\n",
      "train loss:0.055951361299883304\n",
      "train loss:0.10298682266339165\n",
      "train loss:0.03275943501713018\n",
      "train loss:0.058369279055032794\n",
      "train loss:0.05919778972748304\n",
      "train loss:0.14336096545074345\n",
      "train loss:0.1515450593989239\n",
      "train loss:0.06986453139651223\n",
      "train loss:0.04831377733471297\n",
      "train loss:0.04997112654544002\n",
      "train loss:0.2603344555911913\n",
      "train loss:0.24913825218369146\n",
      "train loss:0.11514834731655645\n",
      "train loss:0.09300890126655992\n",
      "train loss:0.05516452273445565\n",
      "train loss:0.0996940939999827\n",
      "train loss:0.09558981683090459\n",
      "train loss:0.06342580275787696\n",
      "train loss:0.04446957347900948\n",
      "train loss:0.05698509866839374\n",
      "train loss:0.2602109295286205\n",
      "train loss:0.08671232815782887\n",
      "train loss:0.07898415666152481\n",
      "train loss:0.08231364539307873\n",
      "train loss:0.050700950755628485\n",
      "train loss:0.030387442642855415\n",
      "train loss:0.2409753117958637\n",
      "train loss:0.6097709776604123\n",
      "train loss:0.27286665985963504\n",
      "train loss:0.15291706702281935\n",
      "train loss:0.5032624014188869\n",
      "train loss:0.0874540980585533\n",
      "train loss:0.0168399243472738\n",
      "train loss:0.12382031663360668\n",
      "train loss:0.5280703127859909\n",
      "train loss:0.3326322999422033\n",
      "train loss:0.3817590772514671\n",
      "train loss:0.12293791904682815\n",
      "train loss:0.08822918523600409\n",
      "train loss:0.1366480543015479\n",
      "train loss:0.06815454090158907\n",
      "train loss:0.09083758480484928\n",
      "train loss:0.2029208906971072\n",
      "train loss:0.33691833966596013\n",
      "train loss:0.16491158086378005\n",
      "train loss:0.1220665697482073\n",
      "train loss:0.04454309175204969\n",
      "train loss:0.14524938845532662\n",
      "train loss:0.17655311935835333\n",
      "train loss:0.8992268868979109\n",
      "train loss:0.4424009637141117\n",
      "train loss:0.11495995747204368\n",
      "train loss:0.17066576098722236\n",
      "train loss:0.08417546397283016\n",
      "train loss:0.06343458900242624\n",
      "train loss:0.4974294983139533\n",
      "train loss:0.11381828313363807\n",
      "train loss:0.08546596479057858\n",
      "train loss:0.025141920059996332\n",
      "train loss:0.0762743029597868\n",
      "train loss:0.03457628519256839\n",
      "train loss:0.07899809455007553\n",
      "train loss:0.07134532166420637\n",
      "train loss:0.18764533591089466\n",
      "train loss:0.11407809849746957\n",
      "train loss:0.06812529377755167\n",
      "train loss:0.2080934035740894\n",
      "train loss:0.042960814827671515\n",
      "train loss:0.5149656655942092\n",
      "train loss:0.09829757826767284\n",
      "train loss:0.0470110568012667\n",
      "train loss:0.10065100530752943\n",
      "train loss:0.055161767859781594\n",
      "train loss:0.027315225219273716\n",
      "train loss:0.034413727622791115\n",
      "train loss:0.13440287874780904\n",
      "train loss:0.04937557369233368\n",
      "train loss:0.13597666061070718\n",
      "train loss:0.1244425201495736\n",
      "train loss:0.10429725312558044\n",
      "train loss:0.26917367554520366\n",
      "train loss:0.16487322233189936\n",
      "train loss:0.07706571012021632\n",
      "train loss:0.1023492318997194\n",
      "train loss:0.17376454649276277\n",
      "train loss:0.231254276354815\n",
      "train loss:0.059982560939264976\n",
      "train loss:0.3372223110365347\n",
      "train loss:0.09106755878208472\n",
      "train loss:0.156725659886074\n",
      "train loss:0.04028010217127557\n",
      "train loss:0.13216646662205225\n",
      "train loss:0.11233383333635304\n",
      "train loss:0.13137183854863616\n",
      "train loss:0.044429467723502564\n",
      "train loss:0.09906682090240237\n",
      "train loss:0.0810880689081265\n",
      "train loss:0.06971611566727533\n",
      "train loss:0.04243550108997882\n",
      "train loss:0.2663213875810737\n",
      "train loss:0.31064992107923983\n",
      "train loss:0.07083952240351453\n",
      "train loss:0.07221242798470415\n",
      "train loss:0.194430272376261\n",
      "train loss:0.054574957928577285\n",
      "train loss:0.03239034780496263\n",
      "train loss:0.23484346097397268\n",
      "train loss:0.04026934952316468\n",
      "train loss:0.11139363953942666\n",
      "train loss:0.21557996986203953\n",
      "train loss:0.2001786073705572\n",
      "train loss:0.22166574177667492\n",
      "train loss:0.022491477417491242\n",
      "train loss:0.12291462763830432\n",
      "train loss:0.08254571492976256\n",
      "train loss:0.04247320721407815\n",
      "train loss:0.11749217572925946\n",
      "train loss:0.2479778413252865\n",
      "train loss:0.11163717554836822\n",
      "train loss:0.11458838026329426\n",
      "train loss:0.07196702726837953\n",
      "train loss:0.11439956603458686\n",
      "train loss:0.28999453037863976\n",
      "train loss:0.05566276547371368\n",
      "train loss:0.16273713162531786\n",
      "train loss:0.03391979468379362\n",
      "train loss:0.21603593161950446\n",
      "train loss:0.12426843641357888\n",
      "train loss:0.06552346187761195\n",
      "train loss:0.05501630803747814\n",
      "train loss:0.1262189741095557\n",
      "train loss:0.0793583342890182\n",
      "train loss:0.1609853859975442\n",
      "train loss:0.11102720076053454\n",
      "train loss:0.11260571402092873\n",
      "train loss:0.09219323188998087\n",
      "train loss:0.05879628797349475\n",
      "train loss:0.04401227540369437\n",
      "train loss:0.035941090607588995\n",
      "train loss:0.06671058783078315\n",
      "train loss:0.19692263513761915\n",
      "train loss:0.09622762067814941\n",
      "train loss:0.05727283356636134\n",
      "train loss:0.1077466197875193\n",
      "train loss:0.029853561129543738\n",
      "train loss:0.08282026204673607\n",
      "train loss:0.0943687022395027\n",
      "train loss:0.20853586166825827\n",
      "train loss:0.061695339057897695\n",
      "train loss:0.038972878744984826\n",
      "train loss:0.03964575732642894\n",
      "train loss:0.1629723822497896\n",
      "train loss:0.13925600494237933\n",
      "train loss:0.14680853461256363\n",
      "train loss:0.09395482723921968\n",
      "train loss:0.03810229493251652\n",
      "train loss:0.08974392360850687\n",
      "train loss:0.07717527189137562\n",
      "train loss:0.04599090707498986\n",
      "train loss:0.4055683612669557\n",
      "train loss:0.09559603628756892\n",
      "train loss:0.02638390114197522\n",
      "train loss:0.49829936593172963\n",
      "train loss:0.023974902020704278\n",
      "train loss:0.11690098574434532\n",
      "train loss:0.19897518383587467\n",
      "train loss:0.11461266042967225\n",
      "train loss:0.14297057118669268\n",
      "train loss:0.03504419501045032\n",
      "train loss:0.2040868730357243\n",
      "train loss:0.3493982821977596\n",
      "train loss:0.10811980089819084\n",
      "train loss:0.14525700765027072\n",
      "train loss:0.04577006262947739\n",
      "train loss:0.14895151103148008\n",
      "train loss:0.10960853650858478\n",
      "train loss:0.07697887417633312\n",
      "train loss:0.18498356712140077\n",
      "train loss:0.34097050248544863\n",
      "train loss:0.2162610259148339\n",
      "train loss:0.3309671941752373\n",
      "train loss:0.04693428912399804\n",
      "train loss:0.08051934094901497\n",
      "train loss:0.08512004247628412\n",
      "train loss:0.203151327746024\n",
      "train loss:0.02017194193433451\n",
      "train loss:0.4513217464904733\n",
      "train loss:0.20036483156190688\n",
      "train loss:0.08194524312876991\n",
      "train loss:0.029239074669752358\n",
      "train loss:0.07452387092077523\n",
      "train loss:0.16635328757799067\n",
      "train loss:0.07326127618007396\n",
      "train loss:0.051691740928589025\n",
      "train loss:0.12141541150276415\n",
      "train loss:0.09021888970136861\n",
      "train loss:0.5640446711333927\n",
      "train loss:0.1637977599982832\n",
      "train loss:0.02476419985010206\n",
      "train loss:0.1874053260327851\n",
      "train loss:0.1864767480774387\n",
      "train loss:0.2197837854828984\n",
      "train loss:0.176270544600332\n",
      "train loss:0.33433214809208084\n",
      "train loss:0.06761978087568313\n",
      "train loss:0.09149289961016174\n",
      "train loss:0.10783011333778839\n",
      "train loss:0.23990435864375648\n",
      "train loss:0.08734652992014542\n",
      "train loss:0.043725505858257144\n",
      "train loss:0.05799286476982439\n",
      "train loss:0.11045707465569506\n",
      "train loss:0.20157209036849338\n",
      "train loss:0.05661101869561175\n",
      "train loss:0.4014680030958018\n",
      "train loss:0.039492819677949716\n",
      "train loss:0.09905089440079051\n",
      "train loss:0.1332630637340124\n",
      "train loss:0.09575975907070805\n",
      "train loss:0.03268161819876832\n",
      "train loss:0.11946783314688976\n",
      "train loss:0.1950741495065771\n",
      "train loss:0.12308715505783124\n",
      "train loss:0.04705218248876255\n",
      "train loss:0.16939309053704654\n",
      "train loss:0.05544083652490109\n",
      "train loss:0.04225740509322919\n",
      "train loss:0.11273505970531623\n",
      "train loss:0.04156293668472989\n",
      "train loss:0.4392600949807799\n",
      "train loss:0.12453227656254144\n",
      "train loss:0.08841326216340045\n",
      "train loss:0.04362806396160569\n",
      "train loss:0.4912408686803393\n",
      "train loss:0.08672469430778511\n",
      "train loss:0.27493976107185225\n",
      "train loss:0.4483958095184923\n",
      "train loss:0.034508720511862746\n",
      "train loss:0.2575414651757834\n",
      "train loss:0.33484249218628537\n",
      "train loss:0.08599804611767495\n",
      "train loss:0.5264305184530522\n",
      "train loss:0.028551484400150852\n",
      "train loss:0.05698786767771853\n",
      "train loss:0.19548510598584604\n",
      "train loss:0.5294266686602525\n",
      "train loss:0.28011102189429415\n",
      "train loss:0.424105995777475\n",
      "train loss:0.2537983966745614\n",
      "train loss:0.10811630155931419\n",
      "train loss:0.2768956000133152\n",
      "train loss:0.24310448876312024\n",
      "train loss:0.1502515882418663\n",
      "train loss:0.04631234362030549\n",
      "train loss:0.05674819652875262\n",
      "train loss:0.16449974774101128\n",
      "train loss:0.22014486176169404\n",
      "train loss:0.2941965747590404\n",
      "train loss:0.010064281951882002\n",
      "train loss:0.31306005004772675\n",
      "train loss:0.6825810271241373\n",
      "train loss:0.16813525988935885\n",
      "train loss:0.28418785584486994\n",
      "train loss:0.28283178839526235\n",
      "train loss:0.3382412864094833\n",
      "train loss:0.065560371861702\n",
      "train loss:0.049053167478338475\n",
      "train loss:0.031637125990438095\n",
      "train loss:0.11381687684519795\n",
      "train loss:0.07088518782127827\n",
      "train loss:0.03910782788867296\n",
      "train loss:0.10204744655198292\n",
      "train loss:0.0936032200330342\n",
      "train loss:0.04533147795009798\n",
      "train loss:0.10911042473083371\n",
      "train loss:0.06966069373904069\n",
      "train loss:0.2695449145530847\n",
      "train loss:0.06516038787721871\n",
      "train loss:0.25372377166289367\n",
      "train loss:0.5032228867488007\n",
      "train loss:0.14931958149426822\n",
      "train loss:0.18590319437180142\n",
      "train loss:0.07120002451423021\n",
      "train loss:0.10604108648311972\n",
      "train loss:0.10432652506381627\n",
      "train loss:0.09958850139054952\n",
      "train loss:0.04961728680618821\n",
      "train loss:0.03771961290071178\n",
      "train loss:0.09933712452958252\n",
      "train loss:0.1249662480222421\n",
      "train loss:0.22379953124238192\n",
      "train loss:0.04754020018904538\n",
      "train loss:0.055409014585570966\n",
      "train loss:0.27376309793229625\n",
      "train loss:0.08682653516454701\n",
      "train loss:0.17663607418184765\n",
      "train loss:0.33215962339211885\n",
      "train loss:0.41666238150865104\n",
      "train loss:0.07202326356822182\n",
      "train loss:0.16754473300368877\n",
      "train loss:0.21936380166783853\n",
      "train loss:0.06024613784964376\n",
      "train loss:0.06161872125581791\n",
      "train loss:0.12237828704357995\n",
      "train loss:0.08641157090715312\n",
      "train loss:0.11477397885098843\n",
      "train loss:0.03739857248898099\n",
      "train loss:0.328399399114222\n",
      "train loss:0.3267556206438619\n",
      "train loss:0.4192857265496329\n",
      "train loss:0.15195151366546888\n",
      "train loss:0.48154705593513325\n",
      "train loss:0.032657875706678066\n",
      "train loss:0.3648801354804385\n",
      "train loss:0.11148257379533238\n",
      "train loss:0.4361082376306187\n",
      "train loss:0.1515895062033052\n",
      "train loss:0.2022334144261227\n",
      "train loss:0.19157121225799953\n",
      "train loss:0.3694198324654558\n",
      "train loss:0.1842235760331358\n",
      "train loss:0.058175466100223616\n",
      "train loss:0.1365513037689546\n",
      "train loss:0.05337936243628335\n",
      "train loss:0.04324680291060247\n",
      "train loss:0.1291051171984735\n",
      "train loss:0.09657315736216265\n",
      "train loss:0.04689158954826307\n",
      "train loss:0.15196487032292672\n",
      "train loss:0.171603917658846\n",
      "train loss:0.05100692026208073\n",
      "train loss:0.24320861581272446\n",
      "train loss:0.07798992026812987\n",
      "train loss:0.05891015123803019\n",
      "train loss:0.0551278856940933\n",
      "train loss:0.12266615099380605\n",
      "train loss:0.3384367621984871\n",
      "train loss:0.14929108006362463\n",
      "train loss:0.08313454642500553\n",
      "train loss:0.13524121329912708\n",
      "train loss:0.08089460393703261\n",
      "train loss:0.11122785057288745\n",
      "train loss:0.1239267655880481\n",
      "train loss:0.13725485186365993\n",
      "train loss:0.11593854468305462\n",
      "train loss:0.09775531629218433\n",
      "train loss:0.14623408198414717\n",
      "train loss:0.25264182403115903\n",
      "train loss:0.5093185277763994\n",
      "train loss:0.10440285141706024\n",
      "train loss:0.11068962368411128\n",
      "train loss:0.39301159507697564\n",
      "train loss:0.03987524158727983\n",
      "train loss:0.06958294601179915\n",
      "train loss:0.17078048954467706\n",
      "train loss:0.0392676848772129\n",
      "train loss:0.18665144017388616\n",
      "train loss:0.08639711479350862\n",
      "train loss:0.17750784511147685\n",
      "train loss:0.03475636840660836\n",
      "train loss:0.07477259234826664\n",
      "train loss:0.0943318072341798\n",
      "train loss:0.22693672816559263\n",
      "train loss:0.06732916807393728\n",
      "train loss:0.047665532566708545\n",
      "train loss:0.022841434687188196\n",
      "train loss:0.1243813225978505\n",
      "train loss:0.09132255811952941\n",
      "train loss:0.04990328798885958\n",
      "train loss:0.07633048399014215\n",
      "train loss:0.07879886968875024\n",
      "train loss:0.047498017477228205\n",
      "train loss:0.20859356458969305\n",
      "train loss:0.1830530574301111\n",
      "train loss:0.26028422146398256\n",
      "train loss:0.04622978290057593\n",
      "train loss:0.1756214721508693\n",
      "train loss:0.08533463173738362\n",
      "train loss:0.06239425242826063\n",
      "train loss:0.08180806303810906\n",
      "train loss:0.06548873397931645\n",
      "train loss:0.1622866301087266\n",
      "train loss:0.06354723688805473\n",
      "train loss:0.1272785078489682\n",
      "train loss:0.14134895920836643\n",
      "train loss:0.10996249158591567\n",
      "train loss:0.08253534598427645\n",
      "train loss:0.18223365214665016\n",
      "train loss:0.2284294264931109\n",
      "train loss:0.04282253989550087\n",
      "train loss:0.04252574746635419\n",
      "train loss:0.15439687881523034\n",
      "train loss:0.08436300038691938\n",
      "train loss:0.04047566457602337\n",
      "train loss:0.2464948642899629\n",
      "train loss:0.18362148529595462\n",
      "train loss:0.06647307609991615\n",
      "train loss:0.02872637820959686\n",
      "train loss:0.24722725739481188\n",
      "train loss:0.1294583218974185\n",
      "train loss:0.04520471928529808\n",
      "train loss:0.05455441559370594\n",
      "train loss:0.13812070497729045\n",
      "train loss:0.4281931698346664\n",
      "train loss:0.1890428583861364\n",
      "train loss:0.1496474573911762\n",
      "train loss:0.19044791846584702\n",
      "train loss:0.5990865412203836\n",
      "train loss:0.08485285397431318\n",
      "train loss:0.04224719827690815\n",
      "train loss:0.11762044523922233\n",
      "train loss:0.12419052774563656\n",
      "train loss:0.20198567539744694\n",
      "train loss:0.08040170671348845\n",
      "train loss:0.23184241768534244\n",
      "train loss:0.07889931321112466\n",
      "train loss:0.37893172077507853\n",
      "train loss:0.31322906855069577\n",
      "train loss:0.17833082071832862\n",
      "train loss:0.2414013965708502\n",
      "train loss:0.12412067734177831\n",
      "train loss:0.21592526966426961\n",
      "train loss:0.10875613185429069\n",
      "train loss:0.3597530100145145\n",
      "train loss:0.09245534170629102\n",
      "train loss:0.07803270168881447\n",
      "train loss:0.10928925932415662\n",
      "train loss:0.061632317656457325\n",
      "train loss:0.019722158421041032\n",
      "train loss:0.07667966355498626\n",
      "train loss:0.36504647308950333\n",
      "train loss:0.11993646510636657\n",
      "train loss:0.10281310213900363\n",
      "train loss:0.2437396117174564\n",
      "train loss:0.15733266457193254\n",
      "train loss:0.058364812857558686\n",
      "train loss:0.09056742592384398\n",
      "train loss:0.17615035233648657\n",
      "train loss:0.3736024168640074\n",
      "train loss:0.29665871020563417\n",
      "train loss:0.13327439152207135\n",
      "train loss:0.05334425920373691\n",
      "train loss:0.03678346848484653\n",
      "train loss:0.24720273135786677\n",
      "train loss:0.11836723608530142\n",
      "train loss:0.09852208149687645\n",
      "train loss:0.0795878103083029\n",
      "train loss:0.33368986401110756\n",
      "train loss:0.050096159336731685\n",
      "train loss:0.1762699344270016\n",
      "train loss:0.06268982898164158\n",
      "train loss:0.10805889528112277\n",
      "train loss:0.2676711653191073\n",
      "train loss:0.18349525193398872\n",
      "train loss:0.14659553871658088\n",
      "train loss:0.1702342062693431\n",
      "train loss:0.1480783602537702\n",
      "train loss:0.029919562916566143\n",
      "train loss:0.11290582891320097\n",
      "train loss:0.1620717380451428\n",
      "train loss:0.06012165095275426\n",
      "train loss:0.06709213848127327\n",
      "train loss:0.3745955113432955\n",
      "train loss:0.035383624797847284\n",
      "train loss:0.10267354165109244\n",
      "train loss:0.19772452333031298\n",
      "train loss:0.06819552952829244\n",
      "train loss:0.07509660113260881\n",
      "train loss:0.1437537781881588\n",
      "train loss:0.17750179878769493\n",
      "train loss:0.479486343258909\n",
      "train loss:0.06468000101176555\n",
      "train loss:0.1197861669506266\n",
      "train loss:0.12389184337278263\n",
      "train loss:0.21253315955698435\n",
      "train loss:0.2696340645462884\n",
      "train loss:0.03602481108133258\n",
      "train loss:0.04840095661685858\n",
      "train loss:0.013497167350612271\n",
      "train loss:0.0717662536437208\n",
      "train loss:0.13579123609363833\n",
      "train loss:0.16487536984526907\n",
      "train loss:0.08118862377369769\n",
      "train loss:0.17354048297662633\n",
      "train loss:0.12908863229043696\n",
      "train loss:0.036109450865254654\n",
      "train loss:0.6314691546900822\n",
      "train loss:0.2087317985698855\n",
      "train loss:0.3376156962325652\n",
      "train loss:0.13940554380650397\n",
      "train loss:0.0795763183746245\n",
      "train loss:0.09356816110305903\n",
      "train loss:0.40456345465398447\n",
      "train loss:0.22656412749111152\n",
      "train loss:0.4806214800904397\n",
      "train loss:0.08177189371262769\n",
      "train loss:0.13366235434517498\n",
      "train loss:0.15211052881378492\n",
      "train loss:0.07859953360762273\n",
      "train loss:0.15841334428904913\n",
      "train loss:0.2865545239637397\n",
      "train loss:0.45689137988878015\n",
      "train loss:0.04718328869452147\n",
      "train loss:0.16290067486161103\n",
      "train loss:0.12209929974301607\n",
      "train loss:0.06817330235317916\n",
      "train loss:0.020913397365177937\n",
      "train loss:0.1330342628040309\n",
      "train loss:0.10255345873898043\n",
      "train loss:0.1952351487495003\n",
      "train loss:0.16694122191874938\n",
      "train loss:0.4650252012027849\n",
      "train loss:0.08776613752504739\n",
      "train loss:0.23497340752965204\n",
      "train loss:0.03193786812949782\n",
      "train loss:0.07718635040450539\n",
      "train loss:0.054129228931588794\n",
      "train loss:0.18256605795020858\n",
      "train loss:0.15132630330606134\n",
      "train loss:0.1567954300266376\n",
      "train loss:0.05118201868915677\n",
      "train loss:0.22214379344307061\n",
      "train loss:0.10227083595636059\n",
      "train loss:0.17908440447417148\n",
      "train loss:0.1891657959200725\n",
      "train loss:0.3271137978885679\n",
      "train loss:0.21119302471792872\n",
      "train loss:0.15254496955506608\n",
      "train loss:0.02181442262497771\n",
      "train loss:0.0461024648224459\n",
      "train loss:0.19942462900349872\n",
      "train loss:0.17865729916464845\n",
      "train loss:0.08824514587134659\n",
      "train loss:0.0348127832345041\n",
      "train loss:0.23825606782980757\n",
      "train loss:0.11762288825410341\n",
      "train loss:0.06534976599695816\n",
      "train loss:0.549556803014574\n",
      "train loss:0.6415527715837158\n",
      "train loss:0.09765500733079358\n",
      "train loss:0.08807889816371242\n",
      "train loss:0.11346285606111221\n",
      "train loss:0.2087127807710156\n",
      "train loss:0.3905622799481331\n",
      "train loss:0.10583670270325056\n",
      "train loss:0.053794792397463743\n",
      "train loss:0.2702985059492486\n",
      "train loss:0.2886776196748332\n",
      "train loss:0.13569260946065215\n",
      "train loss:0.3763038032225473\n",
      "train loss:0.13523426410593625\n",
      "train loss:0.16483952324979678\n",
      "train loss:0.17588282198956873\n",
      "train loss:0.06288053295766592\n",
      "train loss:0.07736619315973388\n",
      "train loss:0.02933048141704183\n",
      "train loss:0.03675258107079449\n",
      "train loss:0.11226449027613195\n",
      "train loss:0.19549250539682483\n",
      "train loss:0.058659619410420735\n",
      "train loss:0.1262883538556385\n",
      "train loss:0.05628264460518823\n",
      "train loss:0.05403680666029753\n",
      "train loss:0.026161377516025694\n",
      "train loss:0.14969453401863841\n",
      "train loss:0.0709455437494638\n",
      "train loss:0.1346169398775095\n",
      "train loss:0.11360801730882723\n",
      "train loss:0.05114879634675203\n",
      "train loss:0.10062684306191035\n",
      "train loss:0.1516839152383299\n",
      "train loss:0.07775407533844973\n",
      "train loss:0.09858749408905076\n",
      "train loss:0.2203918126844791\n",
      "train loss:0.08823907326054281\n",
      "train loss:0.2575232088067326\n",
      "train loss:0.04909731422526464\n",
      "train loss:0.011203387098149633\n",
      "train loss:0.04055226957459095\n",
      "train loss:0.05634182848512413\n",
      "train loss:0.05013844320973132\n",
      "train loss:0.07968413288163154\n",
      "train loss:0.3339429114102535\n",
      "train loss:0.07179760339544033\n",
      "train loss:0.2828325843568087\n",
      "train loss:0.11380191451015777\n",
      "train loss:0.04829550637413238\n",
      "train loss:0.026906117194237758\n",
      "train loss:0.08477199073426464\n",
      "train loss:0.15839736641461558\n",
      "train loss:0.33265079637042255\n",
      "train loss:0.01918613646100599\n",
      "train loss:0.03739637998933646\n",
      "train loss:0.36220427080127643\n",
      "train loss:0.058745374331295216\n",
      "train loss:0.0470653438244737\n",
      "train loss:0.2529834929378589\n",
      "train loss:0.037066938881190954\n",
      "train loss:0.051930103533012806\n",
      "train loss:0.174949077523386\n",
      "train loss:0.2789027367707641\n",
      "train loss:0.22378812240296392\n",
      "train loss:0.256290513471326\n",
      "train loss:0.20159222438980093\n",
      "train loss:0.06640362211495081\n",
      "train loss:0.05006011743534459\n",
      "train loss:0.09763549919778652\n",
      "train loss:0.033139587179509024\n",
      "train loss:0.10658759063110781\n",
      "train loss:0.0365962491856231\n",
      "train loss:0.1347271606726191\n",
      "train loss:0.10636832931985915\n",
      "train loss:0.22757534305261312\n",
      "train loss:0.33839136051358015\n",
      "train loss:0.23811835448884022\n",
      "train loss:0.1497058115179327\n",
      "train loss:0.06024360768752833\n",
      "train loss:0.3472516834370909\n",
      "train loss:0.7302402600457261\n",
      "train loss:0.17786392079879051\n",
      "train loss:0.11572189869026041\n",
      "train loss:0.05943629760717892\n",
      "train loss:0.04847565070381116\n",
      "train loss:0.09736007397282437\n",
      "train loss:0.09698038276955483\n",
      "train loss:0.09881517033147078\n",
      "train loss:0.11778610259870825\n",
      "train loss:0.036397028383206236\n",
      "train loss:0.07952679434096718\n",
      "train loss:0.28983315945818117\n",
      "train loss:0.25278293650200206\n",
      "train loss:0.13825302174518891\n",
      "train loss:0.14174697210573067\n",
      "train loss:0.13076319644938889\n",
      "train loss:0.2881855782074463\n",
      "train loss:0.08840532642005701\n",
      "train loss:0.024860000363093795\n",
      "train loss:0.032391265647006284\n",
      "train loss:0.1618041211406168\n",
      "train loss:0.08256623001931299\n",
      "train loss:0.09343076574366027\n",
      "train loss:0.11868583614167384\n",
      "train loss:0.24744352023536081\n",
      "train loss:0.14312766022932333\n",
      "train loss:0.07245073938795835\n",
      "train loss:0.18837896876723004\n",
      "train loss:0.1081282947395743\n",
      "train loss:0.045531076561158956\n",
      "train loss:0.035371464655356226\n",
      "train loss:0.019241755889354693\n",
      "train loss:0.044500710858794534\n",
      "train loss:0.10016926422433206\n",
      "train loss:0.09751212927545175\n",
      "train loss:0.20443184603301867\n",
      "train loss:0.10536086566492289\n",
      "train loss:0.1134703982289886\n",
      "train loss:0.1749976542470572\n",
      "train loss:0.108091069168281\n",
      "train loss:0.03580759414786269\n",
      "train loss:0.02033566730305811\n",
      "train loss:0.19095495540301044\n",
      "train loss:0.04314489925600318\n",
      "train loss:0.18069537657371437\n",
      "train loss:0.03398479880816654\n",
      "train loss:0.17226743017475682\n",
      "train loss:0.07143347405773794\n",
      "train loss:0.10449513099083714\n",
      "train loss:0.07239325847204463\n",
      "train loss:0.25236142227860137\n",
      "train loss:0.11119052081078173\n",
      "train loss:0.515676216839827\n",
      "train loss:0.12592054612884707\n",
      "train loss:0.14456035950580032\n",
      "train loss:0.15008822066024982\n",
      "train loss:0.06359327857672951\n",
      "train loss:0.1346412603902079\n",
      "train loss:0.10335387027231924\n",
      "train loss:0.19785806824327581\n",
      "train loss:0.1327781744127556\n",
      "train loss:0.12269514820084963\n",
      "train loss:0.19724890500757933\n",
      "train loss:0.029629032618006364\n",
      "train loss:0.09310014218085451\n",
      "train loss:0.1432103608091252\n",
      "train loss:0.07099822828600087\n",
      "train loss:0.03339741744364817\n",
      "train loss:0.17009793662174788\n",
      "train loss:0.15822519091245119\n",
      "train loss:0.03183817362720426\n",
      "train loss:0.05634534370798136\n",
      "train loss:0.056832833139866666\n",
      "train loss:0.21596431858408197\n",
      "train loss:0.14616330561252372\n",
      "train loss:0.1162396492032039\n",
      "train loss:0.16189167835008364\n",
      "train loss:0.22092138653740728\n",
      "train loss:0.05797521042883744\n",
      "train loss:0.46077525129835617\n",
      "train loss:0.09105836519277702\n",
      "train loss:0.09371451124306962\n",
      "train loss:0.3364519847623636\n",
      "train loss:0.12657311225803022\n",
      "train loss:0.053752644544108816\n",
      "train loss:0.15914996918305183\n",
      "train loss:0.057100883316066886\n",
      "train loss:0.10280170396280275\n",
      "train loss:0.07221034975655588\n",
      "train loss:0.03486902256469768\n",
      "train loss:0.06369207338359201\n",
      "train loss:0.5998536095176947\n",
      "train loss:0.12836796859407731\n",
      "train loss:0.0317366799108903\n",
      "train loss:0.2322819929682898\n",
      "train loss:0.06262074040291105\n",
      "train loss:0.11424190138056298\n",
      "train loss:0.17766216544777047\n",
      "train loss:0.13659945759459993\n",
      "train loss:0.13561345170550626\n",
      "train loss:0.1552768540744996\n",
      "train loss:0.06884433079217314\n",
      "train loss:0.08090801795401004\n",
      "train loss:0.19505715398425647\n",
      "train loss:0.13095002125941527\n",
      "train loss:0.07646716854366539\n",
      "train loss:0.22410720004728393\n",
      "train loss:0.0795964541840957\n",
      "train loss:0.04613365545540966\n",
      "train loss:0.32568535382471075\n",
      "train loss:0.40333288359069674\n",
      "train loss:0.10088669489197713\n",
      "train loss:0.25348416577438126\n",
      "train loss:0.11801628992424405\n",
      "train loss:0.1000378123437296\n",
      "train loss:0.20001214208646884\n",
      "train loss:0.07573650212944365\n",
      "train loss:0.22175194840058188\n",
      "train loss:0.06825824888490134\n",
      "train loss:0.11033899074164416\n",
      "train loss:0.1882395995158237\n",
      "train loss:0.1780671254440414\n",
      "train loss:0.023014479883144548\n",
      "train loss:0.26343915170355653\n",
      "train loss:0.15473782238716857\n",
      "train loss:0.24017537744157028\n",
      "train loss:0.5484700289690315\n",
      "train loss:0.04169779750730163\n",
      "train loss:0.7151155952717401\n",
      "train loss:0.0448748972688663\n",
      "train loss:0.1111080332676811\n",
      "train loss:0.24294236708219513\n",
      "train loss:0.0888433394410468\n",
      "train loss:0.19437064978754484\n",
      "train loss:0.03403083326056048\n",
      "train loss:0.0484870637331517\n",
      "train loss:0.047361357644338596\n",
      "train loss:0.14456581176997305\n",
      "train loss:0.03912898950157326\n",
      "train loss:0.06441648867368281\n",
      "train loss:0.2007156529995041\n",
      "train loss:0.12661187353964776\n",
      "train loss:0.24952164585025918\n",
      "train loss:0.10301639171573461\n",
      "train loss:0.12544927581172735\n",
      "train loss:0.35833163841223736\n",
      "train loss:0.18249319770494038\n",
      "train loss:0.15432594444848924\n",
      "train loss:0.17321495892885588\n",
      "train loss:0.13896237750691215\n",
      "train loss:0.05639719447147299\n",
      "train loss:0.18165258454654992\n",
      "train loss:0.16533282256750798\n",
      "train loss:0.03790313909162456\n",
      "train loss:0.04220913433202669\n",
      "train loss:0.09278224388097754\n",
      "train loss:0.0577311386418871\n",
      "train loss:0.21266438085305622\n",
      "train loss:0.057529073055470045\n",
      "train loss:0.04562415760693311\n",
      "train loss:0.04240641811895313\n",
      "train loss:0.010273280368173631\n",
      "train loss:0.052024915398598076\n",
      "train loss:0.04110353572852565\n",
      "train loss:0.07418413404798405\n",
      "train loss:0.15577728664293838\n",
      "train loss:0.2258429391797323\n",
      "train loss:0.19597109221554126\n",
      "train loss:0.03615042563389318\n",
      "train loss:0.17627606473078083\n",
      "train loss:0.10256905437796013\n",
      "train loss:0.04864385226370049\n",
      "train loss:0.037760545219252685\n",
      "train loss:0.05381463983723772\n",
      "train loss:0.09748468778913848\n",
      "train loss:0.15032942476139766\n",
      "train loss:0.06867961075020161\n",
      "train loss:0.09948571109831664\n",
      "train loss:0.026622927194501662\n",
      "train loss:0.12395854212600481\n",
      "train loss:0.17971456963840204\n",
      "train loss:0.0985557375510152\n",
      "train loss:0.09916997470575006\n",
      "train loss:0.06209895963204626\n",
      "train loss:0.05940071370799941\n",
      "train loss:0.10891114357684534\n",
      "train loss:0.09846269743866345\n",
      "train loss:0.04009926302205097\n",
      "train loss:0.033301217678165705\n",
      "train loss:0.2123257683819189\n",
      "train loss:0.2888178788421964\n",
      "train loss:0.03718641104479044\n",
      "train loss:0.2231304834003448\n",
      "train loss:0.08228585783031142\n",
      "train loss:0.25558806151706126\n",
      "train loss:0.2416626343616661\n",
      "train loss:0.157869675681017\n",
      "train loss:0.035955811822892966\n",
      "train loss:0.14509240813763014\n",
      "train loss:0.10926727064593689\n",
      "train loss:0.07302081368798938\n",
      "train loss:0.08504489918963662\n",
      "train loss:0.08045997137601146\n",
      "train loss:0.36002535993446155\n",
      "train loss:0.03522053279129541\n",
      "train loss:0.02033499010994978\n",
      "train loss:0.045559513859908696\n",
      "train loss:0.12167695220288394\n",
      "train loss:0.09910201265675607\n",
      "train loss:0.014052695290103353\n",
      "train loss:0.04103183879045502\n",
      "train loss:0.13943966461308613\n",
      "train loss:0.06520358174971198\n",
      "train loss:0.1820507813375808\n",
      "train loss:0.17279965395496538\n",
      "train loss:0.05997095930586786\n",
      "train loss:0.09643629620304765\n",
      "train loss:0.03928030077854415\n",
      "train loss:0.06190669482382151\n",
      "train loss:0.1246865873757092\n",
      "train loss:0.049707701496723114\n",
      "train loss:0.1493916459386553\n",
      "train loss:0.2138391786772169\n",
      "train loss:0.11875163301949268\n",
      "train loss:0.11831585766054142\n",
      "train loss:0.029728479443231953\n",
      "train loss:0.08913766514910768\n",
      "train loss:0.19755348393398953\n",
      "train loss:0.038077905919389164\n",
      "train loss:0.020818736008817132\n",
      "train loss:0.09342137032209635\n",
      "train loss:0.05370192938895545\n",
      "train loss:0.4816076302223651\n",
      "train loss:0.11559656898713123\n",
      "train loss:0.1326314635053341\n",
      "train loss:0.0677347744878383\n",
      "train loss:0.5288124941717185\n",
      "train loss:0.0291532573946911\n",
      "train loss:0.11881371177094929\n",
      "train loss:0.06918259794568205\n",
      "train loss:0.1708471060781084\n",
      "train loss:0.03834414982791106\n",
      "train loss:0.16099989611461896\n",
      "train loss:0.07669753242981428\n",
      "train loss:0.10316122640686404\n",
      "train loss:0.06856789069397713\n",
      "train loss:0.08154960325445149\n",
      "train loss:0.19876532914324946\n",
      "train loss:0.058008614907131384\n",
      "train loss:0.06334313088380122\n",
      "train loss:0.34880299557487854\n",
      "train loss:0.19061039557888015\n",
      "train loss:0.0917082743477903\n",
      "train loss:0.060623282683192434\n",
      "train loss:0.07865528760514168\n",
      "train loss:0.018777946586179265\n",
      "train loss:0.06409680092557218\n",
      "train loss:0.13240106892442297\n",
      "train loss:0.14954836014527756\n",
      "train loss:0.09442914968127286\n",
      "train loss:0.15854851904937262\n",
      "train loss:0.20330420119039846\n",
      "train loss:0.1241640977715667\n",
      "train loss:0.061247377853421055\n",
      "train loss:0.1844446675609139\n",
      "train loss:0.07491089139125226\n",
      "train loss:0.03223052854683425\n",
      "train loss:0.12847601059444977\n",
      "train loss:0.10745913803068027\n",
      "train loss:0.18673904951919026\n",
      "train loss:0.08981407036785005\n",
      "train loss:0.13780894038507505\n",
      "train loss:0.23250193000841243\n",
      "train loss:0.12305561080216731\n",
      "train loss:0.08162496853461142\n",
      "train loss:0.13058444752390833\n",
      "train loss:0.11089051985010877\n",
      "train loss:0.0769765419394695\n",
      "train loss:0.20036680940880294\n",
      "train loss:0.1475036235550115\n",
      "train loss:0.3510773917012362\n",
      "train loss:0.1990116072427297\n",
      "train loss:0.052021020853354596\n",
      "train loss:0.10675504506866648\n",
      "train loss:0.23328748931950133\n",
      "train loss:0.19266110489494936\n",
      "train loss:0.5806538572532571\n",
      "train loss:0.04169381492268712\n",
      "train loss:0.09720293065176923\n",
      "train loss:0.0790531614099774\n",
      "train loss:0.03353921999872451\n",
      "train loss:0.05603686543264473\n",
      "train loss:0.26683549267130807\n",
      "train loss:0.2044240486770839\n",
      "train loss:0.16571276378609848\n",
      "train loss:0.13283442078711824\n",
      "train loss:0.4165270521919623\n",
      "train loss:0.03659500933845383\n",
      "train loss:0.11144083409540723\n",
      "train loss:0.2683561618010136\n",
      "train loss:0.07698671098830132\n",
      "train loss:0.08577697562179058\n",
      "train loss:0.04088898001574051\n",
      "train loss:0.02420669362059709\n",
      "train loss:0.23779657263396367\n",
      "train loss:0.06666140715795416\n",
      "train loss:0.11819386574043672\n",
      "train loss:0.0648186297586687\n",
      "train loss:0.1202453977880337\n",
      "train loss:0.25701774355857476\n",
      "train loss:0.34795526415750233\n",
      "train loss:0.2502629205769751\n",
      "train loss:0.07263618895552607\n",
      "train loss:0.038593263823980796\n",
      "train loss:0.05542197398911164\n",
      "train loss:0.11854453747416922\n",
      "train loss:0.0825475681593945\n",
      "train loss:0.02997295014548483\n",
      "train loss:0.15110848751933784\n",
      "train loss:0.15973564853899588\n",
      "train loss:0.1359836468050648\n",
      "train loss:0.09152518239448518\n",
      "train loss:0.07433721938856638\n",
      "train loss:0.05076545150456407\n",
      "train loss:0.06736359557747558\n",
      "train loss:0.04524052124111262\n",
      "train loss:0.0460009845115942\n",
      "train loss:0.08103011015625147\n",
      "train loss:0.13886845304525558\n",
      "train loss:0.6499235110039019\n",
      "train loss:0.1229474186775286\n",
      "train loss:0.2550421351416954\n",
      "train loss:0.08366956148516962\n",
      "train loss:0.11234887892528377\n",
      "train loss:0.060083989389979636\n",
      "train loss:0.1339819232222242\n",
      "train loss:0.09248201417279783\n",
      "train loss:0.07046812518506693\n",
      "train loss:0.07580498369486544\n",
      "train loss:0.3931588893113276\n",
      "train loss:0.08782164701443551\n",
      "train loss:0.06417345790828043\n",
      "train loss:0.027437988065677356\n",
      "train loss:0.019684846612670337\n",
      "train loss:0.274188608140681\n",
      "train loss:0.06600181091736337\n",
      "train loss:0.25375375538059014\n",
      "train loss:0.2712730687690338\n",
      "train loss:0.03351285123374313\n",
      "train loss:0.04851741694040117\n",
      "train loss:0.05240716490584704\n",
      "train loss:0.37550447906536627\n",
      "train loss:0.03514535434081502\n",
      "train loss:0.025535462733731796\n",
      "train loss:0.438944785280279\n",
      "train loss:0.09148907280828261\n",
      "train loss:0.06918433082194972\n",
      "train loss:0.10291336123603975\n",
      "train loss:0.031680420986860316\n",
      "train loss:0.06044681628542711\n",
      "train loss:0.04061731597693988\n",
      "train loss:0.17436631946869696\n",
      "train loss:0.09879403899796053\n",
      "train loss:0.026584295479776834\n",
      "train loss:0.083215855480194\n",
      "train loss:0.07615339857485298\n",
      "train loss:0.09889007189223685\n",
      "train loss:0.2133352940385673\n",
      "train loss:0.036856635007149766\n",
      "train loss:0.10937496782185725\n",
      "train loss:0.08137385638905842\n",
      "train loss:0.12373924184375827\n",
      "train loss:0.09201964746739821\n",
      "train loss:0.06878275863952799\n",
      "train loss:0.25184650236420103\n",
      "train loss:0.0596044819667428\n",
      "train loss:0.12400075676788389\n",
      "train loss:0.09056589861259663\n",
      "train loss:0.14583812474313324\n",
      "train loss:0.4921377243055811\n",
      "train loss:0.06691737391520722\n",
      "train loss:0.1730814271919094\n",
      "train loss:0.047526975186153425\n",
      "train loss:0.1469325596496179\n",
      "train loss:0.2595406874316425\n",
      "train loss:0.1379258831684079\n",
      "train loss:0.1306688831499197\n",
      "train loss:0.09478762373882518\n",
      "train loss:0.37444373694522826\n",
      "train loss:0.04496481953522711\n",
      "train loss:0.07127697125390448\n",
      "train loss:0.1592367294980489\n",
      "train loss:0.154725280654893\n",
      "train loss:0.12046777306648206\n",
      "train loss:0.016515003553900724\n",
      "train loss:0.05037978270940714\n",
      "train loss:0.11656285149773368\n",
      "train loss:0.055830635979282756\n",
      "train loss:0.07706520039328327\n",
      "train loss:0.06757197506113952\n",
      "train loss:0.20242092208130508\n",
      "train loss:0.07187632976626596\n",
      "train loss:0.1795866455030309\n",
      "train loss:0.05698763230841736\n",
      "train loss:0.04886063092908088\n",
      "train loss:0.3025110969489383\n",
      "train loss:0.1059660544547493\n",
      "train loss:0.008034903510207447\n",
      "train loss:0.07960150677585145\n",
      "train loss:0.07164557477504649\n",
      "train loss:0.023411172191433172\n",
      "train loss:0.06757406757428774\n",
      "train loss:0.03952191346578826\n",
      "train loss:0.26997294570566344\n",
      "train loss:0.3766788095919074\n",
      "train loss:0.052928524556998896\n",
      "train loss:0.07399078990993756\n",
      "train loss:0.021355953992574245\n",
      "train loss:0.07204950568495473\n",
      "train loss:0.11494329731935005\n",
      "train loss:0.050740837649355\n",
      "train loss:0.08679442979594552\n",
      "train loss:0.05137688419684871\n",
      "train loss:0.032961364202307816\n",
      "train loss:0.07323209565757605\n",
      "train loss:0.08083268609878166\n",
      "train loss:0.1442238516441659\n",
      "train loss:0.0165018426735131\n",
      "train loss:0.08732590341830199\n",
      "train loss:0.06797851185141948\n",
      "train loss:0.04577717264997256\n",
      "train loss:0.21664383491303119\n",
      "train loss:0.17957520993154014\n",
      "train loss:0.13672682563818295\n",
      "train loss:0.03414871313602675\n",
      "train loss:0.0608681308948083\n",
      "train loss:0.031892785043189466\n",
      "train loss:0.071192943835516\n",
      "train loss:0.23043455231867602\n",
      "train loss:0.33928713396029775\n",
      "train loss:0.10545917564067137\n",
      "train loss:0.2630490156777434\n",
      "train loss:0.06385697652029468\n",
      "train loss:0.045229680513175885\n",
      "train loss:0.24846726582637274\n",
      "train loss:0.14066786448391133\n",
      "train loss:0.050331250262871136\n",
      "train loss:0.018955817363581543\n",
      "train loss:0.20258736671271627\n",
      "train loss:0.09834321411133913\n",
      "train loss:0.21462767956601791\n",
      "train loss:0.09133656996296532\n",
      "train loss:0.11968858574584872\n",
      "train loss:0.1110069667203051\n",
      "train loss:0.049597459794100004\n",
      "train loss:0.0523251309330896\n",
      "train loss:0.06266196549443312\n",
      "train loss:0.07113059593661011\n",
      "train loss:0.30749087520507395\n",
      "train loss:0.06895988319734893\n",
      "train loss:0.3308254488444678\n",
      "train loss:0.1322794413079739\n",
      "train loss:0.04804542620903249\n",
      "train loss:0.12731097368760966\n",
      "train loss:0.005611534075548095\n",
      "train loss:0.17388251306749453\n",
      "train loss:0.061273492825352785\n",
      "train loss:0.21536255250005634\n",
      "train loss:0.1586370135869997\n",
      "train loss:0.14293061225092507\n",
      "train loss:0.2617375106759351\n",
      "train loss:0.15001881917784554\n",
      "train loss:0.18572920915709545\n",
      "train loss:0.016200576034302173\n",
      "train loss:0.0351881447813713\n",
      "train loss:0.35555637263095907\n",
      "train loss:0.06822487690071939\n",
      "train loss:0.08754472591707713\n",
      "train loss:0.08943577380393414\n",
      "train loss:0.05611656727183183\n",
      "train loss:0.05974570719554315\n",
      "train loss:0.0939431134800314\n",
      "train loss:0.03203834355570466\n",
      "train loss:0.16176717898381807\n",
      "train loss:0.3263746097272567\n",
      "train loss:0.06715464815839256\n",
      "train loss:0.19880131849326824\n",
      "train loss:0.25438772044570634\n",
      "train loss:0.12424697582470803\n",
      "train loss:0.06815713846515178\n",
      "train loss:0.12211647614052762\n",
      "train loss:0.060729573637390954\n",
      "train loss:0.02465282476705356\n",
      "train loss:0.07709627442029442\n",
      "train loss:0.07934302204088436\n",
      "train loss:0.4602155481830698\n",
      "train loss:0.07681460026900698\n",
      "train loss:0.4005188449141809\n",
      "train loss:0.0216311170009026\n",
      "train loss:0.07269764985320029\n",
      "train loss:0.11209637576816865\n",
      "train loss:0.0678487723756275\n",
      "train loss:0.0754213594453573\n",
      "train loss:0.22770800760451831\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9397\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10 \n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 10, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=50, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "# 에폭 수: 10, 미니배치 사이즈: 100, 최적화기법: Adam(Ir: 0.001) \n",
    "# 평가 시 사용되는 샘플 수: 1000                         \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=16,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.01},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 심층 신경망 VS. CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:26<00:00, 375037.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 153458.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:03<00:00, 498474.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MNIST data 데이터 로드\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True) # 배치 사이즈 128\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)  # 배치 사이즈 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a general deep neural network\n",
    "# FC-Layer가 3개인 MLP, 은닉층 노드 수: 512개, 활성화 함수: ReLU\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # FC-Layer\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        # 출력층\n",
    "        self.fc4 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Define a convolutional neural network\n",
    "# Convolutional layer 2개, Fully connected layer 1개, 활성화 함수: ReLU, Max pooling 사용\n",
    "class CNN(nn.Module):\n",
    "    # Conv - ReLU - Conv - ReLU - Pool - FC - ReLU - FC\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)   # 커널 개수: 32, 커널 크기: 3x3\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # 커널 개수: 64, 커널 크기: 3x3\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)                            # 64개의 14x14 이미지        \n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train and evaluate function\n",
    "def train_and_evaluate(model, epochs=5):                                    # epochs: 5\n",
    "    criterion = nn.CrossEntropyLoss()                                       # 손실 함수: CrossEntropy \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.0)        # optimizer: SGD, learning rate: 0.01, momentum: 0.0\n",
    "    \n",
    "    train_accuracy = []\n",
    "    val_accuracy = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            # loss 계산\n",
    "            loss = criterion(outputs, labels)\n",
    "            # 역전파\n",
    "            loss.backward()\n",
    "            # 가중치 기울기 갱신 \n",
    "            optimizer.step()\n",
    "            # 정확도 계산\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        train_accuracy.append(correct / total)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                # 정확도 계산\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_accuracy.append(correct / total)\n",
    "        \n",
    "    # train, test 데이터에 대한 정확도 반환 \n",
    "    return train_accuracy, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "deep_nn = DeepNN()\n",
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Deep Neural Network...\n",
      "Training Convolutional Neural Network...\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate both models\n",
    "print(\"Training Deep Neural Network...\")\n",
    "deep_nn_train_acc, deep_nn_val_acc = train_and_evaluate(deep_nn)\n",
    "\n",
    "print(\"Training Convolutional Neural Network...\")\n",
    "cnn_train_acc, cnn_val_acc = train_and_evaluate(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot DeepNN\n",
    "plt.plot(range(1, 6), deep_nn_val_acc, label=\"MLP Test Accuracy\", marker='o')\n",
    "\n",
    "# Plot CNN\n",
    "plt.plot(range(1, 6), cnn_val_acc, label=\"CNN Test Accuracy\", marker='x')\n",
    "\n",
    "plt.title(\"Test Data Accuracy Comparison\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
