{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# ptb 모듈 경로 추가\n",
    "import sys\n",
    "sys.path.append('C:/Users/KimDongyoung/Desktop/Github/my_git/mygit/DEEPLEARNING/밑바닥부터시작하는딥러닝2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence import load_data, get_vocab\n",
    "import os\n",
    "import numpy as np\n",
    "from common.time_layers import TimeEmbedding, TimeAffine, TimeSoftmaxWithLoss\n",
    "from common.base_model import BaseModel\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from common.util import eval_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 7) (45000, 5)\n",
      "(5000, 7) (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_data('addition.txt', seed=1984)\n",
    "char_to_id, id_to_char = get_vocab()\n",
    "\n",
    "# x_train, t_train에는 문자 ID의 시퀀스가 저장되어 있다.\n",
    "print(x_train.shape, t_train.shape)  # (45000, 7) (45000, 5) -> 45000개의 데이터가 있고, x_train 데이터에는 7개의 문자로 이루어져 있다.\n",
    "print(x_test.shape, t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  0  2  0  0 11  5]\n",
      "71+118 \n",
      "_189 \n"
     ]
    }
   ],
   "source": [
    "print(x_train[0]) # 0~9, +, ' '의 문자 ID로 이루어진 7개의 문자로 이루어진 문제\n",
    "print(''.join([id_to_char[c] for c in x_train[0]])) # 각 문자 ID에 해당하는 문자를 반환, 문제\n",
    "print(''.join([id_to_char[c] for c in t_train[0]])) # 각 문자 ID에 해당하는 문자를 반환, 정답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder, decoder 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "  def __init__(self,Wx,Wh,b):\n",
    "    self.params = [Wx,Wh,b] # 가중치에는 4개분의 가중치가 담겨 있다. input gate, forget gate, output gate, block input의 가중치 4개\n",
    "    self.grads = [np.zeros_like(Wx),np.zeros_like(Wh),np.zeros_like(b)] # 기울기를 저장하는 grads 인스턴스 변수 초기화\n",
    "    self.cache = None       # 순전파시 중간 결과를 담을 cache변수로 역전파 계산에 사용하는 인스턴스 변수 cache를 초기화\n",
    "  \n",
    "  # 순전파\n",
    "  def forward(self, x, h_prev, c_prev): # x는 입력 데이터, h_prev는 이전 시각의 은닉 상태, c_prev는 이전 시각의 기억 셀\n",
    "    Wx, Wh, b = self.params\n",
    "    N, H = h_prev.shape\n",
    "\n",
    "    A = np.matmul(x, Wx) + np.matmul(h_prev, Wh) + b  # 4개의 가중치를 모아 아핀 변환을 수행하여 A를 계산\n",
    "\n",
    "    f = A[:, :H]\n",
    "    g = A[:, H:2*H]\n",
    "    i = A[:, 2*H:3*H]\n",
    "    o = A[:, 3*H:]\n",
    "\n",
    "    f = sigmoid(f)                # forget gate\n",
    "    g = np.tanh(g)                # 새로운 정보를 기억하기 위한 input\n",
    "    i = sigmoid(i)                # input gate\n",
    "    o = sigmoid(o)                # output gate\n",
    "\n",
    "    c_next = f * c_prev + g * i   # 기억 셀(memory cell) 계산, c_next = f * c_prev + g * i\n",
    "    h_next = o * np.tanh(c_next)  # 은닉 상태 계산, h_next = o * tanh(c_next)\n",
    "\n",
    "    self.cache = (x, h_prev, c_prev, i, f, g, o, c_next) # cache에 순전파시 중간 결과(입력 데이터, 이전 시각의 은닉 상태, 이전 시각의 기억 셀, 4개의 gate, 기억 셀)를 저장\n",
    "    return h_next, c_next                                # 은닉 상태(hidden state)와 기억 셀(memory cell)을 반환\n",
    "  \n",
    "  # 역전파\n",
    "  def backward(self, dh_next, dc_next): # 상류에서 전해지는 기울기 dh_next와 dc_next를 받아 하류로 기울기를 전달\n",
    "    Wx, Wh, b = self.params\n",
    "    x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "    tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "    ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)  # ds는 tanh(c_next)의 역전파에서 내려온 기울기와 상류에서 전해지는 기울기 dh_next * o를 더한 값\n",
    "\n",
    "    dc_prev = ds * f # 이전 시각 기억 셀(c_prev)의 기울기 dc_prev 계산, ds에 forget gate 출력 결과인 f를 곱함\n",
    "\n",
    "    di = ds * g                 # di는 상류에서 전해지는 기울기 ds와 block input g를 곱한 값\n",
    "    df = ds * c_prev            # df는 상류에서 전해지는 기울기 ds와 이전 시각 기억 셀 c_prev를 곱한 값\n",
    "    do = dh_next * tanh_c_next  # do는 상류에서 전해지는 기울기 dh_next와 tanh(c_next)를 곱한 값\n",
    "    dg = ds * i                 # dg는 상류에서 전해지는 기울기 ds와 input gate i를 곱한 값\n",
    "\n",
    "    di *= i * (1 - i)           # 시그모이드 함수 기울기를 di에 곱하고 하류로 전달\n",
    "    df *= f * (1 - f)           # 시그모이드 함수 기울기를 df에 곱하고 하류로 전달\n",
    "    do *= o * (1 - o)           # 시그모이드 함수 기울기를 do에 곱하고 하류로 전달\n",
    "    dg *= (1 - g ** 2)          # tanh 함수 기울기를 dg에 곱하고 하류로 전달\n",
    "\n",
    "    dA = np.hstack((df, dg, di, do)) # 4개의 gate에 대한 기울기를 열 방향으로 합치기\n",
    "\n",
    "    dWh = np.dot(h_prev.T, dA)  # dA와 h_prev의 내적을 구해 Wh에 대한 기울기 dWh를 구함\n",
    "    dWx = np.dot(x.T, dA)       # dA와 x의 내적을 구해 Wx에 대한 기울기 dWx를 구함\n",
    "    db = dA.sum(axis=0)         # dA의 각 행을 다 더해 db를 구함\n",
    "    \n",
    "    # dWx, dWh, db를 각각 grads의 0, 1, 2번째 인덱스에 저장, 4개(input gate, forget gate, output gate, block input)의 가중치에 대한 기울기를 저장\n",
    "    self.grads[0][...] = dWx        \n",
    "    self.grads[1][...] = dWh\n",
    "    self.grads[2][...] = db\n",
    "\n",
    "    dx = np.dot(dA, Wx.T)       # dx는 dA와 Wx의 내적을 구해 구함\n",
    "    dh_prev = np.dot(dA, Wh.T)  # dh_prev는 dA와 Wh의 내적을 구해 구함\n",
    "\n",
    "    return dx, dh_prev, dc_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T개 분의 시계열 데이터를 한꺼번에 처리하는 TimeLSTM 계층\n",
    "class TimeLSTM:\n",
    "  def __init__(self, Wx, Wh, b , stateful=False):\n",
    "    self.params = [Wx, Wh, b]\n",
    "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "    self.layers = None\n",
    "\n",
    "    self.h, self.c = None, None   # hidden state, cell state\n",
    "    self.dh = None\n",
    "    self.stateful = stateful\n",
    "  \n",
    "  def forward(self, xs): # xs는 T개 분의 시계열 데이터를 하나로 모은 것\n",
    "    Wx, Wh, b = self.params\n",
    "    N, T, D = xs.shape\n",
    "    H = Wh.shape[0]\n",
    "\n",
    "    self.layers = []\n",
    "    hs = np.empty((N, T, H), dtype='f')     # 출력값을 저장할 변수 hs를 초기화\n",
    "\n",
    "    if not self.stateful or self.h is None:\n",
    "      self.h = np.zeros((N, H), dtype='f')\n",
    "    if not self.stateful or self.c is None:\n",
    "      self.c = np.zeros((N, H), dtype='f')\n",
    "    \n",
    "    for t in range(T): # 반복문을 사용해 T개 분의 시계열 데이터를 한꺼번에 처리\n",
    "      layer = LSTM(*self.params)\n",
    "      self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c) # LSTM 계층의 forward 메서드를 호출해 은닉 상태 h와 기억 셀 c를 입력값으로 받아 은닉 상태 h와 기억 셀 c를 갱신\n",
    "      hs[:, t, :] = self.h # 갱신한 은닉 상태 h를 hs에 저장\n",
    "\n",
    "      self.layers.append(layer)\n",
    "    \n",
    "    return hs # hs는 은닉 상태의 시계열 데이터를 반환\n",
    "  \n",
    "  def backward(self, dhs):\n",
    "    Wx, Wh, b = self.params\n",
    "    N, T, H = dhs.shape\n",
    "    D = Wx.shape[0]\n",
    "\n",
    "    dxs = np.empty((N, T, D), dtype='f')\n",
    "    dh, dc = 0, 0\n",
    "\n",
    "    grads = [0, 0, 0]\n",
    "    for t in reversed(range(T)):\n",
    "      layer = self.layers[t]\n",
    "      dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "      dxs[:, t, :] = dx\n",
    "      for i, grad in enumerate(layer.grads):\n",
    "        grads[i] += grad\n",
    "    \n",
    "    for i, grad in enumerate(grads):\n",
    "      self.grads[i][...] = grad\n",
    "    self.dh = dh\n",
    "    return dxs\n",
    "  \n",
    "  def set_state(self, h, c=None): # 마지막 LSTM 계층의 은닉 상태와 기억 셀을 h와 c로 설정\n",
    "    self.h, self.c = h, c\n",
    "  \n",
    "  # 상태 초기화\n",
    "  def reset_state(self):\n",
    "    self.h, self.c = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "class Encoder:\n",
    "  def __init__(self, vocab_size, wordvec_size, hidden_size): # vocab_size는 어휘 수, wordvec_size는 단어 벡터의 차원 수, hidden_size는 LSTM 계층의 은닉 상태 벡터의 차원 수\n",
    "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "    rn = np.random.randn # 랜덤값 생성\n",
    "\n",
    "    # 가중치 초기화\n",
    "    embed_W = (rn(V, D) / 100).astype('f')            # 단어 임베딩 계층의 가중치\n",
    "    lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f') # 입력값 x와 곱해지는 가중치 Wx,4개의 가중치를 모아 아핀 변환을 수행하기 위한 가중치\n",
    "    lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f') # 이전 시각의 은닉 상태 h와 곱해지는 가중치 Wh, 4개의 가중치를 모아 아핀 변환을 수행하기 위한 가중치\n",
    "    lstm_b = np.zeros(4 * H).astype('f')\n",
    "\n",
    "    self.embed = TimeEmbedding(embed_W)\n",
    "    self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False) # stateful은 은닉 상태를 유지할지 여부를 설정하는 인수\n",
    "\n",
    "    self.params = self.embed.params + self.lstm.params\n",
    "    self.grads = self.embed.grads + self.lstm.grads\n",
    "    self.hs = None\n",
    "  \n",
    "  # 순전파\n",
    "  def forward(self, xs): # xs는 단어 ID의 시퀀스 데이터\n",
    "    xs = self.embed.forward(xs)\n",
    "    hs = self.lstm.forward(xs)\n",
    "    self.hs = hs  # hs는 LSTM 계층의 은닉 상태 벡터들을 저장\n",
    "    return hs \n",
    "  \n",
    "  # 역전파\n",
    "  def backward(self, dh): # dh는 LSTM 계층의 마지막 시각의 은닉 상태의 기울기로 decoder에서 전해진 기울기이다. \n",
    "    dhs = np.zeros_like(self.hs) # hs와 같은 형상의 0으로 채워진 배열 dhs를 생성\n",
    "    dhs[:, -1, :] = dh           # dh를 dhs의 해당 위치에 저장\n",
    "    \n",
    "    dout = self.lstm.backward(dhs)\n",
    "    dout = self.embed.backward(dout)\n",
    "    return dout\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "class decoder:\n",
    "  def __init__(self, vocab_size, wordvec_size, hidden_size): # vocab_size는 어휘 수, wordvec_size는 단어 벡터의 차원 수, hidden_size는 LSTM 계층의 은닉 상태 벡터의 차원 수\n",
    "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "    rn = np.random.randn # 랜덤값 생성\n",
    "\n",
    "    # 가중치 초기화\n",
    "    embed_W = (rn(V, D) / 100).astype('f')\n",
    "    \n",
    "    lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f') # 입력값 x와 곱해지는 가중치 Wx,4개의 가중치를 모아 아핀 변환을 수행하기 위한 가중치\n",
    "    lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f') # 이전 시각의 은닉 상태 h와 곱해지는 가중치 Wh, 4개의 가중치를 모아 아핀 변환을 수행하기 위한 가중치\n",
    "    lstm_b = np.zeros(4 * H).astype('f')\n",
    "    \n",
    "    affine_W = (rn(H, V) / np.sqrt(H)).astype('f') # Affine 계층의 가중치 초기화\n",
    "    affine_b = np.zeros(V).astype('f')             # Affine 계층의 편향 초기화\n",
    "    \n",
    "    self.embed = TimeEmbedding(embed_W)\n",
    "    self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "    self.affine = TimeAffine(affine_W, affine_b)\n",
    "    \n",
    "    self.params, self.grads = [], []  # params와 grads에는 각 계층의 가중치와 편향을 모두 모은 리스트를 저장\n",
    "    # 각 계층의 가중치와 편향을 params와 grads에 추가\n",
    "    for layer in (self.embed, self.lstm, self.affine):\n",
    "      self.params += layer.params\n",
    "      self.grads += layer.grads\n",
    "      \n",
    "  # 순전파\n",
    "  def forward(self, xs, h): # xs는 단어 ID의 시퀀스 데이터, h는 encoder에서 출력된 마지막 은닉 상태\n",
    "    self.lstm.set_state(h)  # LSTM 계층의 은닉 상태를 encoder에서 출력된 마지막 은닉 상태로 설정\n",
    "    \n",
    "    out = self.embed.forward(xs)\n",
    "    out = self.lstm.forward(out)\n",
    "    score = self.affine.forward(out)\n",
    "    return score\n",
    "  \n",
    "  # 역전파\n",
    "  def backward(self, dscore): # dscore는 Softmax with Loss 계층의 역전파에서 전해지는 기울기\n",
    "    dout = self.affine.backward(dscore)\n",
    "    dout = self.lstm.backward(dout)\n",
    "    dout = self.embed.backward(dout)\n",
    "    dh = self.lstm.dh # decoder의 LSTM 계층의 은닉 상태의 기울기를 추출\n",
    "    return dh\n",
    "  \n",
    "  # generate\n",
    "  def generate(self, h, start_id, sample_size): # h는 encoder에서 출력된 마지막 은닉 상태, start_id는 최초로 주는 단어 ID, sample_size는 생성하는 단어의 수\n",
    "    sampled = []\n",
    "    sample_id = start_id\n",
    "    self.lstm.set_state(h)\n",
    "    \n",
    "    for _ in range(sample_size):\n",
    "      x = np.array(sample_id).reshape((1, 1))\n",
    "      out = self.embed.forward(x)\n",
    "      out = self.lstm.forward(out)\n",
    "      score = self.affine.forward(out)\n",
    "      \n",
    "      sample_id = np.argmax(score.flatten()) # Affine 계층이 출력하는 점수가 가장 높은 문자의 ID를 sample_id로 선택\n",
    "      sampled.append(int(sample_id))         # sample_id를 sampled에 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq 계층 구현  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(BaseModel):\n",
    "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "    V, D, H = vocab_size, wordvec_size, hidden_size # vocab_size는 어휘 수, wordvec_size는 단어 벡터의 차원 수, hidden_size는 LSTM 계층의 은닉 상태 벡터의 차원 수\n",
    "    self.encoder = Encoder(V, D, H)\n",
    "    self.decoder = decoder(V, D, H)\n",
    "    self.softmax = TimeSoftmaxWithLoss()\n",
    "    \n",
    "    self.params = self.encoder.params + self.decoder.params\n",
    "    self.grads = self.encoder.grads + self.decoder.grads\n",
    "  \n",
    "  # 순전파\n",
    "  def forward(self, xs, ts): # xs는 입력 데이터, ts는 정답 레이블\n",
    "    decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "    \n",
    "    h = self.encoder.forward(xs)                    # encoder의 forward 메서드를 호출해 은닉 상태 h를 구함\n",
    "    score = self.decoder.forward(decoder_xs, h)     # decoder의 forward 메서드를 호출해 점수를 구함\n",
    "    loss = self.softmax.forward(score, decoder_ts)  # score와 decoder의 정답 레이블을 사용해 Softmax with Loss 계층의 forward 메서드를 호출해 손실을 구함\n",
    "    return loss\n",
    "  \n",
    "  # 역전파\n",
    "  def backward(self, dout=1):                       \n",
    "    dout = self.softmax.backward(dout)              # Softmax with Loss 계층의 backward 메서드를 호출해 Softmax 계층의 기울기를 구함\n",
    "    dh = self.decoder.backward(dout)                # decoder의 backward 메서드를 호출해 decoder의 LSTM 계층의 은닉 상태의 기울기를 구함\n",
    "    dout = self.encoder.backward(dh)                # encoder의 backward 메서드를 호출해 encoder의 embedding 계층과 LSTM 계층의 기울기를 구함\n",
    "    return dout\n",
    "  \n",
    "  # generate\n",
    "  def generate(self, xs, start_id, sample_size):\n",
    "    h = self.encoder.forward(xs)\n",
    "    sampled = self.decoder.generate(h, start_id, sample_size)\n",
    "    return sampled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq 평가   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_data('addition.txt', seed=1984)\n",
    "char_to_id, id_to_char = get_vocab()\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 128\n",
    "batch_size = 128\n",
    "max_epoch = 25\n",
    "max_grad = 5.0\n",
    "\n",
    "# 모델, 옵티마이저, 트레이너 생성\n",
    "model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 2.56\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 0[s] | 손실 2.53\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 0[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 1[s] | 손실 2.14\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 1[s] | 손실 2.06\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 2[s] | 손실 1.99\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 2[s] | 손실 1.95\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 3[s] | 손실 1.93\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 3[s] | 손실 1.93\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 3[s] | 손실 1.92\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 4[s] | 손실 1.90\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 4[s] | 손실 1.89\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 5[s] | 손실 1.89\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 5[s] | 손실 1.87\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 5[s] | 손실 1.88\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 6[s] | 손실 1.86\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 6[s] | 손실 1.87\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 7[s] | 손실 1.85\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m   question, correct \u001b[38;5;241m=\u001b[39m x_test[[i]], t_test[[i]]\n\u001b[0;32m      9\u001b[0m   verbose \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 10\u001b[0m   correct_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43meval_seq2seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_to_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(correct_num) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(x_test)\n\u001b[0;32m     13\u001b[0m acc_list\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "File \u001b[1;32mC:\\Users/KimDongyoung/Desktop/Github/my_git/mygit/DEEPLEARNING/밑바닥부터시작하는딥러닝2\\common\\util.py:240\u001b[0m, in \u001b[0;36meval_seq2seq\u001b[1;34m(model, question, correct, id_to_char, verbos, is_reverse)\u001b[0m\n\u001b[0;32m    238\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([id_to_char[\u001b[38;5;28mint\u001b[39m(c)] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m question\u001b[38;5;241m.\u001b[39mflatten()])\n\u001b[0;32m    239\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([id_to_char[\u001b[38;5;28mint\u001b[39m(c)] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m correct])\n\u001b[1;32m--> 240\u001b[0m guess \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43m[\u001b[49m\u001b[43mid_to_char\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mguess\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbos:\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_reverse:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "acc_list = [] # 정확도를 저장할 리스트\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "  trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "  correct_num = 0\n",
    "  \n",
    "  for i in range(len(x_test)):\n",
    "    question, correct = x_test[[i]], t_test[[i]]\n",
    "    verbose = i < 10\n",
    "    correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose)\n",
    "  \n",
    "  acc = float(correct_num) / len(x_test)\n",
    "  acc_list.append(acc)\n",
    "  print('정확도 %.3f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Seq2seq' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m guess \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(question)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(guess)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Seq2seq' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "guess = model.predict(question)\n",
    "print(guess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
