{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/KimDongyoung/Desktop/Github/my_git/mygit/DEEPLEARNING/밑바닥부터시작하는딥러닝2')\n",
    "\n",
    "import numpy as np\n",
    "from common.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "from common.optimizer import SGD\n",
    "\n",
    "sys.path.append('C:/Users/KimDongyoung/Desktop/Github/my_git/mygit/DEEPLEARNING/밑바닥부터시작하는딥러닝2/dataset')\n",
    "from ptb import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 계층 구현  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "  def __init__(self, Wx, Wh, b):  # Wx: 입력 x에 대한 가중치, Wh: 이전 상태 h에 대한 가중치, b: 편향\n",
    "    self.params = [Wx, Wh, b]   # 입력받은 Wx, Wh, b를 params에 리스트로 저장\n",
    "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]  # 기울기 초기화\n",
    "    self.cache = None           # 순전파 시 입력된 x, 이전 상태 h, 다음 상태 h를 저장할 cache\n",
    "  \n",
    "  # 순전파\n",
    "  def forward(self, x, h_prev):                 # x: 입력, h_prev(h_t-1): 이전 상태\n",
    "    Wx, Wh, b = self.params                     # Wx, Wh, b를 params에서 가져옴\n",
    "    t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b  # h_prev, x, b를 행렬 곱하고 편향을 더해 t를 구함\n",
    "    h_next = np.tanh(t)                         # tanh 함수를 통과시켜 h_next를 구함, h_t = tanh(h_t-1 * Wh + x_t * Wx + b)\n",
    "    self.cache = (x, h_prev, h_next)            # x, h_prev, h_next를 cache에 저장\n",
    "    return h_next                               # h_next(h_t) 반환\n",
    "  \n",
    "  # 역전파\n",
    "  def backward(self, dh_next):        # dh_next: 출력 쪽에서 전해지는 기울기\n",
    "    Wx, Wh, b = self.params           # Wx, Wh, b를 params에서 가져옴\n",
    "    x, h_prev, h_next = self.cache    # cache에서 x, h_prev, h_next를 가져옴\n",
    "    \n",
    "    dt = dh_next * (1 - h_next ** 2)  # tanh 미분, dt = dh_next * (1 - h_next^2)\n",
    "    db = np.sum(dt, axis=0)           # db = dt\n",
    "    dWh = np.dot(h_prev.T, dt)        # dWh = h_prev * dt\n",
    "    dh_prev = np.dot(dt, Wh.T)        # dh_prev = dt * Wh\n",
    "    dWx = np.dot(x.T, dt)             # dWx = x * dt\n",
    "    dx = np.dot(dt, Wx.T)             # dx = dt * Wx\n",
    "    \n",
    "    self.grads[0][...] = dWx          # dWx, dWh, db를 grads에 저장\n",
    "    self.grads[1][...] = dWh\n",
    "    \n",
    "    self.grads[2][...] = db\n",
    "    \n",
    "    return dx, dh_prev                     # 입력 x와 이전 상태 h에 대한 기울기 dx, dh_prev 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00751938 -0.42677832  0.68071477 -1.0521159 ]]\n",
      "[[ 1.10414212 -0.95186828]]\n",
      "[[-0.99795991 -0.43135969]]\n",
      "(array([[-1.00751938, -0.42677832,  0.68071477, -1.0521159 ]]), array([[ 1.10414212, -0.95186828]]), array([[-0.99795991, -0.43135969]]))\n",
      "[-1.20395718  0.3096301 ]\n",
      "[[ 0.35089753 -0.27154679 -0.13105494  0.03157246]]\n",
      "[[-0.09102914 -0.07537056]]\n",
      "[array([[ 0.00494426, -0.25391187],\n",
      "       [ 0.00209435, -0.10755533],\n",
      "       [-0.00334051,  0.1715516 ],\n",
      "       [ 0.00516311, -0.26515095]]), array([[-0.00541842,  0.27826243],\n",
      "       [ 0.00467116, -0.23988686]]), array([-0.00490736,  0.25201686])]\n"
     ]
    }
   ],
   "source": [
    "# x dimension: 4, h dimension: 2\n",
    "# w_x : 4 x 2, w_h : 2 x 2, b : 2\n",
    "\n",
    "# RNN 클래스의 인스턴스 생성\n",
    "W_x = np.random.randn(4, 2)  # 입력 x에 대한 가중치\n",
    "W_h = np.random.randn(2, 2)  # 이전 상태 h에 대한 가중치\n",
    "b = np.random.randn(2)       # 편향\n",
    "\n",
    "rnn = RNN(W_x, W_h, b)\n",
    "# 순전파 forward 메서드 호출\n",
    "x = np.random.randn(1,4)  # 입력\n",
    "print(x)\n",
    "h_prev = np.random.randn(1,2)  # 이전 상태\n",
    "print(h_prev)\n",
    "h_next = rnn.forward(x, h_prev)\n",
    "print(h_next)\n",
    "\n",
    "print(rnn.cache)  # (x, h_prev, h_next)\n",
    "\n",
    "# 역전파 backward 메서드 호출\n",
    "dh_next = np.random.randn(2)  # 출력 쪽에서 전해지는 기울기\n",
    "print(dh_next)\n",
    "dx, dh_prev = rnn.backward(dh_next)\n",
    "print(dx)\n",
    "print(dh_prev)\n",
    "print(rnn.grads)  # [dW_x, dW_h, db]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time RNN 계층 구현    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "  def __init__(self, Wx, Wh, b, stateful=False):   # Wx: 입력 x에 대한 가중치, Wh: 이전 상태 h에 대한 가중치, b: 편향, stateful: 상태 유지 여부\n",
    "    self.params = [Wx, Wh, b]                      # Wx, Wh, b를 params에 저장\n",
    "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]  # 기울기 초기화\n",
    "    self.layers = None                             # 다수의 RNN 계층을 리스트로 저장\n",
    "    \n",
    "    self.h, self.dh = None, None                   # h는 forward() 메서드를 불렀을 때 마지막 RNN 계층의 은닉 상태를 저장, dh는 backward() 메서드를 불렀을 때 하나 앞 블록의 은닉 상태의 기울기를 저장\n",
    "    self.stateful = stateful                       # 상태 유지 여부, 만약 False라면 은닉 상태를 영행렬(모든 요소가 0인 행렬)로 초기화\n",
    "    \n",
    "  def set_state(self, h):                          # 은닉 상태 설정\n",
    "    self.h = h                                     # h에 입력된 h를 저장\n",
    "    \n",
    "  def reset_state(self):                           # 은닉 상태 초기화\n",
    "    self.h = None                                  # h를 None으로 초기화\n",
    "  \n",
    "  # 순전파\n",
    "  def forward(self, xs):                           # xs: T개 분량의 시계열 데이터를 하나로 모은 것\n",
    "    Wx, Wh, b = self.params                        # Wx, Wh, b를 params에서 가져옴\n",
    "    N, T, D = xs.shape                             # N: 미니배치 크기, T: 시계열 데이터의 길이, D: 입력 벡터의 차원 수\n",
    "    D, H = Wx.shape                                # Wx의 형상을 가져옴\n",
    "    \n",
    "    self.layers = []                               # RNN 계층을 저장할 리스트 layers 초기화\n",
    "    hs = np.empty((N, T, H), dtype='f')            # 은닉 상태를 저장할 hs를 영행렬로 초기화\n",
    "    \n",
    "    if not self.stateful or self.h is None:        # 상태 유지가 False이거나 은닉 상태가 None이면\n",
    "      self.h = np.zeros((N, H), dtype='f')         # 은닉 상태를 영행렬로 초기화\n",
    "    \n",
    "    for t in range(T):                             # T번 반복, 데이터 개수 T만큼 반복\n",
    "      layer = RNN(*self.params)                    # RNN 계층 생성\n",
    "      self.h = layer.forward(xs[:, t, :], self.h)  # RNN 계층의 forward 메서드 호출, 은닉 상태 구한 후 h에 저장\n",
    "      hs[:, t, :] = self.h                         # hs에 계산한 은닉 상태 저장\n",
    "      self.layers.append(layer)                    # RNN 계층을 layers에 추가\n",
    "    \n",
    "    return hs                                     # hs 반환 -> 행렬 hs에는 각 시각의 은닉 상태(h)가 저장되어 있음, 총 T개의 은닉 상태(h)가 저장되어 있음\n",
    "  \n",
    "  # 역전파\n",
    "  def backward(self, dhs):                         # dhs: 각 시각의 은닉 상태의 기울기\n",
    "    Wx, Wh, b = self.params                        # Wx, Wh, b를 params에서 가져옴\n",
    "    N, T, H = dhs.shape                            # dhs의 형상을 가져옴, N: 미니배치 크기, T: 시계열 데이터의 길이, H: 은닉 상태의 차원 수\n",
    "    D, H = Wx.shape                                # Wx의 형상을 가져옴\n",
    "    \n",
    "    dxs = np.empty((N, T, D), dtype='f')           # dxs를 영행렬로 초기화\n",
    "    dh = 0                                         # dh를 0으로 초기화\n",
    "    grads = [0, 0, 0]                              # grads를 0으로 초기화\n",
    "    \n",
    "    for t in reversed(range(T)):                   # T-1부터 0까지 반복\n",
    "      layer = self.layers[t]                       # t번째 RNN 계층을 가져옴\n",
    "      dx, dh = layer.backward(dhs[:, t, :] + dh)   # RNN 계층의 backward 메서드 호출, dx와 dh를 구함, dh는 이전 블록의 은닉 상태 기울기를 더함\n",
    "      dxs[:, t, :] = dx                            # dxs에 dx 저장\n",
    "      \n",
    "      for i, grad in enumerate(layer.grads):       # layer.grads에는 dWx, dWh, db가 저장되어 있음\n",
    "        grads[i] += grad                          # grads에 grad를 더함\n",
    "    \n",
    "    for i, grad in enumerate(grads):               # grads에 저장된 기울기를 각각 꺼내어\n",
    "      self.grads[i][...] = grad                    # self.grads에 저장, 각 RNN 계층의 가중치 기울기를 모두 더한 것이 grads이므로 이를 self.grads에 저장\n",
    "    \n",
    "    self.dh = dh                                   # dh를 self.dh에 저장\n",
    "    \n",
    "    return dxs                                     # dxs 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1467112   0.00304117 -1.51448636]\n",
      " [-0.04356993  1.06638404  0.61999271]\n",
      " [-0.49841917  0.48560109  1.0319122 ]\n",
      " [ 1.44938991 -0.16839239  0.59199169]]\n",
      "[[-0.04809326  0.14912318 -0.23466471]\n",
      " [ 0.16076289  1.06380016  0.03566217]\n",
      " [ 0.99698378  2.22111557 -0.06945657]]\n",
      "[-0.80749947  0.74242327  0.31804092]\n",
      "[[[ 1.59850756 -0.0714816   1.13563671 -1.98752   ]\n",
      "  [ 0.57576763  0.16059268 -1.30446178  0.34465988]\n",
      "  [ 0.26934324  0.12673328 -0.71064272  0.36321462]\n",
      "  [-0.97604689 -0.11418968 -0.10214062  2.5348975 ]\n",
      "  [ 0.38429224 -0.13491219 -0.38352429 -1.04124709]]]\n",
      "[[[-0.999746    0.9149669  -0.97332704]\n",
      "  [-0.4811428  -0.80535716 -0.85150474]\n",
      "  [-0.7292706  -0.98188895 -0.3681435 ]\n",
      "  [ 0.98848695 -0.94980127  0.99718565]\n",
      "  [-0.8814146   0.959589   -0.9347657 ]]]\n"
     ]
    }
   ],
   "source": [
    "# TimeRNN 클래스의 인스턴스 생성\n",
    "N = 1 # 미니배치 크기\n",
    "T = 5 # 시계열 데이터의 길이\n",
    "D = 4  # 입력 벡터의 차원 수\n",
    "H = 3  # 은닉 상태의 차원 수\n",
    "W_x = np.random.randn(D, H)  # 입력 x에 대한 가중치\n",
    "W_h = np.random.randn(H, H)  # 이전 상태 h에 대한 가중치\n",
    "b = np.random.randn(H)       # 편향\n",
    "print(W_x)\n",
    "print(W_h)\n",
    "print(b)\n",
    "\n",
    "time_rnn = TimeRNN(W_x, W_h, b)\n",
    "# 순전파 forward 메서드 호출\n",
    "xs = np.random.randn(N, T, D)  # 입력\n",
    "print(xs)\n",
    "hs = time_rnn.forward(xs)\n",
    "print(hs)                      # hs: (N, T, H), 은닉 상태의 시계열 데이터\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.67114405 -0.3053001   0.26461786]\n",
      "  [ 0.06642941  1.26212145 -0.48489914]\n",
      "  [-1.58546602 -0.48308994 -0.00998992]\n",
      "  [ 0.74866748  1.16711631  0.68830207]\n",
      "  [ 0.47397924  0.19897907 -1.78276825]]]\n",
      "[array([[-0.15541398,  0.14859629, -0.12666962],\n",
      "       [-0.10385399,  0.04585561, -0.00119688],\n",
      "       [ 0.43384232, -0.50980995,  0.44677213],\n",
      "       [-0.31810355,  0.37604893,  0.08379752]]), array([[ 0.40977142, -0.46343017,  0.00989114],\n",
      "       [ 0.5056844 ,  0.24676377, -0.2945003 ],\n",
      "       [ 0.69045308, -0.4058301 , -0.08507961]]), array([-0.57618341,  0.53834505, -0.27244311])]\n"
     ]
    }
   ],
   "source": [
    "# 역전파 backward 메서드 호출\n",
    "dhs = np.random.randn(N, T, H)  # 각 시각의 은닉 상태의 기울기\n",
    "print(dhs)\n",
    "dxs = time_rnn.backward(dhs)\n",
    "print(time_rnn.grads)           # [dW_x, dW_h, db], 가중치 매개변수 기울기, dW_x: 입력 x에 대한 가중치 기울기, dW_h: 이전 상태 h에 대한 가중치 기울기, db: 편향 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNNlm 계층 구현  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):   # idx는 단어 ID로 이루어진 배열로 idx에 해당하는 단어 벡터를 추출\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t]) # Embedding 계층의 forward 메서드 호출, 각 시각의 단어를 단어 벡터로 변환\n",
    "            self.layers.append(layer) \n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        self.ignore_label = -1\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "\n",
    "        if ts.ndim == 3:  # 정답 레이블이 원핫 벡터인 경우\n",
    "            ts = ts.argmax(axis=2)\n",
    "\n",
    "        mask = (ts != self.ignore_label)\n",
    "\n",
    "        # 배치용과 시계열용을 정리(reshape)\n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "        mask = mask.reshape(N * T)\n",
    "\n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N * T), ts])\n",
    "        ls *= mask  # ignore_label에 해당하는 데이터는 손실을 0으로 설정\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= mask.sum()\n",
    "\n",
    "        self.cache = (ts, ys, mask, (N, T, V))\n",
    "        return loss              # 손실 반환\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, mask, (N, T, V) = self.cache\n",
    "\n",
    "        dx = ys\n",
    "        dx[np.arange(N * T), ts] -= 1\n",
    "        dx *= dout\n",
    "        dx /= mask.sum()\n",
    "        dx *= mask[:, np.newaxis]  # ignore_label에 해당하는 데이터는 기울기를 0으로 설정\n",
    "\n",
    "        dx = dx.reshape((N, T, V))\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')    # W_x의 초깃값을 Xavier 초깃값으로 설정, W_x는 입력 x에 대한 가중치\n",
    "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')    # W_h의 초깃값을 Xavier 초깃값으로 설정, W_h는 이전 상태 h에 대한 가중치\n",
    "        rnn_b = np.zeros(H).astype('f')                 # 편향 b는 0으로 초기화\n",
    "        \n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')  # afiine_W는 Xavier 초깃값으로 설정\n",
    "        affine_b = np.zeros(V).astype('f')              # 편향 affine_b는 0으로 초기화\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),                         # TimeEmbedding 계층\n",
    "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),  # TimeRNN 계층\n",
    "            TimeAffine(affine_W, affine_b)                  # TimeAffine 계층\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()             # TimeSoftmaxWithLoss 계층\n",
    "        self.rnn_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)                          # 각 계층의 forward 메서드 호출\n",
    "        loss = self.loss_layer.forward(xs, ts)              # TimeSoftmaxWithLoss 계층의 forward 메서드 호출, TimeEmbedding, TimeRNN, TimeAffine 순서로 호출\n",
    "        return loss                                         # 손실 반환\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)                     # 각 계층의 backward 메서드 호출\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNLM 학습 코드   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말뭉치 크기: 10000, 어휘 수: 2138\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "batch_size = 100    # 미니배치 크기\n",
    "wordvec_size = 1000  # 단어 임베딩 차원 수\n",
    "hidden_size = 100\n",
    "time_size = 5\n",
    "lr = 0.1\n",
    "max_epoch = 100\n",
    "\n",
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = load_data('train')\n",
    "corpus_size = 10000  # 테스트 데이터셋을 작게 설정\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "\n",
    "xs = corpus[:-1]  # 입력\n",
    "ts = corpus[1:]   # 출력(정답 레이블)\n",
    "data_size = len(xs)\n",
    "print('말뭉치 크기: %d, 어휘 수: %d' % (corpus_size, vocab_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 | 반복 10 / 19 | 퍼플렉서티 2107.07\n",
      "| 에폭 2 | 반복 10 / 19 | 퍼플렉서티 1849.93\n",
      "| 에폭 3 | 반복 10 / 19 | 퍼플렉서티 986.84\n",
      "| 에폭 4 | 반복 10 / 19 | 퍼플렉서티 730.48\n",
      "| 에폭 5 | 반복 10 / 19 | 퍼플렉서티 647.71\n",
      "| 에폭 6 | 반복 10 / 19 | 퍼플렉서티 601.56\n",
      "| 에폭 7 | 반복 10 / 19 | 퍼플렉서티 568.52\n",
      "| 에폭 8 | 반복 10 / 19 | 퍼플렉서티 548.16\n",
      "| 에폭 9 | 반복 10 / 19 | 퍼플렉서티 532.09\n",
      "| 에폭 10 | 반복 10 / 19 | 퍼플렉서티 510.39\n",
      "| 에폭 11 | 반복 10 / 19 | 퍼플렉서티 498.81\n",
      "| 에폭 12 | 반복 10 / 19 | 퍼플렉서티 493.06\n",
      "| 에폭 13 | 반복 10 / 19 | 퍼플렉서티 486.70\n",
      "| 에폭 14 | 반복 10 / 19 | 퍼플렉서티 481.56\n",
      "| 에폭 15 | 반복 10 / 19 | 퍼플렉서티 475.36\n",
      "| 에폭 16 | 반복 10 / 19 | 퍼플렉서티 468.62\n",
      "| 에폭 17 | 반복 10 / 19 | 퍼플렉서티 463.47\n",
      "| 에폭 18 | 반복 10 / 19 | 퍼플렉서티 465.83\n",
      "| 에폭 19 | 반복 10 / 19 | 퍼플렉서티 456.86\n",
      "| 에폭 20 | 반복 10 / 19 | 퍼플렉서티 451.84\n",
      "| 에폭 21 | 반복 10 / 19 | 퍼플렉서티 452.04\n",
      "| 에폭 22 | 반복 10 / 19 | 퍼플렉서티 446.08\n",
      "| 에폭 23 | 반복 10 / 19 | 퍼플렉서티 450.03\n",
      "| 에폭 24 | 반복 10 / 19 | 퍼플렉서티 445.76\n",
      "| 에폭 25 | 반복 10 / 19 | 퍼플렉서티 441.89\n",
      "| 에폭 26 | 반복 10 / 19 | 퍼플렉서티 442.95\n",
      "| 에폭 27 | 반복 10 / 19 | 퍼플렉서티 439.07\n",
      "| 에폭 28 | 반복 10 / 19 | 퍼플렉서티 437.61\n",
      "| 에폭 29 | 반복 10 / 19 | 퍼플렉서티 436.80\n",
      "| 에폭 30 | 반복 10 / 19 | 퍼플렉서티 434.35\n",
      "| 에폭 31 | 반복 10 / 19 | 퍼플렉서티 430.08\n",
      "| 에폭 32 | 반복 10 / 19 | 퍼플렉서티 430.89\n",
      "| 에폭 33 | 반복 10 / 19 | 퍼플렉서티 430.29\n",
      "| 에폭 34 | 반복 10 / 19 | 퍼플렉서티 429.85\n",
      "| 에폭 35 | 반복 10 / 19 | 퍼플렉서티 423.98\n",
      "| 에폭 36 | 반복 10 / 19 | 퍼플렉서티 426.02\n",
      "| 에폭 37 | 반복 10 / 19 | 퍼플렉서티 427.58\n",
      "| 에폭 38 | 반복 10 / 19 | 퍼플렉서티 427.46\n",
      "| 에폭 39 | 반복 10 / 19 | 퍼플렉서티 425.85\n",
      "| 에폭 40 | 반복 10 / 19 | 퍼플렉서티 421.50\n",
      "| 에폭 41 | 반복 10 / 19 | 퍼플렉서티 419.75\n",
      "| 에폭 42 | 반복 10 / 19 | 퍼플렉서티 422.97\n",
      "| 에폭 43 | 반복 10 / 19 | 퍼플렉서티 419.04\n",
      "| 에폭 44 | 반복 10 / 19 | 퍼플렉서티 417.80\n",
      "| 에폭 45 | 반복 10 / 19 | 퍼플렉서티 413.83\n",
      "| 에폭 46 | 반복 10 / 19 | 퍼플렉서티 413.38\n",
      "| 에폭 47 | 반복 10 / 19 | 퍼플렉서티 414.05\n",
      "| 에폭 48 | 반복 10 / 19 | 퍼플렉서티 416.87\n",
      "| 에폭 49 | 반복 10 / 19 | 퍼플렉서티 413.79\n",
      "| 에폭 50 | 반복 10 / 19 | 퍼플렉서티 412.44\n",
      "| 에폭 51 | 반복 10 / 19 | 퍼플렉서티 412.47\n",
      "| 에폭 52 | 반복 10 / 19 | 퍼플렉서티 410.00\n",
      "| 에폭 53 | 반복 10 / 19 | 퍼플렉서티 407.78\n",
      "| 에폭 54 | 반복 10 / 19 | 퍼플렉서티 406.41\n",
      "| 에폭 55 | 반복 10 / 19 | 퍼플렉서티 406.51\n",
      "| 에폭 56 | 반복 10 / 19 | 퍼플렉서티 401.43\n",
      "| 에폭 57 | 반복 10 / 19 | 퍼플렉서티 402.35\n",
      "| 에폭 58 | 반복 10 / 19 | 퍼플렉서티 404.58\n",
      "| 에폭 59 | 반복 10 / 19 | 퍼플렉서티 400.99\n",
      "| 에폭 60 | 반복 10 / 19 | 퍼플렉서티 394.56\n",
      "| 에폭 61 | 반복 10 / 19 | 퍼플렉서티 395.43\n",
      "| 에폭 62 | 반복 10 / 19 | 퍼플렉서티 396.26\n",
      "| 에폭 63 | 반복 10 / 19 | 퍼플렉서티 396.27\n",
      "| 에폭 64 | 반복 10 / 19 | 퍼플렉서티 394.29\n",
      "| 에폭 65 | 반복 10 / 19 | 퍼플렉서티 391.02\n",
      "| 에폭 66 | 반복 10 / 19 | 퍼플렉서티 391.43\n",
      "| 에폭 67 | 반복 10 / 19 | 퍼플렉서티 390.32\n",
      "| 에폭 68 | 반복 10 / 19 | 퍼플렉서티 386.90\n",
      "| 에폭 69 | 반복 10 / 19 | 퍼플렉서티 382.89\n",
      "| 에폭 70 | 반복 10 / 19 | 퍼플렉서티 378.25\n",
      "| 에폭 71 | 반복 10 / 19 | 퍼플렉서티 380.59\n",
      "| 에폭 72 | 반복 10 / 19 | 퍼플렉서티 379.87\n",
      "| 에폭 73 | 반복 10 / 19 | 퍼플렉서티 381.19\n",
      "| 에폭 74 | 반복 10 / 19 | 퍼플렉서티 378.42\n",
      "| 에폭 75 | 반복 10 / 19 | 퍼플렉서티 377.04\n",
      "| 에폭 76 | 반복 10 / 19 | 퍼플렉서티 377.37\n",
      "| 에폭 77 | 반복 10 / 19 | 퍼플렉서티 375.08\n",
      "| 에폭 78 | 반복 10 / 19 | 퍼플렉서티 372.96\n",
      "| 에폭 79 | 반복 10 / 19 | 퍼플렉서티 371.91\n",
      "| 에폭 80 | 반복 10 / 19 | 퍼플렉서티 369.64\n",
      "| 에폭 81 | 반복 10 / 19 | 퍼플렉서티 366.51\n",
      "| 에폭 82 | 반복 10 / 19 | 퍼플렉서티 369.66\n",
      "| 에폭 83 | 반복 10 / 19 | 퍼플렉서티 367.68\n",
      "| 에폭 84 | 반복 10 / 19 | 퍼플렉서티 361.85\n",
      "| 에폭 85 | 반복 10 / 19 | 퍼플렉서티 361.34\n",
      "| 에폭 86 | 반복 10 / 19 | 퍼플렉서티 362.34\n",
      "| 에폭 87 | 반복 10 / 19 | 퍼플렉서티 362.84\n",
      "| 에폭 88 | 반복 10 / 19 | 퍼플렉서티 359.94\n",
      "| 에폭 89 | 반복 10 / 19 | 퍼플렉서티 360.49\n",
      "| 에폭 90 | 반복 10 / 19 | 퍼플렉서티 356.92\n",
      "| 에폭 91 | 반복 10 / 19 | 퍼플렉서티 357.59\n",
      "| 에폭 92 | 반복 10 / 19 | 퍼플렉서티 355.59\n",
      "| 에폭 93 | 반복 10 / 19 | 퍼플렉서티 354.13\n",
      "| 에폭 94 | 반복 10 / 19 | 퍼플렉서티 352.83\n",
      "| 에폭 95 | 반복 10 / 19 | 퍼플렉서티 349.23\n",
      "| 에폭 96 | 반복 10 / 19 | 퍼플렉서티 347.80\n",
      "| 에폭 97 | 반복 10 / 19 | 퍼플렉서티 350.47\n",
      "| 에폭 98 | 반복 10 / 19 | 퍼플렉서티 351.30\n",
      "| 에폭 99 | 반복 10 / 19 | 퍼플렉서티 346.95\n",
      "| 에폭 100 | 반복 10 / 19 | 퍼플렉서티 346.02\n"
     ]
    }
   ],
   "source": [
    "# 학습 시 사용하는 변수\n",
    "max_iters = data_size // (batch_size * time_size) # \n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "ppl_list = []\n",
    "\n",
    "# 모델 생성\n",
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "\n",
    "# 미니배치의 각 샘플의 읽기 시작 위치를 계산\n",
    "jump = (corpus_size - 1) // batch_size \n",
    "offsets = [i * jump for i in range(batch_size)]\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "  for iter in range(max_iters):\n",
    "    # 미니배치 취득\n",
    "    batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "    batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "    for t in range(time_size):\n",
    "      for i, offset in enumerate(offsets):\n",
    "        batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "        batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "      time_idx += 1\n",
    "\n",
    "    # 기울기를 구하여 매개변수 갱신\n",
    "    # 미니배치 학습이기 때문에, 미니배치의 손실을 구하고, 그 손실을 이용하여 기울기를 구하고, 그 기울기를 이용하여 매개변수를 갱신\n",
    "    # 하나의 미니배치 학습이 끝나면 다음 미니배치 학습을 진행\n",
    "    loss = model.forward(batch_x, batch_t)\n",
    "    model.backward()\n",
    "    optimizer.update(model.params, model.grads)\n",
    "    total_loss += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "\n",
    "    # 퍼플렉서티 평가\n",
    "    if (iter + 1) % 10 == 0:\n",
    "      ppl = np.exp(total_loss / loss_count)\n",
    "      print('| 에폭 %d | 반복 %d / %d | 퍼플렉서티 %.2f'\n",
    "            % (epoch + 1, iter + 1, max_iters, ppl))\n",
    "      ppl_list.append(float(ppl))\n",
    "      total_loss, loss_count = 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "\n",
    "\n",
    "x = np.arange(len(ppl_list))\n",
    "plt.plot(x, ppl_list, label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('perplexity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [[0.28093451 0.54269608 0.14092422]\n",
      " [0.80219698 0.07455064 0.98688694]\n",
      " [0.77224477 0.19871568 0.00552212]\n",
      " [0.81546143 0.70685734 0.72900717]\n",
      " [0.77127035 0.07404465 0.35846573]\n",
      " [0.11586906 0.86310343 0.62329813]\n",
      " [0.33089802 0.06355835 0.31098232]\n",
      " [0.32518332 0.72960618 0.63755747]\n",
      " [0.88721274 0.47221493 0.11959425]\n",
      " [0.71324479 0.76078505 0.5612772 ]]\n",
      "y [2 1 2 2 0 2 2 1 1 0]\n",
      "y_true_one_hot [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "y_t [0.14446895 0.43275623 0.42277482]\n",
      "y_t [0.13746243 0.80426841 0.05826915]\n",
      "y_t [0.20535518 0.77338275 0.02126206]\n",
      "y_t [0.02371574 0.95114254 0.02514172]\n",
      "y_t [0.10029468 0.8817912  0.01791412]\n",
      "y_t [0.04447533 0.93544919 0.02007548]\n",
      "y_t [0.1216515  0.86522911 0.01311938]\n",
      "y_t [0.04054541 0.93931515 0.02013944]\n",
      "y_t [0.03277509 0.94131489 0.02591002]\n",
      "y_t [0.02836869 0.94476311 0.0268682 ]\n",
      "Loss: 2.2839909487461503\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Sequence length\n",
    "D = 3   # Input dimension\n",
    "H = 4   # Hidden state dimension\n",
    "O = 3   # Output dimension\n",
    "np.random.seed(42)\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "W_xh = np.random.randn(D, H)\n",
    "W_hh = np.random.randn(H, H)\n",
    "b_h = np.random.randn(H)\n",
    "W_hy = np.random.randn(H, O)\n",
    "b_y = np.random.randn(O)\n",
    "\n",
    "# Input data (randomly generated)\n",
    "x = np.random.rand(T, D)  # Input sequence\n",
    "print('x',x)\n",
    "y_true = np.random.randint(0, O, T)  # Ground truth labels (class indices)\n",
    "print('y',y_true)\n",
    "\n",
    "# One-hot encode y_true\n",
    "y_true_one_hot = np.zeros((T, O))\n",
    "y_true_one_hot[np.arange(T), y_true] = 1\n",
    "print('y_true_one_hot',y_true_one_hot)\n",
    "\n",
    "# Initialize hidden state\n",
    "h_t_prev = np.zeros(H)\n",
    "\n",
    "# Forward pass\n",
    "h = np.zeros((T, H))\n",
    "z = np.zeros((T, O))\n",
    "y_pred = np.zeros((T, O))\n",
    "\n",
    "for t in range(T):\n",
    "    h_t = np.tanh(np.dot(x[t], W_xh) + np.dot(h_t_prev, W_hh) + b_h)\n",
    "    z_t = np.dot(h_t, W_hy) + b_y\n",
    "    y_t = softmax(z_t)\n",
    "    print('y_t',y_t)\n",
    "\n",
    "    h[t] = h_t\n",
    "    z[t] = z_t\n",
    "    y_pred[t] = y_t\n",
    "\n",
    "    h_t_prev = h_t\n",
    "\n",
    "# Compute Cross-Entropy Loss\n",
    "loss = -np.sum(y_true_one_hot * np.log(y_pred + 1e-9)) / T\n",
    "print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [[0.26340316 0.12233592 0.45872423 0.9748125  0.68057437 0.19325373\n",
      "  0.04673923 0.95264394 0.85864902 0.82790318 0.96390052 0.59893645\n",
      "  0.93091584 0.14676881 0.11396031]\n",
      " [0.3720289  0.76656013 0.32114203 0.63803508 0.0092211  0.54346504\n",
      "  0.33775465 0.89802431 0.94070704 0.38412185 0.39611779 0.89727994\n",
      "  0.05882237 0.40537929 0.13017554]\n",
      " [0.08696941 0.78396429 0.30457336 0.58405587 0.87846618 0.88079206\n",
      "  0.60095421 0.64522775 0.0566282  0.2974841  0.69177461 0.18902945\n",
      "  0.80300754 0.51476375 0.75728604]\n",
      " [0.17778875 0.08262029 0.48207197 0.52885388 0.69630827 0.20476161\n",
      "  0.67137233 0.79326933 0.04173781 0.96335751 0.97539272 0.55066078\n",
      "  0.06490698 0.34523679 0.02042997]\n",
      " [0.80085181 0.2079271  0.14325251 0.69989398 0.05794981 0.25660757\n",
      "  0.51033178 0.99525878 0.14651568 0.44951478 0.60144038 0.09727249\n",
      "  0.28873488 0.7207999  0.55080606]\n",
      " [0.83857702 0.58033135 0.18457174 0.61550212 0.88695509 0.51678928\n",
      "  0.62614367 0.50505912 0.90966258 0.41330466 0.53546872 0.34264313\n",
      "  0.12957678 0.66282175 0.93567   ]\n",
      " [0.61272918 0.8427385  0.21790844 0.90317174 0.00976998 0.22581273\n",
      "  0.13205513 0.90754295 0.91023064 0.58105791 0.08749305 0.11687944\n",
      "  0.77107126 0.73279527 0.08712836]\n",
      " [0.3574632  0.77321162 0.1314716  0.53783243 0.75472357 0.27252618\n",
      "  0.566517   0.47668497 0.55662069 0.44073706 0.69373736 0.71823718\n",
      "  0.75638236 0.03732201 0.6788354 ]\n",
      " [0.4772208  0.10017139 0.61419623 0.83791451 0.73389404 0.32156558\n",
      "  0.06785531 0.03733024 0.5591253  0.1608527  0.26788819 0.23571196\n",
      "  0.01934746 0.15168642 0.03390251]\n",
      " [0.98183486 0.36048531 0.82546453 0.41223741 0.25317915 0.23671088\n",
      "  0.7731156  0.67125691 0.70633089 0.85315541 0.52204957 0.44276965\n",
      "  0.55374844 0.65199485 0.78855387]]\n",
      "y_true [ 3 15  2  9 14  9  7  6 17 16]\n",
      "y_true_one_hot [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "T = 10  # Sequence length (number of words in a sentence)\n",
    "D = 15  # Embedding dimension for each word\n",
    "H = 5 # Hidden state dimension\n",
    "V = 20 # Vocabulary size (output dimension for predictions)\n",
    "np.random.seed(10) # 랜덤시드고정\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "W_xh = np.random.randn(D, H)\n",
    "W_hh = np.random.randn(H, H)\n",
    "b_h = np.random.randn(H)\n",
    "W_hy = np.random.randn(H, V)\n",
    "b_y = np.random.randn(V)\n",
    "\n",
    "# Example input sentence represented as word embeddings (randomly initialized for simplicity)\n",
    "# In practice, these embeddings would come from a pretrained model like Word2Vec or GloVe.\n",
    "x = np.random.rand(T, D)  # Input sequence: each word is a D-dimensional vector\n",
    "print('x',x)\n",
    "\n",
    "# Ground truth next word indices for each time step\n",
    "y_true = np.random.randint(0, V, T)  # Ground truth indices for the next word\n",
    "print('y_true',y_true) # 다음 단어의 인덱스\n",
    "\n",
    "# One-hot encode y_true\n",
    "y_true_one_hot = np.zeros((T, V))\n",
    "y_true_one_hot[np.arange(T), y_true] = 1\n",
    "print('y_true_one_hot',y_true_one_hot) # 인덱스를 원핫인코딩으로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_t_prev [0. 0. 0. 0. 0.]\n",
      "h [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "z [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "y_pred [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize hidden state\n",
    "h_t_prev = np.zeros(H) # t-1 은닉 상태 배열 초기화\n",
    "print('h_t_prev',h_t_prev)\n",
    "\n",
    "# Forward pass\n",
    "h = np.zeros((T, H)) # 순전파를 통해 구한 은닉 상태를 넣을 배열, RNN 계층을 통과한 값\n",
    "print('h',h)\n",
    "z = np.zeros((T, V)) # 순전파를 통해 구한 출력을 넣을 배열, Affine 계층을 통과한 값\n",
    "print('z',z)\n",
    "y_pred = np.zeros((T, V))\n",
    "print('y_pred',y_pred) # 순전파를 통해 구한 예측값을 넣을 배열, Softmax 계층을 통과한 값\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_t 0 [2.47381030e-05 5.21334418e-03 3.33376681e-03 5.97924481e-04\n",
      " 3.98121183e-05 3.28415757e-04 5.72007517e-02 5.89502092e-02\n",
      " 1.90909033e-03 4.66516676e-03 1.34223532e-03 2.89771453e-04\n",
      " 3.81843565e-02 5.32812389e-04 2.32513718e-02 1.87152368e-04\n",
      " 4.50255248e-03 3.22924098e-02 3.62175459e-03 7.63532364e-01]\n",
      "y_t 1 [0.00052837 0.02480086 0.02317873 0.00382231 0.0013317  0.00884985\n",
      " 0.11542902 0.25734991 0.00574435 0.008209   0.05140938 0.00424132\n",
      " 0.03974955 0.04693805 0.00427987 0.00271072 0.07102185 0.06328218\n",
      " 0.11106932 0.15605363]\n",
      "y_t 2 [1.99276583e-04 3.54259782e-04 1.35343116e-03 6.79071453e-04\n",
      " 1.58169928e-04 6.57517122e-04 4.39795737e-02 8.23527600e-01\n",
      " 1.45440146e-03 1.10774445e-03 1.37813942e-03 5.73125385e-05\n",
      " 3.23433645e-02 3.50819326e-03 1.53307267e-05 6.84207834e-04\n",
      " 2.06806948e-03 4.60596912e-02 3.24312491e-02 7.98339662e-03]\n",
      "y_t 3 [1.78675034e-03 3.27217665e-03 1.64664028e-03 1.07459947e-03\n",
      " 2.42977424e-03 1.70845382e-03 5.57955420e-01 6.57822690e-02\n",
      " 2.91400722e-02 1.19063573e-02 9.02399584e-03 5.25950240e-04\n",
      " 1.72245563e-01 6.95336229e-03 2.47876913e-03 8.70008831e-03\n",
      " 8.23954846e-02 3.33720020e-02 1.65596707e-03 5.94630505e-03]\n",
      "y_t 4 [1.13597868e-04 3.78086675e-02 2.04763686e-03 4.69072935e-04\n",
      " 8.98256292e-04 7.51985671e-04 1.32195229e-01 8.27767279e-04\n",
      " 1.08279410e-02 6.90782312e-03 2.71552630e-02 7.21669986e-03\n",
      " 1.68733120e-02 7.12785309e-03 4.37183472e-01 9.58977171e-04\n",
      " 2.33607584e-01 3.11345235e-03 2.93252171e-04 7.36221570e-02]\n",
      "y_t 5 [7.29897611e-05 3.20595758e-02 9.08288005e-03 1.09423165e-03\n",
      " 4.01234383e-04 1.78596859e-03 1.01063262e-01 2.73987583e-02\n",
      " 4.26661020e-03 6.68930923e-03 3.56101215e-02 5.77282508e-03\n",
      " 1.87393138e-02 1.36342923e-02 4.18481690e-02 5.61952505e-04\n",
      " 9.07496219e-02 1.77937892e-02 1.67843489e-02 5.74590746e-01]\n",
      "y_t 6 [9.16802393e-06 1.67708633e-03 2.55493380e-03 4.13353934e-04\n",
      " 9.65163856e-06 1.77981720e-04 1.92994411e-02 1.41377161e-01\n",
      " 5.21005712e-04 1.86464444e-03 3.77768598e-04 7.27858685e-05\n",
      " 2.13935858e-02 2.08441292e-04 3.43620768e-03 6.25897669e-05\n",
      " 7.27269273e-04 3.59534114e-02 7.09968586e-03 7.62763827e-01]\n",
      "y_t 7 [7.74251972e-04 1.60002512e-03 3.72535533e-03 1.27344128e-03\n",
      " 1.27124084e-03 3.58744408e-03 1.03005221e-01 6.08740683e-01\n",
      " 3.78827745e-03 2.42130202e-03 1.91078539e-02 5.30095084e-04\n",
      " 2.90602111e-02 3.73210934e-02 2.95056092e-05 2.81299519e-03\n",
      " 2.88612799e-02 4.05409557e-02 1.05781654e-01 5.76711366e-03]\n",
      "y_t 8 [1.12823652e-04 4.18670407e-04 1.42203117e-03 7.08830649e-04\n",
      " 6.44663877e-05 3.73613527e-04 4.85170183e-02 7.69223577e-01\n",
      " 1.54251470e-03 1.69892878e-03 4.82527336e-04 3.37673630e-05\n",
      " 5.98529085e-02 8.75167427e-04 8.60203612e-05 4.49545595e-04\n",
      " 9.64292419e-04 6.96109187e-02 1.30192852e-02 3.05430929e-02]\n",
      "y_t 9 [1.44158146e-04 3.45701846e-02 1.15800821e-02 1.31661505e-03\n",
      " 6.60336935e-04 2.98322201e-03 1.58747960e-01 3.93734865e-02\n",
      " 6.22900290e-03 1.07037704e-02 4.76638684e-02 5.79400098e-03\n",
      " 2.79200150e-02 1.53749665e-02 3.75069010e-02 1.12432790e-03\n",
      " 1.45810552e-01 2.42654508e-02 2.00232089e-02 4.08207890e-01]\n"
     ]
    }
   ],
   "source": [
    "for t in range(T):\n",
    "    h_t = np.tanh(np.dot(x[t], W_xh) + np.dot(h_t_prev, W_hh) + b_h)\n",
    "    z_t = np.dot(h_t, W_hy) + b_y\n",
    "    y_t = softmax(z_t)\n",
    "    print('y_t', t, y_t) # RNN, Affine, Softmax 계층을 통과한 값 - > 각 단어의 확률값\n",
    "\n",
    "    h[t] = h_t\n",
    "    z[t] = z_t\n",
    "    y_pred[t] = y_t\n",
    "\n",
    "    h_t_prev = h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 3.9023\n",
      "delta_z [[ 2.47381030e-05  5.21334418e-03  3.33376681e-03 -9.99402076e-01\n",
      "   3.98121183e-05  3.28415757e-04  5.72007517e-02  5.89502092e-02\n",
      "   1.90909033e-03  4.66516676e-03  1.34223532e-03  2.89771453e-04\n",
      "   3.81843565e-02  5.32812389e-04  2.32513718e-02  1.87152368e-04\n",
      "   4.50255248e-03  3.22924098e-02  3.62175459e-03  7.63532364e-01]\n",
      " [ 5.28373073e-04  2.48008591e-02  2.31787347e-02  3.82231334e-03\n",
      "   1.33169954e-03  8.84985195e-03  1.15429024e-01  2.57349910e-01\n",
      "   5.74434604e-03  8.20900213e-03  5.14093826e-02  4.24132471e-03\n",
      "   3.97495469e-02  4.69380515e-02  4.27987277e-03 -9.97289283e-01\n",
      "   7.10218482e-02  6.32821837e-02  1.11069325e-01  1.56053634e-01]\n",
      " [ 1.99276583e-04  3.54259782e-04 -9.98646569e-01  6.79071453e-04\n",
      "   1.58169928e-04  6.57517122e-04  4.39795737e-02  8.23527600e-01\n",
      "   1.45440146e-03  1.10774445e-03  1.37813942e-03  5.73125385e-05\n",
      "   3.23433645e-02  3.50819326e-03  1.53307267e-05  6.84207834e-04\n",
      "   2.06806948e-03  4.60596912e-02  3.24312491e-02  7.98339662e-03]\n",
      " [ 1.78675034e-03  3.27217665e-03  1.64664028e-03  1.07459947e-03\n",
      "   2.42977424e-03  1.70845382e-03  5.57955420e-01  6.57822690e-02\n",
      "   2.91400722e-02 -9.88093643e-01  9.02399584e-03  5.25950240e-04\n",
      "   1.72245563e-01  6.95336229e-03  2.47876913e-03  8.70008831e-03\n",
      "   8.23954846e-02  3.33720020e-02  1.65596707e-03  5.94630505e-03]\n",
      " [ 1.13597868e-04  3.78086675e-02  2.04763686e-03  4.69072935e-04\n",
      "   8.98256292e-04  7.51985671e-04  1.32195229e-01  8.27767279e-04\n",
      "   1.08279410e-02  6.90782312e-03  2.71552630e-02  7.21669986e-03\n",
      "   1.68733120e-02  7.12785309e-03 -5.62816528e-01  9.58977171e-04\n",
      "   2.33607584e-01  3.11345235e-03  2.93252171e-04  7.36221570e-02]\n",
      " [ 7.29897611e-05  3.20595758e-02  9.08288005e-03  1.09423165e-03\n",
      "   4.01234383e-04  1.78596859e-03  1.01063262e-01  2.73987583e-02\n",
      "   4.26661020e-03 -9.93310691e-01  3.56101215e-02  5.77282508e-03\n",
      "   1.87393138e-02  1.36342923e-02  4.18481690e-02  5.61952505e-04\n",
      "   9.07496219e-02  1.77937892e-02  1.67843489e-02  5.74590746e-01]\n",
      " [ 9.16802393e-06  1.67708633e-03  2.55493380e-03  4.13353934e-04\n",
      "   9.65163856e-06  1.77981720e-04  1.92994411e-02 -8.58622839e-01\n",
      "   5.21005712e-04  1.86464444e-03  3.77768598e-04  7.27858685e-05\n",
      "   2.13935858e-02  2.08441292e-04  3.43620768e-03  6.25897669e-05\n",
      "   7.27269273e-04  3.59534114e-02  7.09968586e-03  7.62763827e-01]\n",
      " [ 7.74251972e-04  1.60002512e-03  3.72535533e-03  1.27344128e-03\n",
      "   1.27124084e-03  3.58744408e-03 -8.96994779e-01  6.08740683e-01\n",
      "   3.78827745e-03  2.42130202e-03  1.91078539e-02  5.30095084e-04\n",
      "   2.90602111e-02  3.73210934e-02  2.95056092e-05  2.81299519e-03\n",
      "   2.88612799e-02  4.05409557e-02  1.05781654e-01  5.76711366e-03]\n",
      " [ 1.12823652e-04  4.18670407e-04  1.42203117e-03  7.08830649e-04\n",
      "   6.44663877e-05  3.73613527e-04  4.85170183e-02  7.69223577e-01\n",
      "   1.54251470e-03  1.69892878e-03  4.82527336e-04  3.37673630e-05\n",
      "   5.98529085e-02  8.75167427e-04  8.60203612e-05  4.49545595e-04\n",
      "   9.64292419e-04 -9.30389081e-01  1.30192852e-02  3.05430929e-02]\n",
      " [ 1.44158146e-04  3.45701846e-02  1.15800821e-02  1.31661505e-03\n",
      "   6.60336935e-04  2.98322201e-03  1.58747960e-01  3.93734865e-02\n",
      "   6.22900290e-03  1.07037704e-02  4.76638684e-02  5.79400098e-03\n",
      "   2.79200150e-02  1.53749665e-02  3.75069010e-02  1.12432790e-03\n",
      "  -8.54189448e-01  2.42654508e-02  2.00232089e-02  4.08207890e-01]]\n",
      "dW_hy 9 [[ 5.75188231e-05  1.37934372e-02  4.62043050e-03  5.25326875e-04\n",
      "   2.63473167e-04  1.19029985e-03  6.33401309e-02  1.57099454e-02\n",
      "   2.48536018e-03  4.27078380e-03  1.90177919e-02  2.31179525e-03\n",
      "   1.11400323e-02  6.13458209e-03  1.49651814e-02  4.48604671e-04\n",
      "  -3.40819948e-01  9.68186822e-03  7.98922186e-03  1.62874163e-01]\n",
      " [-1.38341865e-04 -3.31753976e-02 -1.11128660e-02 -1.26349420e-03\n",
      "  -6.33694629e-04 -2.86285935e-03 -1.52343031e-01 -3.77849029e-02\n",
      "  -5.97768424e-03 -1.02719104e-02 -4.57407966e-02 -5.56023313e-03\n",
      "  -2.67935392e-02 -1.47546399e-02 -3.59936277e-02 -1.07896517e-03\n",
      "   8.19725866e-01 -2.32864240e-02 -1.92153419e-02 -3.91738117e-01]\n",
      " [-1.18847749e-04 -2.85005651e-02 -9.54692280e-03 -1.08545191e-03\n",
      "  -5.44399054e-04 -2.45944631e-03 -1.30875974e-01 -3.24605329e-02\n",
      "  -5.13535304e-03 -8.82446848e-03 -3.92953407e-02 -4.77672607e-03\n",
      "  -2.30179912e-02 -1.26755248e-02 -3.09216710e-02 -9.26925351e-04\n",
      "   7.04216140e-01 -2.00050728e-02 -1.65076575e-02 -3.36537269e-01]\n",
      " [ 1.31057294e-04  3.14285039e-02  1.05277036e-02  1.19696327e-03\n",
      "   6.00326615e-04  2.71211177e-03  1.44321210e-01  3.57952897e-02\n",
      "   5.66292151e-03  9.73102958e-03  4.33322555e-02  5.26745183e-03\n",
      "   2.53826906e-02  1.39777152e-02  3.40983364e-02  1.02215086e-03\n",
      "  -7.76562135e-01  2.20602471e-02  1.82035331e-02  3.71110637e-01]\n",
      " [ 1.33584273e-04  3.20344919e-02  1.07306932e-02  1.22004250e-03\n",
      "   6.11901800e-04  2.76440529e-03  1.47103937e-01  3.64854759e-02\n",
      "   5.77211101e-03  9.91865822e-03  4.41677655e-02  5.36901609e-03\n",
      "   2.58721064e-02  1.42472262e-02  3.47558028e-02  1.04185944e-03\n",
      "  -7.91535402e-01  2.24856013e-02  1.85545241e-02  3.78266199e-01]]\n",
      "dW_hy 8 [[ 1.45332124e-04  1.41192982e-02  5.72723080e-03  1.07702644e-03\n",
      "   3.13648873e-04  1.48109204e-03  1.01102067e-01  6.14414744e-01\n",
      "   3.68593565e-03  5.59310003e-03  1.93933542e-02  2.33807718e-03\n",
      "   5.77249586e-02  6.81574547e-03  1.50321331e-02  7.98496579e-04\n",
      "  -3.40069416e-01 -7.14461833e-01  1.81224377e-02  1.86646571e-01]\n",
      " [-2.51164053e-04 -3.35940626e-02 -1.25348787e-02 -1.97231566e-03\n",
      "  -6.98160181e-04 -3.23646803e-03 -2.00859420e-01 -8.06998504e-01\n",
      "  -7.52017893e-03 -1.19708171e-02 -4.62233177e-02 -5.59400006e-03\n",
      "  -8.66456716e-02 -1.56297960e-02 -3.60796469e-02 -1.52850493e-03\n",
      "   8.18761586e-01  9.07090591e-01 -3.22344583e-02 -4.22280814e-01]\n",
      " [-4.64978918e-05 -2.82320864e-02 -8.63502401e-03 -6.30903638e-04\n",
      "  -5.03059016e-04 -2.21986104e-03 -9.97637087e-02  4.60815612e-01\n",
      "  -4.14619237e-03 -7.73500494e-03 -3.89859128e-02 -4.75507224e-03\n",
      "   1.53635833e-02 -1.21143106e-02 -3.08665091e-02 -6.38647502e-04\n",
      "   7.04834507e-01 -6.16631012e-01 -8.15884579e-03 -3.16951053e-01]\n",
      " [ 2.43685239e-04  3.18464481e-02  1.19472680e-02  1.90456436e-03\n",
      "   6.64681178e-04  3.08507721e-03  1.92754070e-01  8.03684549e-01\n",
      "   7.20276052e-03  1.14270113e-02  4.38139458e-02  5.30116062e-03\n",
      "   8.51317766e-02  1.48513645e-02  3.41842076e-02  1.47091666e-03\n",
      "  -7.75599516e-01 -9.06714955e-01  3.12002347e-02  4.01600749e-01]\n",
      " [ 2.08855074e-05  3.16162850e-02  9.31023615e-03  5.11996463e-04\n",
      "   5.47506771e-04  2.39120533e-03  9.86406228e-02 -7.31886634e-01\n",
      "   4.23130374e-03  8.22161001e-03  4.36857723e-02  5.33528611e-03\n",
      "  -3.39145499e-02  1.33730275e-02  3.46698776e-02  5.92811451e-04\n",
      "  -7.92498627e-01  9.51844819e-01  5.54965014e-03  3.47756915e-01]]\n",
      "dW_hy 7 [[ 8.92689534e-04  1.56637446e-02  9.32318148e-03  2.30623322e-03\n",
      "   1.54073165e-03  4.94392199e-03 -7.64734532e-01  1.20201010e+00\n",
      "   7.34262278e-03  7.93029525e-03  3.78374741e-02  2.84975879e-03\n",
      "   8.57757287e-02  4.28404463e-02  1.50606138e-02  3.51377904e-03\n",
      "  -3.12210667e-01 -6.75329116e-01  1.20229641e-01  1.92213357e-01]\n",
      " [-1.02512657e-03 -3.51934895e-02 -1.62588413e-02 -3.24528086e-03\n",
      "  -1.96892577e-03 -6.82257094e-03  6.95800017e-01 -1.41551161e+00\n",
      "  -1.13070401e-02 -1.43912139e-02 -6.53240281e-02 -6.12389696e-03\n",
      "  -1.15695019e-01 -5.29369369e-02 -3.61091415e-02 -4.34044848e-03\n",
      "   7.89911096e-01  8.66564792e-01 -1.37976566e-01 -4.28045771e-01]\n",
      " [ 6.70268765e-04 -2.67508572e-02 -5.18626238e-03  5.47989387e-04\n",
      "   6.73796944e-04  1.10122873e-03 -9.30159979e-01  1.02435957e+00\n",
      "  -6.39180362e-04 -5.49347554e-03 -2.12967456e-02 -4.26433473e-03\n",
      "   4.22661824e-02  2.24358311e-02 -3.08391942e-02  1.96549331e-03\n",
      "   7.31552944e-01 -5.79100071e-01  8.97689161e-02 -3.11612126e-01]\n",
      " [ 9.43947102e-04  3.32935694e-02  1.53166159e-02  3.05631134e-03\n",
      "   1.81443799e-03  6.32969314e-03 -6.18520888e-01  1.35425194e+00\n",
      "   1.06290175e-02  1.36169256e-02  6.10957893e-02  5.78059804e-03\n",
      "   1.11414897e-01  4.86059294e-02  3.42108935e-02  4.01509262e-03\n",
      "  -7.49496316e-01 -8.70048228e-01  1.26873040e-01  4.06816738e-01]\n",
      " [ 7.94704941e-04  3.32154162e-02  1.30335103e-02  1.78472633e-03\n",
      "   1.81803743e-03  5.97664527e-03 -7.97853048e-01 -1.23486025e-01\n",
      "   8.01746486e-03  1.06415594e-02  6.27829515e-02  5.86508505e-03\n",
      "  -4.87057331e-03  5.06732714e-02  3.46993668e-02  3.40423516e-03\n",
      "  -7.63653470e-01  9.92363126e-01  1.11272209e-01  3.53520807e-01]]\n",
      "dW_hy 6 [[ 9.01856415e-04  1.73406219e-02  1.18777968e-02  2.71953563e-03\n",
      "   1.55038209e-03  5.12188152e-03 -7.45437497e-01  3.43494288e-01\n",
      "   7.86356355e-03  9.79470725e-03  3.82151956e-02  2.92253558e-03\n",
      "   1.07166648e-01  4.30488616e-02  1.84963931e-02  3.57636101e-03\n",
      "  -3.11483489e-01 -6.39380186e-01  1.27328441e-01  9.54882102e-01]\n",
      " [-1.03428793e-03 -3.68693565e-02 -1.88119176e-02 -3.65833427e-03\n",
      "  -1.97857039e-03 -7.00042326e-03  6.76514607e-01 -5.57513018e-01\n",
      "  -1.18276671e-02 -1.62545027e-02 -6.57015220e-02 -6.19662992e-03\n",
      "  -1.37073050e-01 -5.31452266e-02 -3.95428509e-02 -4.40299275e-03\n",
      "   7.89184356e-01  8.30637520e-01 -1.45071090e-01 -1.19025504e+00]\n",
      " [ 6.63064408e-04 -2.80687343e-02 -7.19396394e-03  2.23170272e-04\n",
      "   6.66212556e-04  9.61368284e-04 -9.45325741e-01  1.69907702e+00\n",
      "  -1.04859370e-03 -6.95873837e-03 -2.15936013e-02 -4.32153085e-03\n",
      "   2.54548131e-02  2.22720351e-02 -3.35394127e-02  1.91630943e-03\n",
      "   7.30981446e-01 -6.07352748e-01  8.41898868e-02 -9.11002264e-01]\n",
      " [ 9.52664084e-04  3.48881477e-02  1.77458539e-02  3.44932938e-03\n",
      "   1.82361480e-03  6.49891864e-03 -6.00170927e-01  5.37871006e-01\n",
      "   1.11243912e-02  1.53898347e-02  6.14549727e-02  5.84980305e-03\n",
      "   1.31755976e-01  4.88041160e-02  3.74780491e-02  4.07460314e-03\n",
      "  -7.48804827e-01 -8.35863627e-01  1.33623441e-01  1.13205466e+00]\n",
      " [ 7.86164521e-04  3.16531362e-02  1.06534765e-02  1.39966887e-03\n",
      "   1.80904650e-03  5.81084743e-03 -8.15831330e-01  6.76359109e-01\n",
      "   7.53212501e-03  8.90456064e-03  6.24310434e-02  5.79728180e-03\n",
      "  -2.47996443e-02  5.04790991e-02  3.14983874e-02  3.34593002e-03\n",
      "  -7.64330954e-01  9.58870934e-01  1.04658538e-01 -3.57027419e-01]]\n",
      "dW_hy 5 [[ 9.36728272e-04  3.26575229e-02  1.62172670e-02  3.24231977e-03\n",
      "   1.74207730e-03  5.97515246e-03 -6.97153140e-01  3.56584420e-01\n",
      "   9.90199495e-03 -4.64773068e-01  5.52284185e-02  5.68058176e-03\n",
      "   1.16119611e-01  4.95628313e-02  3.84899285e-02  3.84484151e-03\n",
      "  -2.68126615e-01 -6.30878960e-01  1.35347394e-01  1.22940069e+00]\n",
      " [-1.09896961e-03 -6.52797412e-02 -2.68609360e-02 -4.62801467e-03\n",
      "  -2.33413414e-03 -8.58310342e-03  5.86954902e-01 -5.81793104e-01\n",
      "  -1.56086290e-02  8.63992291e-01 -9.72583104e-02 -1.13123614e-02\n",
      "  -1.53679356e-01 -6.52275913e-02 -7.66276391e-02 -4.90098083e-03\n",
      "   7.08764338e-01  8.14869114e-01 -1.59944955e-01 -1.69944282e+00]\n",
      " [ 5.90626334e-04 -5.98859903e-02 -1.62081917e-02 -8.62790713e-04\n",
      "   2.68010872e-04 -8.11101203e-04 -1.04562512e+00  1.67188535e+00\n",
      "  -5.28295505e-03  9.78844089e-01 -5.69345664e-02 -1.00507225e-02\n",
      "   6.85713899e-03  8.74079658e-03 -7.50712755e-02  1.35860440e-03\n",
      "   6.40917749e-01 -6.25012045e-01  6.75324011e-02 -1.48125001e+00]\n",
      " [ 1.02565076e-03  6.69463696e-02  2.68283504e-02  4.54351482e-03\n",
      "   2.22483223e-03  8.28481181e-03 -4.99111933e-01  5.65268607e-01\n",
      "   1.53908212e-02 -9.77878907e-01  9.70635903e-02  1.16223843e-02\n",
      "   1.50494499e-01  6.24378324e-02  7.93244508e-02  4.63653191e-03\n",
      "  -6.58059037e-01 -8.18070589e-01  1.50407081e-01  1.70662114e+00]\n",
      " [ 8.58943856e-04  6.36202857e-02  1.97101710e-02  2.49074590e-03\n",
      "   2.20912414e-03  7.59166715e-03 -7.15059429e-01  7.03678877e-01\n",
      "   1.17864348e-02 -9.81542459e-01  9.79385024e-02  1.15534641e-02\n",
      "  -6.11435507e-03  6.40740843e-02  7.32259099e-02  3.90626244e-03\n",
      "  -6.73842959e-01  9.76613424e-01  1.21394498e-01  2.15906807e-01]]\n",
      "dW_hy 4 [[ 8.23265226e-04 -5.10627193e-03  1.41720604e-02  2.77380355e-03\n",
      "   8.44887094e-04  5.22405927e-03 -8.29191475e-01  3.55757635e-01\n",
      "  -9.13095112e-04 -4.71672692e-01  2.81053843e-02 -1.52755308e-03\n",
      "   9.92663252e-02  4.24434378e-02  6.00638486e-01  2.88700248e-03\n",
      "  -5.01456945e-01 -6.33988717e-01  1.35054490e-01  1.15586591e+00]\n",
      " [-1.17683886e-03 -9.11968903e-02 -2.82645535e-02 -4.94955557e-03\n",
      "  -2.94987238e-03 -9.09857583e-03  4.96337494e-01 -5.82360523e-01\n",
      "  -2.30309843e-02  8.59257104e-01 -1.15872746e-01 -1.62592770e-02\n",
      "  -1.65245702e-01 -7.01136041e-02  3.09172782e-01 -5.55834212e-03\n",
      "   5.48630618e-01  8.12734900e-01 -1.60145974e-01 -1.74990946e+00]\n",
      " [ 4.86727133e-04 -9.44666601e-02 -1.80810071e-02 -1.29181552e-03\n",
      "  -5.53554823e-04 -1.49888445e-03 -1.16653390e+00  1.67112826e+00\n",
      "  -1.51864369e-02  9.72526036e-01 -8.17713894e-02 -1.66512808e-02\n",
      "  -8.57557707e-03  2.22149954e-03  4.39693560e-01  4.81502002e-04\n",
      "   4.27254924e-01 -6.27859679e-01  6.72641860e-02 -1.54858651e+00]\n",
      " [ 1.13905970e-03  1.04692156e-01  2.88725818e-02  5.01180762e-03\n",
      "   3.12159460e-03  9.03554682e-03 -3.67136564e-01  5.66094998e-01\n",
      "   2.62007538e-02 -9.70982572e-01  1.24173690e-01  1.88270818e-02\n",
      "   1.67339748e-01  6.95538309e-02 -4.82556034e-01  5.59391417e-03\n",
      "  -4.24839976e-01 -8.14962315e-01  1.50699845e-01  1.78012085e+00]\n",
      " [ 9.32181927e-04  8.79960414e-02  2.10303100e-02  2.79316352e-03\n",
      "   2.78824204e-03  8.07648242e-03 -6.29831390e-01  7.04212550e-01\n",
      "   1.87673538e-02 -9.77088892e-01  1.15445865e-01  1.62061674e-02\n",
      "   4.76409581e-03  6.86695070e-02 -2.89629465e-01  4.52452791e-03\n",
      "  -5.23233012e-01  9.78620709e-01  1.21583562e-01  2.63372000e-01]]\n",
      "dW_hy 3 [[-6.77228914e-04 -7.85421147e-03  1.27892292e-02  1.87136621e-03\n",
      "  -1.19561175e-03  3.78931772e-03 -1.29775657e+00  3.00514377e-01\n",
      "  -2.53846213e-02  3.58117956e-01  2.05271275e-02 -1.96924056e-03\n",
      "  -4.53836866e-02  3.66040773e-02  5.98556842e-01 -4.41924017e-03\n",
      "  -5.70651807e-01 -6.62014173e-01  1.33663826e-01  1.15087227e+00]\n",
      " [-2.96304337e-03 -9.44680673e-02 -2.99106908e-02 -6.02382677e-03\n",
      "  -5.37890435e-03 -1.08065077e-02 -6.14474769e-02 -6.48122696e-01\n",
      "  -5.21621545e-02  1.84704890e+00 -1.24893985e-01 -1.67850665e-02\n",
      "  -3.37438646e-01 -7.70648422e-02  3.06694770e-01 -1.42557727e-02\n",
      "   4.66260304e-01  7.79373092e-01 -1.61801435e-01 -1.75585395e+00]\n",
      " [ 2.02696170e-03 -9.16459416e-02 -1.66615518e-02 -3.65477228e-04\n",
      "   1.54098642e-03 -2.61439257e-05 -6.85558906e-01  1.72783463e+00\n",
      "   9.93321496e-03  1.20758472e-01 -7.39924233e-02 -1.61978953e-02\n",
      "   1.39905475e-01  8.21551489e-03  4.41830336e-01  7.98124971e-03\n",
      "   4.98282403e-01 -5.99091972e-01  6.86916813e-02 -1.54346061e+00]\n",
      " [ 2.92286293e-03  1.07958935e-01  3.05165060e-02  6.08463463e-03\n",
      "   5.54736111e-03  1.07411827e-02  1.89898551e-01  6.31768764e-01\n",
      "   5.52927617e-02 -1.95744643e+00  1.33182802e-01  1.93521645e-02\n",
      "   3.39301205e-01  7.64957242e-02 -4.80081353e-01  1.42796523e-02\n",
      "  -3.42580396e-01 -7.81645358e-01  1.52353081e-01  1.78605735e+00]\n",
      " [ 7.85708147e-05  8.64327752e-02  2.02436358e-02  2.27977895e-03\n",
      "   1.62742945e-03  7.26027708e-03 -8.96391824e-01  6.72785397e-01\n",
      "   4.84582880e-03 -5.05032055e-01  1.11134696e-01  1.59548973e-02\n",
      "  -7.75253675e-02  6.53475726e-02 -2.90813685e-01  3.68103862e-04\n",
      "  -5.62597046e-01  9.62677400e-01  1.20792432e-01  2.60531182e-01]]\n",
      "dW_hy 2 [[-4.79947041e-04 -7.50349774e-03 -9.75861131e-01  2.54364033e-03\n",
      "  -1.03902507e-03  4.44025325e-03 -1.25421722e+00  1.11579867e+00\n",
      "  -2.39447780e-02  3.59214612e-01  2.18914721e-02 -1.91250171e-03\n",
      "  -1.33640713e-02  4.00771544e-02  5.98572019e-01 -3.74188109e-03\n",
      "  -5.68604438e-01 -6.16415528e-01  1.65770446e-01  1.15877575e+00]\n",
      " [-3.16231349e-03 -9.48223156e-02  9.68703502e-01 -6.70287621e-03\n",
      "  -5.53706915e-03 -1.14640035e-02 -1.05425625e-01 -1.47162360e+00\n",
      "  -5.36165088e-02  1.84594119e+00 -1.26272080e-01 -1.68423772e-02\n",
      "  -3.69780962e-01 -8.05729217e-02  3.06679440e-01 -1.49399583e-02\n",
      "   4.64192302e-01  7.33314894e-01 -1.94231633e-01 -1.76383708e+00]\n",
      " [ 2.21783764e-03 -9.13066159e-02 -9.73209504e-01  2.84967512e-04\n",
      "   1.69248859e-03  6.03655120e-04 -6.43433321e-01  2.51664587e+00\n",
      "   1.13263051e-02  1.21819518e-01 -7.26723803e-02 -1.61429988e-02\n",
      "   1.70885383e-01  1.15758179e-02  4.41845021e-01  8.63661430e-03\n",
      "   5.00263292e-01 -5.54973958e-01  9.97557693e-02 -1.53581376e+00]\n",
      " [ 3.12201876e-03  1.08312981e-01 -9.67524947e-01  6.76329461e-03\n",
      "   5.70543520e-03  1.13983014e-02  2.33851476e-01  1.45479736e+00\n",
      "   5.67462819e-02 -1.95633936e+00  1.34560106e-01  1.94094423e-02\n",
      "   3.71624971e-01  8.00017917e-02 -4.80066032e-01  1.49634456e-02\n",
      "  -3.40513580e-01 -7.35613576e-01  1.84764679e-01  1.79403591e+00]\n",
      " [ 4.79105483e-05  8.63782696e-02  1.73893248e-01  2.17529848e-03\n",
      "   1.60309376e-03  7.15911291e-03 -9.03158427e-01  5.46079213e-01\n",
      "   4.62205772e-03 -5.05202490e-01  1.10922658e-01  1.59460793e-02\n",
      "  -8.25016480e-02  6.48078095e-02 -2.90816043e-01  2.62833117e-04\n",
      "  -5.62915235e-01  9.55590755e-01  1.15802630e-01  2.59302874e-01]]\n",
      "dW_hy 1 [[ 4.65369950e-05  1.72086935e-02 -9.52765265e-01  6.35228815e-03\n",
      "   2.87913382e-04  1.32584653e-02 -1.13920088e+00  1.37222850e+00\n",
      "  -1.82209691e-02  3.67394266e-01  7.31170561e-02  2.31365944e-03\n",
      "   2.62433633e-02  8.68473932e-02  6.02836591e-01 -9.97465661e-01\n",
      "  -4.97836507e-01 -5.53359591e-01  2.76442676e-01  1.31427147e+00]\n",
      " [-3.54726326e-03 -1.12891149e-01  9.51816479e-01 -9.48764837e-03\n",
      "  -6.50728783e-03 -1.79116229e-02 -1.89522219e-01 -1.65911761e+00\n",
      "  -5.78015909e-02  1.83996046e+00 -1.63726732e-01 -1.99324229e-02\n",
      "  -3.98740762e-01 -1.14769956e-01  3.03561310e-01  7.11641877e-01\n",
      "   4.12448855e-01  6.87210233e-01 -2.75151939e-01 -1.87753101e+00]\n",
      " [ 2.09344438e-03 -9.71454067e-02 -9.78666403e-01 -6.14908086e-04\n",
      "   1.37897062e-03 -1.47983859e-03 -6.70608425e-01  2.45605876e+00\n",
      "   9.97393124e-03  1.19886898e-01 -8.47755348e-02 -1.71415209e-02\n",
      "   1.61527268e-01  5.25335282e-04  4.40837423e-01  2.43425396e-01\n",
      "   4.83542834e-01 -5.69872290e-01  7.36070562e-02 -1.57255299e+00]\n",
      " [ 3.45314450e-03  1.23855413e-01 -9.52999083e-01  9.15869739e-03\n",
      "   6.53999702e-03  1.69444087e-02  3.06189608e-01  1.61607579e+00\n",
      "   6.03462020e-02 -1.95119486e+00  1.66777815e-01  2.20674351e-02\n",
      "   3.96535586e-01  1.09417365e-01 -4.77383882e-01 -6.10027055e-01\n",
      "  -2.96004949e-01 -6.95955310e-01  2.54370634e-01  1.89183305e+00]\n",
      " [ 5.64754899e-04  1.10637992e-01  1.96566240e-01  5.91421167e-03\n",
      "   2.90573657e-03  1.58158674e-02 -7.90247982e-01  7.97813932e-01\n",
      "   1.02410662e-02 -4.97172602e-01  1.61210325e-01  2.00948613e-02\n",
      "  -4.36194077e-02  1.10721706e-01 -2.86629554e-01 -9.75266312e-01\n",
      "  -4.93443033e-01  1.01749217e+00  2.24448501e-01  4.11951530e-01]]\n",
      "dW_hy 0 [[ 5.57997693e-05  1.91607441e-02 -9.51516991e-01 -3.67857321e-01\n",
      "   3.02820373e-04  1.33814351e-02 -1.11778300e+00  1.39430143e+00\n",
      "  -1.75061418e-02  3.69141060e-01  7.36196340e-02  2.42215958e-03\n",
      "   4.05408652e-02  8.70468960e-02  6.11542683e-01 -9.97395585e-01\n",
      "  -4.96150600e-01 -5.41268231e-01  2.77798782e-01  1.60016356e+00]\n",
      " [-3.57138269e-03 -1.17974113e-01  9.48566086e-01  9.64920488e-01\n",
      "  -6.54610429e-03 -1.82318253e-02 -2.45292443e-01 -1.71659354e+00\n",
      "  -5.96629370e-02  1.83541197e+00 -1.65035400e-01 -2.02149475e-02\n",
      "  -4.35970171e-01 -1.15289443e-01  2.80891429e-01  7.11459405e-01\n",
      "   4.08058906e-01  6.55725420e-01 -2.78683117e-01 -2.62196828e+00]\n",
      " [ 2.07287504e-03 -1.01480219e-01 -9.81438377e-01  8.30371974e-01\n",
      "   1.34586748e-03 -1.75291106e-03 -7.18169938e-01  2.40704260e+00\n",
      "   8.38655309e-03  1.16007886e-01 -8.58915820e-02 -1.73824613e-02\n",
      "   1.29777585e-01  8.23102808e-05  4.21504278e-01  2.43269781e-01\n",
      "   4.79799034e-01 -5.96722913e-01  7.05956250e-02 -2.20741797e+00]\n",
      " [ 3.47788112e-03  1.29068443e-01 -9.49665517e-01 -9.90183197e-01\n",
      "   6.57980674e-03  1.72728047e-02  3.63386915e-01  1.67502245e+00\n",
      "   6.22551773e-02 -1.94652998e+00  1.68119970e-01  2.23571891e-02\n",
      "   4.34717643e-01  1.09950146e-01 -4.54133910e-01 -6.09839914e-01\n",
      "  -2.91502668e-01 -6.63664844e-01  2.57992170e-01  2.65531943e+00]\n",
      " [ 5.46144377e-04  1.06715983e-01  1.94058241e-01  7.57766292e-01\n",
      "   2.87578584e-03  1.55687996e-02 -8.33280216e-01  7.53465578e-01\n",
      "   8.80485396e-03 -5.00682216e-01  1.60200559e-01  1.98768657e-02\n",
      "  -7.23455717e-02  1.10320871e-01 -3.04121606e-01 -9.75407107e-01\n",
      "  -4.96830312e-01  9.93198526e-01  2.21723848e-01 -1.62455318e-01]]\n",
      "Gradients:\n",
      "dW_xh:\n",
      " [[ 2.1670455   0.8481598  -1.69434184  0.85943413 -0.15139834]\n",
      " [ 0.92016607  0.49477505 -1.44078898  1.26681    -0.5127755 ]\n",
      " [ 2.06077283  0.18097369 -1.34666733  0.64485089 -0.31610706]\n",
      " [ 1.80050116  0.49717335 -2.67284033  1.07284772 -0.27529728]\n",
      " [ 1.56143467  0.70261755 -1.46814433  0.3687578  -0.60696623]\n",
      " [ 0.88835699  0.43616244 -0.63326884  0.76113867 -0.60209427]\n",
      " [ 1.41124477  0.64054516 -0.34849653  0.79779387 -0.55280374]\n",
      " [ 0.95268282  0.45136208 -1.51106907  1.39302344 -0.36144645]\n",
      " [ 1.8037859   0.70169795 -2.57704342  1.4749095   0.01275875]\n",
      " [ 1.41567913  0.36945656 -1.48198252  0.8644695  -0.24423051]\n",
      " [ 1.15970655  0.44205497 -0.91202151  0.83301749 -0.46432988]\n",
      " [ 1.10073069  0.22358725 -0.8097592   1.34251197 -0.12583045]\n",
      " [ 0.8538029   0.09174483 -1.92212298  0.56842732 -0.51013711]\n",
      " [ 1.13959847  0.66930199 -1.08380683  0.69355431 -0.38985247]\n",
      " [ 1.37472861  0.91254203 -0.49177336  0.61979772 -0.65643787]]\n",
      "dW_hh:\n",
      " [[ 3.05082893 -0.88460278 -2.29959982  1.03327849 -0.86955723]\n",
      " [-3.34327051 -0.74324971  2.39970172 -1.76344461  0.59301737]\n",
      " [ 2.10075141 -0.56189514  0.57418559 -1.05523512 -0.05849565]\n",
      " [ 3.16140852  1.00514921 -2.43737418  1.78079247 -0.51253911]\n",
      " [-0.25838411  0.43388093 -2.22789794 -1.31509626 -0.43814499]]\n",
      "db_h:\n",
      " [ 3.06011326  0.89147788 -3.07598935  1.80089097 -0.66663023]\n",
      "dW_hy:\n",
      " [[ 5.57997693e-05  1.91607441e-02 -9.51516991e-01 -3.67857321e-01\n",
      "   3.02820373e-04  1.33814351e-02 -1.11778300e+00  1.39430143e+00\n",
      "  -1.75061418e-02  3.69141060e-01  7.36196340e-02  2.42215958e-03\n",
      "   4.05408652e-02  8.70468960e-02  6.11542683e-01 -9.97395585e-01\n",
      "  -4.96150600e-01 -5.41268231e-01  2.77798782e-01  1.60016356e+00]\n",
      " [-3.57138269e-03 -1.17974113e-01  9.48566086e-01  9.64920488e-01\n",
      "  -6.54610429e-03 -1.82318253e-02 -2.45292443e-01 -1.71659354e+00\n",
      "  -5.96629370e-02  1.83541197e+00 -1.65035400e-01 -2.02149475e-02\n",
      "  -4.35970171e-01 -1.15289443e-01  2.80891429e-01  7.11459405e-01\n",
      "   4.08058906e-01  6.55725420e-01 -2.78683117e-01 -2.62196828e+00]\n",
      " [ 2.07287504e-03 -1.01480219e-01 -9.81438377e-01  8.30371974e-01\n",
      "   1.34586748e-03 -1.75291106e-03 -7.18169938e-01  2.40704260e+00\n",
      "   8.38655309e-03  1.16007886e-01 -8.58915820e-02 -1.73824613e-02\n",
      "   1.29777585e-01  8.23102808e-05  4.21504278e-01  2.43269781e-01\n",
      "   4.79799034e-01 -5.96722913e-01  7.05956250e-02 -2.20741797e+00]\n",
      " [ 3.47788112e-03  1.29068443e-01 -9.49665517e-01 -9.90183197e-01\n",
      "   6.57980674e-03  1.72728047e-02  3.63386915e-01  1.67502245e+00\n",
      "   6.22551773e-02 -1.94652998e+00  1.68119970e-01  2.23571891e-02\n",
      "   4.34717643e-01  1.09950146e-01 -4.54133910e-01 -6.09839914e-01\n",
      "  -2.91502668e-01 -6.63664844e-01  2.57992170e-01  2.65531943e+00]\n",
      " [ 5.46144377e-04  1.06715983e-01  1.94058241e-01  7.57766292e-01\n",
      "   2.87578584e-03  1.55687996e-02 -8.33280216e-01  7.53465578e-01\n",
      "   8.80485396e-03 -5.00682216e-01  1.60200559e-01  1.98768657e-02\n",
      "  -7.23455717e-02  1.10320871e-01 -3.04121606e-01 -9.75407107e-01\n",
      "  -4.96830312e-01  9.93198526e-01  2.21723848e-01 -1.62455318e-01]]\n",
      "db_y:\n",
      " [ 0.00376613  0.14177485 -0.94007451 -0.98855055  0.00726464  0.02120445\n",
      "  0.3373929   1.79255142  0.06542326 -1.94382595  0.19355116  0.02453453\n",
      "  0.45636218  0.13247423 -0.44988438 -0.98174745 -0.33929145 -0.63371574\n",
      "  0.31177973  2.78901053]\n"
     ]
    }
   ],
   "source": [
    "# Compute Cross-Entropy Loss\n",
    "loss = -np.sum(y_true_one_hot * np.log(y_pred + 1e-9)) / T\n",
    "\n",
    "print(f\"Cross-Entropy Loss: {loss:.4f}\")\n",
    "\n",
    "# Backward pass (gradients)\n",
    "\n",
    "dW_hy = np.zeros_like(W_hy)\n",
    "db_y = np.zeros_like(b_y)\n",
    "dh_next = np.zeros(H)\n",
    "dW_xh = np.zeros_like(W_xh)\n",
    "dW_hh = np.zeros_like(W_hh)\n",
    "db_h = np.zeros_like(b_h)\n",
    "\n",
    "delta_z = y_pred - y_true_one_hot # Softmax + Cross-Entropy gradient, y - y_true\n",
    "print('delta_z',delta_z) # 리스트 내 각 시각 별 예측값과 실제값의 차이가 저장된 배열, 순서대로 t=0, t=1, ..., t=T-1 저장되어 있음\n",
    "\n",
    "\n",
    "# 역전파는 순전파의 역순으로 진행\n",
    "for t in reversed(range(T)):\n",
    "    # Gradients w.r.t. W_hy and b_y\n",
    "    dW_hy += np.outer(h[t], delta_z[t]) # W_hy의 기울기\n",
    "    print('dW_hy',t,dW_hy)\n",
    "    db_y += delta_z[t]                 # b_y의 기울기\n",
    "\n",
    "    # Backpropagate into hidden state\n",
    "    dh = np.dot(delta_z[t], W_hy.T) + dh_next\n",
    "    dh_raw = dh * (1 - h[t] ** 2)  # tanh derivative\n",
    "\n",
    "    # Gradients w.r.t. W_xh, W_hh, b_h\n",
    "    dW_xh += np.outer(x[t], dh_raw)\n",
    "    dW_hh += np.outer(h[t - 1] if t > 0 else np.zeros(H), dh_raw)\n",
    "    db_h += dh_raw\n",
    "\n",
    "    dh_next = np.dot(dh_raw, W_hh.T)\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradients:\")\n",
    "print(\"dW_xh:\\n\", dW_xh)\n",
    "print(\"dW_hh:\\n\", dW_hh)\n",
    "print(\"db_h:\\n\", db_h)\n",
    "print(\"dW_hy:\\n\", dW_hy)\n",
    "print(\"db_y:\\n\", db_y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
