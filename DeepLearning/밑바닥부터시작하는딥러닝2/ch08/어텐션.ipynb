{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# ptb 모듈 경로 추가\n",
    "import sys\n",
    "sys.path.append('C:/Users/KimDongyoung/Desktop/Github/my_git/mygit/DEEPLEARNING/밑바닥부터시작하는딥러닝2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Softmax\n",
    "from common.time_layers import *\n",
    "from ch07.seq2seq import Encoder, Seq2seq\n",
    "from common.base_model import BaseModel\n",
    "import pickle\n",
    "from common.util import to_cpu, to_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.78862847  0.43650985  0.09649747 -1.8634927 ]\n",
      " [-0.2773882  -0.35475898 -0.08274148 -0.62700068]\n",
      " [-0.04381817 -0.47721803 -1.31386475  0.88462238]\n",
      " [ 0.88131804  1.70957306  0.05003364 -0.40467741]\n",
      " [-0.54535995 -1.54647732  0.98236743 -1.10106763]]\n",
      "(5, 4)\n",
      "(5, 4)\n",
      "[[ 1.43090278e+00  3.49207880e-01  7.71979745e-02 -1.49079416e+00]\n",
      " [-2.77388203e-02 -3.54758979e-02 -8.27414815e-03 -6.27000677e-02]\n",
      " [-1.31454507e-03 -1.43165409e-02 -3.94159426e-02  2.65386714e-02]\n",
      " [ 4.40659021e-02  8.54786532e-02  2.50168211e-03 -2.02338707e-02]\n",
      " [-1.09071990e-02 -3.09295463e-02  1.96473487e-02 -2.20213526e-02]]\n",
      "[ 1.43500812  0.35396455  0.05165691 -1.56921078]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "T, H = 5, 4 # T: 시계열의 길이, H: 은닉 상태의 차원 수\n",
    "hs = np.random.randn(T, H) # Encoder에서 출력된 각 시각의 은닉 상태\n",
    "print(hs)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02]) # 가중치\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "print(t)\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print(c) # 맥락 벡터\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.78862847e+00  4.36509851e-01  9.64974681e-02 -1.86349270e+00]\n",
      "  [-2.77388203e-01 -3.54758979e-01 -8.27414815e-02 -6.27000677e-01]\n",
      "  [-4.38181690e-02 -4.77218030e-01 -1.31386475e+00  8.84622380e-01]\n",
      "  [ 8.81318042e-01  1.70957306e+00  5.00336422e-02 -4.04677415e-01]\n",
      "  [-5.45359948e-01 -1.54647732e+00  9.82367434e-01 -1.10106763e+00]]\n",
      "\n",
      " [[-1.18504653e+00 -2.05649899e-01  1.48614836e+00  2.36716267e-01]\n",
      "  [-1.02378514e+00 -7.12993200e-01  6.25244966e-01 -1.60513363e-01]\n",
      "  [-7.68836350e-01 -2.30030722e-01  7.45056266e-01  1.97611078e+00]\n",
      "  [-1.24412333e+00 -6.26416911e-01 -8.03766095e-01 -2.41908317e+00]\n",
      "  [-9.23792022e-01 -1.02387576e+00  1.12397796e+00 -1.31914233e-01]]\n",
      "\n",
      " [[-1.62328545e+00  6.46675452e-01 -3.56270759e-01 -1.74314104e+00]\n",
      "  [-5.96649642e-01 -5.88594380e-01 -8.73882298e-01  2.97138154e-02]\n",
      "  [-2.24825777e+00 -2.67761865e-01  1.01318344e+00  8.52797841e-01]\n",
      "  [ 1.10818750e+00  1.11939066e+00  1.48754313e+00 -1.11830068e+00]\n",
      "  [ 8.45833407e-01 -1.86088953e+00 -6.02885104e-01 -1.91447204e+00]]\n",
      "\n",
      " [[ 1.04814751e+00  1.33373782e+00 -1.97414679e-01  1.77464503e+00]\n",
      "  [-6.74727510e-01  1.50616865e-01  1.52945703e-01 -1.06419527e+00]\n",
      "  [ 4.37946611e-01  1.93897846e+00 -1.02493087e+00  8.99338446e-01]\n",
      "  [-1.54506852e-01  1.76962730e+00  4.83788348e-01  6.76216400e-01]\n",
      "  [ 6.43163281e-01  2.49086707e-01 -1.39576350e+00  1.39166291e+00]]\n",
      "\n",
      " [[-1.37066901e+00  2.38563192e-01  6.14077088e-01 -8.37912273e-01]\n",
      "  [ 1.45063214e-01  1.16788229e+00 -2.41044701e-02 -8.88657418e-01]\n",
      "  [-2.91573775e+00 -9.71840503e-01 -5.91078738e-01 -5.16417368e-01]\n",
      "  [-9.59996180e-01  3.77295234e-01 -5.74708420e-01 -1.09454334e-01]\n",
      "  [ 6.79071600e-01 -8.55437169e-01 -3.00206075e-01  2.15814934e+00]]\n",
      "\n",
      " [[ 8.74285723e-01 -1.29353663e+00 -7.97409382e-02  5.64485518e-01]\n",
      "  [ 1.23347104e+00  1.48986395e-01 -5.30582144e-01 -7.30526644e-01]\n",
      "  [ 6.45061985e-01  3.13060374e-01 -5.16647925e-01 -1.89071666e-01]\n",
      "  [-4.16198015e-01  7.24657658e-01 -6.89960677e-01  4.86414475e-01]\n",
      "  [ 8.51518950e-01  4.86249326e-01 -8.34239851e-01  1.34499246e+00]]\n",
      "\n",
      " [[-6.78212679e-01  4.26435074e-01 -7.53334794e-01 -1.74411025e+00]\n",
      "  [ 2.25750266e-01  2.87035165e-01 -7.74409606e-02  2.76068497e-01]\n",
      "  [-6.48410888e-01 -7.37464837e-01 -1.68090099e-01  1.90927681e+00]\n",
      "  [ 8.14814541e-01 -5.19991754e-01  5.58713205e-01 -4.78364660e-01]\n",
      "  [-4.57260787e-01  8.59284008e-01 -5.25264645e-01 -1.67563463e+00]]\n",
      "\n",
      " [[-9.06494701e-01  8.84152062e-02  1.28007821e-01  1.24161652e+00]\n",
      "  [-7.16025798e-01  7.31465736e-01  4.25966750e-01 -1.49013772e-01]\n",
      "  [ 8.35843902e-01  4.92118903e-01 -8.62308487e-01  1.07168393e+00]\n",
      "  [-1.22090192e+00  5.96154331e-02  2.44416199e-03  4.24635721e-01]\n",
      "  [-7.25433480e-01 -3.49433856e-02 -1.40620027e-01  9.97088367e-01]]\n",
      "\n",
      " [[-7.95914693e-01  7.27455292e-02 -2.61240485e-01 -1.29804664e+00]\n",
      "  [ 2.67611247e+00 -7.12190272e-02 -1.48665807e+00  1.40862696e+00]\n",
      "  [-1.07058550e+00  3.70869972e-01  8.62832095e-01 -6.48432023e-01]\n",
      "  [-4.30890055e-01 -5.40270264e-01 -1.29361010e-01 -1.62246117e+00]\n",
      "  [-1.23563662e+00 -1.40786442e-01  1.03895212e+00  6.31744177e-01]]\n",
      "\n",
      " [[ 1.72941743e+00  6.94052272e-01 -5.11128993e-01 -1.22843406e-01]\n",
      "  [-2.03039355e+00 -9.60775110e-01 -1.02035928e+00  2.70593425e-01]\n",
      "  [ 6.47829797e-01 -5.60373419e-01 -5.88501620e-01 -1.54655582e+00]\n",
      "  [-1.27762058e-01  2.48168027e-01  4.45780959e-01 -7.82709043e-01]\n",
      "  [ 1.98848968e+00  1.19505834e+00 -9.52375987e-02 -5.27187779e-01]]]\n",
      "(10, 5, 4)\n",
      "[[[-0.32158469 -0.32158469 -0.32158469 -0.32158469]\n",
      "  [ 0.15113037  0.15113037  0.15113037  0.15113037]\n",
      "  [-0.01862772 -0.01862772 -0.01862772 -0.01862772]\n",
      "  [ 0.48352879  0.48352879  0.48352879  0.48352879]\n",
      "  [ 0.76896516  0.76896516  0.76896516  0.76896516]]\n",
      "\n",
      " [[ 1.36624284  1.36624284  1.36624284  1.36624284]\n",
      "  [ 1.14726479  1.14726479  1.14726479  1.14726479]\n",
      "  [-0.11022916 -0.11022916 -0.11022916 -0.11022916]\n",
      "  [ 0.38825041  0.38825041  0.38825041  0.38825041]\n",
      "  [-0.38712718 -0.38712718 -0.38712718 -0.38712718]]\n",
      "\n",
      " [[-0.58722031 -0.58722031 -0.58722031 -0.58722031]\n",
      "  [ 1.91082685  1.91082685  1.91082685  1.91082685]\n",
      "  [-0.45984615 -0.45984615 -0.45984615 -0.45984615]\n",
      "  [ 1.99073781  1.99073781  1.99073781  1.99073781]\n",
      "  [-0.34903539 -0.34903539 -0.34903539 -0.34903539]]\n",
      "\n",
      " [[ 0.25282509  0.25282509  0.25282509  0.25282509]\n",
      "  [ 1.08940955  1.08940955  1.08940955  1.08940955]\n",
      "  [ 0.02392202  0.02392202  0.02392202  0.02392202]\n",
      "  [ 0.39312528  0.39312528  0.39312528  0.39312528]\n",
      "  [-0.2413848  -0.2413848  -0.2413848  -0.2413848 ]]\n",
      "\n",
      " [[-0.47552486 -0.47552486 -0.47552486 -0.47552486]\n",
      "  [-0.16577702 -0.16577702 -0.16577702 -0.16577702]\n",
      "  [-0.64971742 -0.64971742 -0.64971742 -0.64971742]\n",
      "  [ 1.63138295  1.63138295  1.63138295  1.63138295]\n",
      "  [-0.1676986  -0.1676986  -0.1676986  -0.1676986 ]]\n",
      "\n",
      " [[ 1.7226692   1.7226692   1.7226692   1.7226692 ]\n",
      "  [-2.68510868 -2.68510868 -2.68510868 -2.68510868]\n",
      "  [ 0.01842079  0.01842079  0.01842079  0.01842079]\n",
      "  [ 0.56195167  0.56195167  0.56195167  0.56195167]\n",
      "  [-0.29382124 -0.29382124 -0.29382124 -0.29382124]]\n",
      "\n",
      " [[ 1.09465308  1.09465308  1.09465308  1.09465308]\n",
      "  [ 0.63969236  0.63969236  0.63969236  0.63969236]\n",
      "  [-0.27460012 -0.27460012 -0.27460012 -0.27460012]\n",
      "  [ 0.43500926  0.43500926  0.43500926  0.43500926]\n",
      "  [ 2.81187838  2.81187838  2.81187838  2.81187838]]\n",
      "\n",
      " [[ 0.25199513  0.25199513  0.25199513  0.25199513]\n",
      "  [ 0.29950233  0.29950233  0.29950233  0.29950233]\n",
      "  [-0.43999131 -0.43999131 -0.43999131 -0.43999131]\n",
      "  [ 0.13349704  0.13349704  0.13349704  0.13349704]\n",
      "  [-1.28926119 -1.28926119 -1.28926119 -1.28926119]]\n",
      "\n",
      " [[-0.19829026 -0.19829026 -0.19829026 -0.19829026]\n",
      "  [ 2.45758763  2.45758763  2.45758763  2.45758763]\n",
      "  [ 1.06721555  1.06721555  1.06721555  1.06721555]\n",
      "  [ 0.64142066  0.64142066  0.64142066  0.64142066]\n",
      "  [ 1.10392166  1.10392166  1.10392166  1.10392166]]\n",
      "\n",
      " [[ 1.88175499  1.88175499  1.88175499  1.88175499]\n",
      "  [ 0.59358811  0.59358811  0.59358811  0.59358811]\n",
      "  [ 2.07087853  2.07087853  2.07087853  2.07087853]\n",
      "  [ 1.06979836  1.06979836  1.06979836  1.06979836]\n",
      "  [ 0.16651951  0.16651951  0.16651951  0.16651951]]]\n",
      "(10, 5, 4)\n",
      "(10, 4)\n",
      "[[-0.60952124 -0.54765964  0.76053647 -0.55432243]\n",
      " [-2.8342722  -0.9204401   1.91844923 -0.96670691]\n",
      " [ 2.75786967  1.49661597  1.24519976 -0.86979498]\n",
      " [-0.67557042  1.18322961  0.61929623 -0.75924422]\n",
      " [ 0.84214389  1.08333956 -0.79120418  0.34081189]\n",
      " [-2.27809369 -2.35826135  1.13517923  2.80863647]\n",
      " [-1.35125302  3.04292149 -2.06195596 -7.17665824]\n",
      " [-0.03836126  0.07783749  0.72086586 -1.4321    ]\n",
      " [ 3.9516297  -0.29561124 -1.61701628  2.68391342]\n",
      " [ 3.5851419   0.03975664 -2.32517122 -4.19839729]]\n"
     ]
    }
   ],
   "source": [
    "# 미니배치 버전\n",
    "np.random.seed(3)\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H) # 미니배치 수: N , 시계열의 길이: T, 은닉 상태의 차원 수: H -> 하나의 미니배치에 10개의 시계열 데이터가 있고 이 시계열 데이터의 길이는 5이며, 각 시계열 데이터의 은닉 상태의 차원 수는 4이다.\n",
    "print(hs)\n",
    "\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "print(ar.shape)\n",
    "print(ar)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(c.shape) # 맥락 벡터\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WeightSum 계층    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.cache = None\n",
    "    \n",
    "  def forward(self, hs, a):  # hs: Encoder LSTM 계층에서 출력된 각 시각에서의 은닉 상태들, a: 각 시각의 가중치\n",
    "    N, T, H = hs.shape       # N: 미니배치 크기, T: 시계열 데이터의 수, H: 은닉 상태의 차원 수\n",
    "\n",
    "    ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "    t = hs * ar             # 원소 곱 계산, 은닉 상태 hs의 각 행에 가중치 a의 원소를 곱한다.\n",
    "    c = np.sum(t, axis=1)   # 맥락 벡터\n",
    "\n",
    "    self.cache = (hs, ar)   # 역전파 계산 시 사용하기 위해 저장\n",
    "    return c\n",
    "  \n",
    "  def backward(self, dc):   # dc: 덧셈 노드로부터 전해지는 미분\n",
    "    hs, ar = self.cache\n",
    "    N, T, H = hs.shape\n",
    "\n",
    "    dt = dc.reshape(N, 1, H).repeat(T, axis=1)  # sum 노드 역전파\n",
    "    dar = dt * hs\n",
    "    dhs = dt * ar\n",
    "    da = np.sum(dar, axis=2)\n",
    "\n",
    "    return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "[[[ 4.41149179e-01  6.66490002e-01 -5.51332190e-01  6.81004293e-02]\n",
      "  [-2.34006190e-01  4.40872819e-02 -6.16411348e-01 -1.67675428e-01]\n",
      "  [ 6.95772253e-01 -1.77264522e-01 -5.20578277e-02  3.36974482e-01]\n",
      "  [-6.08913406e-02 -2.50112901e-01  3.38525278e-01  4.16763579e-01]\n",
      "  [-3.64282409e-01 -2.16320045e-01  1.17966819e-01 -2.96546825e-01]]\n",
      "\n",
      " [[-4.39972084e-01 -1.11426307e+00 -6.61208332e-01 -3.26909444e-01]\n",
      "  [-2.87204562e-01  1.02766159e-01 -3.69359904e-01  1.18867140e+00]\n",
      "  [-3.08593208e-02 -8.02448648e-01 -1.15422992e-01  3.00453093e-02]\n",
      "  [-2.54619783e-01 -1.06988517e+00 -7.51008923e-01 -1.47551425e+00]\n",
      "  [ 8.60119707e-02 -1.91248664e-02 -1.17422798e-01 -8.05393101e-01]]\n",
      "\n",
      " [[-1.02351960e+00  1.76707033e+00  3.40282468e-01 -1.97654264e+00]\n",
      "  [ 6.00428458e-01  2.40234289e+00 -2.00705490e-01 -2.15682510e+00]\n",
      "  [-8.11121985e-01 -1.33111573e+00  5.69642692e-01 -1.91709223e+00]\n",
      "  [ 3.30427085e-01 -2.05161036e+00  4.70008883e-01 -2.77931326e+00]\n",
      "  [ 2.66678737e-02 -5.47192541e-01  3.63599553e-01  3.01356266e+00]]\n",
      "\n",
      " [[-2.18403031e-01  3.81408152e-03  9.48087116e-01  2.42889620e-02]\n",
      "  [ 1.42163525e-01 -1.91298045e-02 -3.18776951e-01  1.09726094e+00]\n",
      "  [ 2.61455395e-02 -1.24089828e-02 -2.23814321e+00 -1.99461469e-01]\n",
      "  [-7.82357283e-01  7.73704099e-03  1.46777632e+00  1.39498092e+00]\n",
      "  [ 2.99350542e-01  7.55301736e-03 -1.00882930e-01 -4.80176480e-01]]\n",
      "\n",
      " [[ 1.42245654e+00 -1.33771870e+00 -1.75559093e-01  1.22317005e-02]\n",
      "  [-8.66462923e-01 -1.81877044e+00  1.41519835e-01 -2.89029339e-03]\n",
      "  [ 6.35391979e-01 -7.99913537e-01 -5.53599425e-02 -1.37925949e-02]\n",
      "  [ 1.29690748e+00  1.42239693e+00 -7.92030268e-01 -9.42481729e-03]\n",
      "  [ 4.84760729e-01 -2.67469881e+00 -1.72814308e-01 -4.00327740e-04]]\n",
      "\n",
      " [[ 8.22529991e-01 -2.69605501e-01 -1.26258870e-01 -8.42865381e-01]\n",
      "  [-1.05157345e-01 -1.84309441e-01  1.77939528e+00  1.04043359e+00]\n",
      "  [ 3.93094779e-01 -2.32787070e-01  5.76226281e-03 -1.34650844e+00]\n",
      "  [ 7.18754938e-01  1.87920755e-01 -2.33353482e-01  2.07685534e-01]\n",
      "  [-4.09351370e-01  4.47438420e-01 -1.29936067e+00  2.86717028e-01]]\n",
      "\n",
      " [[ 3.63741522e-01 -1.80848425e-02  7.28224588e-01  2.36892382e-01]\n",
      "  [ 1.68449787e-01 -2.58158625e-01  1.12979684e-01 -2.02105158e-01]\n",
      "  [-3.71287774e-01 -7.88978312e-01 -4.87445133e-01 -4.31026310e-02]\n",
      "  [-6.15013124e-01  5.76464806e-02 -6.27991813e-01  1.93788631e-01]\n",
      "  [-1.20333218e-01  2.27138038e-01 -3.05842179e-01  8.33703707e-02]]\n",
      "\n",
      " [[ 5.68263532e-02  1.83034105e-02  2.47389195e+00 -3.79048152e-01]\n",
      "  [-1.15385031e-01 -7.36724992e-02 -1.89141258e+00  5.87817147e-01]\n",
      "  [-1.07410773e-01  6.47457313e-02 -8.23114733e-02  6.86442722e-01]\n",
      "  [-1.71744495e+00  5.90915309e-02 -2.18508213e-01  7.18793140e-01]\n",
      "  [-2.52706636e+00 -1.71527980e-02  5.00508496e-01 -6.07744959e-01]]\n",
      "\n",
      " [[-1.98895372e-01 -4.86913679e-01 -2.87938902e-01 -1.93653317e+00]\n",
      "  [ 4.03341465e-01  8.90433808e-01  1.58078582e-01 -6.88794084e-01]\n",
      "  [-2.14043110e-01  3.26083045e-02 -4.26746221e-01  5.38512198e-02]\n",
      "  [-1.03765803e+00 -1.88826120e-01  2.48334283e-01  2.73267784e+00]\n",
      "  [-4.39793411e-01  7.17482530e-01 -5.85576013e-01  8.85599318e-01]]\n",
      "\n",
      " [[ 9.60325462e-01  1.44487890e+00  1.72033913e+00 -5.42874853e-03]\n",
      "  [-4.25882373e-02  2.11400236e-01  2.36403705e+00 -4.81523165e-02]\n",
      "  [ 1.01328312e+00  1.54786671e+00 -2.71648979e+00 -4.65479125e-02]\n",
      "  [ 1.79079455e+00  8.16613646e-01  2.01612567e+00  1.05038139e-01]\n",
      "  [-3.37447774e-01  6.36554297e-01  1.52075467e-01  1.66635494e-01]]]\n",
      "(10, 5)\n",
      "[[ 0.62440742 -0.97400568  0.80342439  0.44428461 -0.75918246]\n",
      " [-2.54235293  0.6348731  -0.91868565 -3.55102813 -0.8559288 ]\n",
      " [-0.89270943  0.64524076 -3.48968725 -4.03048765  2.85663755]\n",
      " [ 0.75778713  0.90151771 -2.42386813  2.088137   -0.27415585]\n",
      " [-0.07858955 -2.54660382 -0.2336741   1.91784932 -2.36315272]\n",
      " [-0.41619976  2.53036208 -1.18043847  0.88100775 -0.97455659]\n",
      " [ 1.31077365 -0.17883431 -1.69081385 -0.99156983 -0.11566699]\n",
      " [ 2.16997356 -1.49265296  0.56146621 -1.15806849 -2.65145562]\n",
      " [-2.91028112  0.76305977 -0.55432981  1.75452797  0.57771242]\n",
      " [ 4.12011475  2.48469673 -0.20188787  4.728572    0.61781748]]\n",
      "(10, 5)\n",
      "[[0.28701793 0.05803995 0.34328522 0.23970809 0.0719488 ]\n",
      " [0.02791999 0.66952525 0.14160056 0.01018247 0.15077173]\n",
      " [0.02071842 0.09644502 0.00154349 0.00089875 0.88039433]\n",
      " [0.15785892 0.18225965 0.00655381 0.59708029 0.05624734]\n",
      " [0.10631847 0.00901081 0.09104505 0.78280049 0.01082519]\n",
      " [0.04042482 0.7697038  0.01882537 0.14791702 0.023129  ]\n",
      " [0.61905877 0.13957363 0.03077223 0.06192084 0.14867453]\n",
      " [0.78754437 0.02021234 0.15765551 0.0282439  0.00634388]\n",
      " [0.00526879 0.20750206 0.05557601 0.55925719 0.17239596]\n",
      " [0.32511164 0.0633548  0.0043153  0.59742328 0.00979499]]\n"
     ]
    }
   ],
   "source": [
    "# 가중치 a를 구하는 단계\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h = np.random.randn(N, H)\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "\n",
    "t = hs * hr\n",
    "print(t.shape)\n",
    "print(t)\n",
    "\n",
    "s = np.sum(t, axis=2)\n",
    "print(s.shape)\n",
    "print(s)\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AttentionWeight 계층    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.softmax = Softmax()\n",
    "    self.cache = None\n",
    "    \n",
    "  def forward(self,hs,h): # hs: encoder의 출력(각 시각에서 출력된 은닉 상태), h: decoder의 출력\n",
    "    N, T, H = hs.shape\n",
    "\n",
    "    # hs와 h의 내적\n",
    "    hr = h.reshape(N, 1, H).repeat(T, axis=1)  \n",
    "    t = hs * hr\n",
    "    # s가 내적 결과 (N, T) N은 미니배치의 수, T는 시계열의 길이, 만약 N =10, T=5라면 s는 (10, 5)의 형상을 가진다.\n",
    "    s = np.sum(t, axis=2)\n",
    "\n",
    "    a = self.softmax.forward(s)\n",
    "    self.cache = (hs, hr)\n",
    "    return a\n",
    "  \n",
    "  def backward(self, da): # da: softmax의 역전파 출력\n",
    "    hs, hr = self.cache\n",
    "    N, T, H = hs.shape\n",
    "\n",
    "    ds = self.softmax.backward(da)\n",
    "    dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "    dhs = dt * hr\n",
    "    dhr = dt * hs\n",
    "    dh = np.sum(dhr, axis=1)\n",
    "\n",
    "    return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention 계층  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.attention_weight_layer = AttentionWeight()\n",
    "    self.weight_sum_layer = WeightSum()\n",
    "    self.attention_weight = None\n",
    "    \n",
    "  def forward(self, hs, h):\n",
    "    a = self.attention_weight_layer.forward(hs, h) # 가중치 a를 구한다.\n",
    "    out = self.weight_sum_layer.forward(hs, a)     # 맥락 벡터를 구한다.    \n",
    "    self.attention_weight = a\n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "    dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "    dhs = dhs0 + dhs1\n",
    "    return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Attention 계층  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.layers = None\n",
    "    self.attention_weights = None\n",
    "    \n",
    "  # 순전파\n",
    "  def forward(self, hs_enc, hs_dec): # hs_enc: Encoder의 출력, hs_dec: Decoder의 은닉 상태\n",
    "    N, T, H = hs_dec.shape           # N: 미니배치 크기, T: 시계열 데이터의 수, H: 은닉 상태의 차원 수\n",
    "    out = np.empty_like(hs_dec)\n",
    "    self.layers = []\n",
    "    self.attention_weights = [] # Attention의 가중치를 저장할 리스트\n",
    "    \n",
    "    for t in range(T):      # 시계열의 길이만큼 반복  \n",
    "      layer = Attention()   # Attention 계층 생성\n",
    "      out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "      self.layers.append(layer)\n",
    "      self.attention_weights.append(layer.attention_weight)\n",
    "      \n",
    "    return out\n",
    "  \n",
    "  # 역전파\n",
    "  def backward(self, dout):\n",
    "    N, T, H = dout.shape\n",
    "    dhs_enc = 0\n",
    "    dhs_dec = np.empty_like(dout)\n",
    "    \n",
    "    for t in range(T):\n",
    "      layer = self.layers[t]\n",
    "      dhs, dh = layer.backward(dout[:, t, :])\n",
    "      dhs_enc += dhs\n",
    "      dhs_dec[:, t, :] = dh\n",
    "      \n",
    "    return dhs_enc, dhs_dec\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention을 갖춘 seq2seq2 구현  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEncoder(Encoder):\n",
    "  def forward(self, xs):\n",
    "    xs = self.embed.forward(xs)\n",
    "    hs = self.lstm.forward(xs)\n",
    "    return hs\n",
    "\n",
    "  def backward(self, dhs):\n",
    "    dout = self.lstm.backward(dhs)\n",
    "    self.embed.backward(dout)\n",
    "    return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "    rn = np.random.randn\n",
    "    # 가중치 초기화\n",
    "    # Embedding 계층의 가중치 초기화\n",
    "    embed_W = (rn(V, D) / 100).astype('f')\n",
    "    \n",
    "    # LSTM 계층의 가중치 초기화\n",
    "    lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "    lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "    lstm_b = np.zeros(4 * H).astype('f')\n",
    "    \n",
    "    # Affine 계층의 가중치 초기화\n",
    "    affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "    affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "    # 계층 생성\n",
    "    self.embed = TimeEmbedding(embed_W)\n",
    "    self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "    self.attention = TimeAttention()\n",
    "    self.affine = TimeAffine(affine_W, affine_b)\n",
    "    \n",
    "    # layers에 생성한 계층을 리스트로 저장\n",
    "    layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "    # 매개변수, 기울기를 각 리스트에 저장\n",
    "    self.params, self.grads = [], []\n",
    "    for layer in layers:\n",
    "      self.params += layer.params\n",
    "      self.grads += layer.grads\n",
    "\n",
    "  # 순전파\n",
    "  def forward(self, xs, enc_hs): # xs: Decoder의 입력 데이터, enc_hs: Encoder의 출력(은닉 상태)\n",
    "    # Encoder에서의 마지막 은닉 상태를 h에 저장\n",
    "    h = enc_hs[:, -1]\n",
    "    self.lstm.set_state(h)\n",
    "\n",
    "    out = self.embed.forward(xs)\n",
    "    dec_hs = self.lstm.forward(out)             # des_hs: Decoder LSTM 계층을 통과한 후의 출력 값(은닉 상태)\n",
    "    c = self.attention.forward(enc_hs, dec_hs)  # c: 맥락 벡터, Encoder의 출력과 Decoder의 출력을 Attention 계층에 통과시켜 맥락 벡터를 구한다.\n",
    "    out = np.concatenate((c, dec_hs), axis=2)   # concatencate를 통해 맥락 벡터 c와 Decoder의 출력을 결합한다.\n",
    "    score = self.affine.forward(out)            # score: Affine 계층을 통과한 후의 출력 값\n",
    "\n",
    "    return score\n",
    "\n",
    "  def backward(self, dscore):                   # dscore: Softmax with loss 계층의 역전파 출력\n",
    "    dout = self.affine.backward(dscore)         # dout: Affine 계층의 역전파 출력\n",
    "    N, T, H2 = dout.shape\n",
    "    H = H2 // 2\n",
    "\n",
    "    dc, ddec_hs0 = dout[:, :, :H], dout[:, :, H:] # dc: Affine 계층의 역전파 출력 중 앞부분은 맥락 벡터 c에 대한 기울기, ddec_hs0: LSTM Decoder의 출력에 대한 기울기\n",
    "    denc_hs, ddec_hs1 = self.attention.backward(dc) # denc_hs: Encoder의 출력에 대한 기울기, ddec_hs1: LSTM Decoder의 출력에 대한 기울기\n",
    "    ddec_hs = ddec_hs0 + ddec_hs1  # ddec_hs: Affine 계층의 역전파 출력 중 LSTM Decoder의 출력에 대한 기울기와와 Attention 계층의 역전파 출력 중중 LSTM Decoder의 출력에 대한 기울기를 더한 값이다.\n",
    "    dout = self.lstm.backward(ddec_hs)            # Decoder의 LSTM 계층에 입력한 x 값에 대한 기울기  \n",
    "    dh = self.lstm.dh                             # dh: Encoder의 마지막 은닉 상태에 대한 기울기\n",
    "    denc_hs[:, -1] += dh                          # Encoder의 마지막 은닉 상태에 대한 기울기를 갱신한다. \n",
    "    self.embed.backward(dout)                     # Decoder의 Embedding 계층에 대한 기울기\n",
    "\n",
    "    return denc_hs                                # Encoder의 은닉 상태에 대한 기울기를 반환 \n",
    "  \n",
    "  def generate(self, enc_hs, start_id, sample_size): # enc_hs: Encoder의 출력(은닉 상태), start_id: 최초로 주어지는 단어 ID, sample_size: 샘플링하는 단어의 수\n",
    "    sampled = []\n",
    "    sample_id = start_id\n",
    "    h = enc_hs[:, -1]\n",
    "    self.lstm.set_state(h)\n",
    "\n",
    "    for _ in range(sample_size):\n",
    "        x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "        out = self.embed.forward(x)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        sample_id = np.argmax(score.flatten())\n",
    "        sampled.append(sample_id)\n",
    "\n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AttentionSeq2seq 구현    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "    self.encoder = AttentionEncoder(V, D, H)\n",
    "    self.decoder = AttentionDecoder(V, D, H)\n",
    "    self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "    self.params = self.encoder.params + self.decoder.params\n",
    "    self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AttentionSeq2seq 학습     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence import *\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_data('date.txt')\n",
    "char_to_id, id_to_char = get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 1\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 8[s] | 손실 3.09\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 16[s] | 손실 1.90\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 24[s] | 손실 1.72\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 31[s] | 손실 1.46\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 39[s] | 손실 1.19\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 47[s] | 손실 1.14\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 55[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 64[s] | 손실 1.06\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 71[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 80[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 88[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 96[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 103[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 110[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 118[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 125[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 133[s] | 손실 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "정확도 0.0\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "  trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "  correct_num = 0\n",
    "  for i in range(len(x_test)):\n",
    "    question, correct = x_test[[i]], t_test[[i]]\n",
    "    # \n",
    "    verbose = i < 10\n",
    "    correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose, is_reverse=True) # is_reverse=True로 지정하여 입력 문장을 반전시킨다.\n",
    "    \n",
    "  acc = float(correct_num) / len(x_test)\n",
    "  acc_list.append(acc)\n",
    "  print('정확도', acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 저장\n",
    "model.save_params()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
