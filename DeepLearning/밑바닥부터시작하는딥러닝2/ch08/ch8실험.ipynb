{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AttentionWithSeq2Seq vs Seq2seq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/KimDongyoung/Desktop/Github/my_git/mygit/DEEPLEARNING/밑바닥부터시작하는딥러닝2')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sequence as sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "from ch07.seq2seq import Seq2seq\n",
    "from ch07.peeky_seq2seq import PeekySeq2seq\n",
    "from common.base_model import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 29)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W] # 가중치 W를 인스턴스 변수 params에 리스트로 저장\n",
    "        self.grads = [np.zeros_like(W)] # W와 형상이 같은 행렬을 하나 준비, 0으로 초기화\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx] # W의 idx번째 행을 추출\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout) # dW의 self.idx번째 행에 dout을 더함\n",
    "        return None\n",
    "    \n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = softmax(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Embedding\n",
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape # N은 미니배치 크기, T는 시계열 데이터의 길이, 하나의 데이터 당 T개의 단어가 들어있음\n",
    "        V, D = self.W.shape # V는 어휘 수, D는 단어 벡터의 차원 수 \n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f') # 출력 데이터 저장용 배열\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None\n",
    "\n",
    "# RNN\n",
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "\n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev\n",
    "\n",
    "# LSTM\n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        '''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가장추 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        b: 편향（4개분의 편향이 담겨 있음）\n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "\n",
    "        dc_prev = ds * f\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "\n",
    "# Time RNN\n",
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "            dxs[:, t, :] = dx\n",
    "\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "\n",
    "# Tiem Affine\n",
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx\n",
    "\n",
    "# Time Softmax with Loss\n",
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        self.ignore_label = -1\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "\n",
    "        if ts.ndim == 3:  # 정답 레이블이 원핫 벡터인 경우\n",
    "            ts = ts.argmax(axis=2)\n",
    "\n",
    "        mask = (ts != self.ignore_label)\n",
    "\n",
    "        # 배치용과 시계열용을 정리(reshape)\n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "        mask = mask.reshape(N * T)\n",
    "\n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N * T), ts])\n",
    "        ls *= mask  # ignore_label에 해당하는 데이터는 손실을 0으로 설정\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= mask.sum()\n",
    "\n",
    "        self.cache = (ts, ys, mask, (N, T, V))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, mask, (N, T, V) = self.cache\n",
    "\n",
    "        dx = ys\n",
    "        dx[np.arange(N * T), ts] -= 1\n",
    "        dx *= dout\n",
    "        dx /= mask.sum()\n",
    "        dx *= mask[:, np.newaxis]  # ignore_labelㅇㅔ 해당하는 데이터는 기울기를 0으로 설정\n",
    "\n",
    "        dx = dx.reshape((N, T, V))\n",
    "\n",
    "        return dx\n",
    "    \n",
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1)#.repeat(T, axis=1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da\n",
    "\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh\n",
    "\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh\n",
    "\n",
    "\n",
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f') # embed_W: 입력 데이터의 단어 ID를 단어의 분산 표현으로 변환하는 가중치\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "\n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "        return hs[:, -1, :]\n",
    "\n",
    "    def backward(self, dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:, -1, :] = dh\n",
    "\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, h):\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        out = self.lstm.forward(out)\n",
    "        score = self.affine.forward(out)\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dout = self.embed.backward(dout)\n",
    "        dh = self.lstm.dh\n",
    "        return dh\n",
    "\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape((1, 1))\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n",
    "\n",
    "\n",
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled\n",
    "\n",
    "\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(BaseModel):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params # 인코더와 디코더의 params를 다 합침\n",
    "        self.grads = self.encoder.grads + self.decoder.grads    # 인코더와 디코더의 grads를 다 합침\n",
    "\n",
    "    def forward(self, xs, ts): # xs: 입력 데이터, ts: 정답 레이블\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:] # 정답 레이블에서 첫 번째 문자를 제외한 것이 디코더의 입력, 정답 레이블에서 마지막 문자를 제외한 것이 디코더의 정답 레이블\n",
    "\n",
    "        h = self.encoder.forward(xs)\n",
    "        score = self.decoder.forward(decoder_xs, h) # 인코더의 은닉 상태 h와 디코더의 입력 데이터를 사용해 점수를 출력\n",
    "        loss = self.softmax.forward(score, decoder_ts) # 점수와 디코더의 정답 레이블을 사용해 손실을 계산\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.softmax.backward(dout)\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dout = self.encoder.backward(dh)\n",
    "        return dout\n",
    "\n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        h = self.encoder.forward(xs)\n",
    "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AttentionSeq2seq...\n",
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 8[s] | 손실 3.10\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 16[s] | 손실 1.88\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 25[s] | 손실 1.66\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 33[s] | 손실 1.33\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 43[s] | 손실 1.17\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 50[s] | 손실 1.13\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 58[s] | 손실 1.08\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 66[s] | 손실 1.05\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 74[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 82[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 90[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 99[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 107[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 117[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 128[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 136[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 146[s] | 손실 0.99\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1988-08-21\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1981-08-21\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1988-08-21\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1981-08-21\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1988-08-21\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1981-08-21\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1988-08-21\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1988-08-21\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1988-08-21\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1981-08-21\n",
      "---\n",
      "AttentionSeq2seq Accuracy 0.020%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 9[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 18[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 26[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 36[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 45[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 53[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 62[s] | 손실 0.96\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 70[s] | 손실 0.93\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 79[s] | 손실 0.91\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 88[s] | 손실 0.87\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 97[s] | 손실 0.82\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 106[s] | 손실 0.76\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 116[s] | 손실 0.68\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 125[s] | 손실 0.60\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 135[s] | 손실 0.50\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 144[s] | 손실 0.41\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 158[s] | 손실 0.30\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1972-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-08\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1993-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "AttentionSeq2seq Accuracy 70.420%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 8[s] | 손실 0.18\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 18[s] | 손실 0.13\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 29[s] | 손실 0.10\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 38[s] | 손실 0.07\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 47[s] | 손실 0.06\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 56[s] | 손실 0.05\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 64[s] | 손실 0.04\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 73[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 83[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 93[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 101[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 106[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 112[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 117[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 122[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 127[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 132[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "AttentionSeq2seq Accuracy 97.760%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 5[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 10[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 15[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 20[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 25[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 30[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 36[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 41[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 46[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 51[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 56[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 61[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 66[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 71[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 76[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 81[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 86[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "AttentionSeq2seq Accuracy 98.140%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 5[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 10[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 15[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 20[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 25[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 30[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 35[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 40[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 45[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 51[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 56[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 61[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 66[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 71[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 76[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 81[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 87[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "AttentionSeq2seq Accuracy 99.800%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 5[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 12[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 18[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 24[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 30[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 36[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 53[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 58[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 68[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 74[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 84[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 89[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 94[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "AttentionSeq2seq Accuracy 100.000%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 5[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 11[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 27[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 32[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 39[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 54[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 70[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 77[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 87[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 98[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 110[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 121[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 131[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "AttentionSeq2seq Accuracy 100.000%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 20[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 30[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 39[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 49[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 60[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 70[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 80[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 90[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 100[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 110[s] | 손실 0.02\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 121[s] | 손실 0.02\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 130[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 140[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 150[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 160[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 169[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "AttentionSeq2seq Accuracy 100.000%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 19[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 29[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 39[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 49[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 59[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 68[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 89[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 99[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 110[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 120[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 131[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 141[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 151[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 162[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 172[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "AttentionSeq2seq Accuracy 100.000%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 51[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 61[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 72[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 82[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 92[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 102[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 122[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 132[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 142[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 152[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 162[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 172[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "AttentionSeq2seq Accuracy 100.000%\n",
      "Training Seq2seq...\n",
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 3[s] | 손실 3.28\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 7[s] | 손실 1.93\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 11[s] | 손실 1.78\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 14[s] | 손실 1.69\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 18[s] | 손실 1.46\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 21[s] | 손실 1.22\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 25[s] | 손실 1.16\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 28[s] | 손실 1.11\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 32[s] | 손실 1.08\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 35[s] | 손실 1.05\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 39[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 42[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 45[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 49[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 53[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 56[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 60[s] | 손실 1.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1999-01-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1999-01-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1999-01-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1999-01-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1999-01-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1999-01-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1999-01-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1999-01-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1999-01-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1999-01-11\n",
      "---\n",
      "Seq2seq Accuracy 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 3[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 7[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 11[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 15[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 19[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 23[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 27[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 31[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 34[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 38[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 41[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 45[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 49[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 52[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 56[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 59[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 63[s] | 손실 0.99\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1998-03-23\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1998-03-23\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1998-03-23\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1998-03-23\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1998-03-23\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1998-03-23\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1998-03-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1998-03-23\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1998-03-23\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1998-03-23\n",
      "---\n",
      "Seq2seq Accuracy 0.000%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 3[s] | 손실 0.99\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 7[s] | 손실 0.99\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 10[s] | 손실 0.99\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 14[s] | 손실 0.99\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 17[s] | 손실 0.98\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 21[s] | 손실 0.99\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 25[s] | 손실 0.99\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 28[s] | 손실 0.98\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 32[s] | 손실 0.98\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 35[s] | 손실 0.99\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 40[s] | 손실 0.98\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 44[s] | 손실 0.98\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 48[s] | 손실 0.98\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 52[s] | 손실 0.98\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 56[s] | 손실 0.98\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 59[s] | 손실 0.98\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 63[s] | 손실 0.98\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1983-03-24\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1983-03-24\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1983-03-24\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1983-03-24\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1983-03-24\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1983-03-24\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1983-03-24\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1983-03-24\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-03-24\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1983-03-24\n",
      "---\n",
      "Seq2seq Accuracy 0.000%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 3[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 8[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 12[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 16[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 20[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 24[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 28[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 31[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 35[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 38[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 42[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 45[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 49[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 52[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 56[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 59[s] | 손실 0.98\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 63[s] | 손실 0.98\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1996-06-17\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1996-06-17\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1996-06-17\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1996-06-17\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1996-06-17\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1996-06-17\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1996-06-17\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1996-06-17\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1996-06-17\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1996-06-17\n",
      "---\n",
      "Seq2seq Accuracy 0.000%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 3[s] | 손실 0.98\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 7[s] | 손실 0.98\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 11[s] | 손실 0.98\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 14[s] | 손실 0.98\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 18[s] | 손실 0.98\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 22[s] | 손실 0.98\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 27[s] | 손실 0.98\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 32[s] | 손실 0.97\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 37[s] | 손실 0.97\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 41[s] | 손실 0.97\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 46[s] | 손실 0.96\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 50[s] | 손실 0.96\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 55[s] | 손실 0.95\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 59[s] | 손실 0.94\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 64[s] | 손실 0.94\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 68[s] | 손실 0.94\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 72[s] | 손실 0.93\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1993-03-29\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1993-03-29\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1993-03-29\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1993-03-29\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1993-03-29\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1993-03-29\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1993-08-09\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1993-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1993-03-29\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1993-08-29\n",
      "---\n",
      "Seq2seq Accuracy 0.000%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 4[s] | 손실 0.93\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 9[s] | 손실 0.92\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 13[s] | 손실 0.92\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 18[s] | 손실 0.91\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 23[s] | 손실 0.91\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 28[s] | 손실 0.91\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 33[s] | 손실 0.91\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 37[s] | 손실 0.90\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 43[s] | 손실 0.90\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 49[s] | 손실 0.90\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 55[s] | 손실 0.89\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 60[s] | 손실 0.89\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 64[s] | 손실 0.89\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 68[s] | 손실 0.89\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 72[s] | 손실 0.89\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 75[s] | 손실 0.89\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 79[s] | 손실 0.88\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1994-10-14\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1994-05-24\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1994-04-14\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1994-01-24\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1994-01-24\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1994-10-14\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1994-07-24\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1994-07-21\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1994-10-14\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1994-01-24\n",
      "---\n",
      "Seq2seq Accuracy 0.080%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 3[s] | 손실 0.88\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 6[s] | 손실 0.88\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 10[s] | 손실 0.87\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 13[s] | 손실 0.87\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 17[s] | 손실 0.86\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 20[s] | 손실 0.86\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 23[s] | 손실 0.86\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 27[s] | 손실 0.85\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 31[s] | 손실 0.85\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 34[s] | 손실 0.85\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 38[s] | 손실 0.86\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 42[s] | 손실 0.85\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 46[s] | 손실 0.84\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 50[s] | 손실 0.84\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 53[s] | 손실 0.83\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 56[s] | 손실 0.83\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 60[s] | 손실 0.83\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1980-10-02\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1981-05-02\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1980-03-02\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1981-05-02\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1981-05-02\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1980-10-02\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1980-08-02\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1980-08-02\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1980-10-02\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1981-05-02\n",
      "---\n",
      "Seq2seq Accuracy 0.000%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 4[s] | 손실 0.83\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 8[s] | 손실 0.82\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 13[s] | 손실 0.82\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 17[s] | 손실 0.82\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 22[s] | 손실 0.82\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 26[s] | 손실 0.82\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 29[s] | 손실 0.82\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 33[s] | 손실 0.81\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 37[s] | 손실 0.81\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 41[s] | 손실 0.81\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 44[s] | 손실 0.81\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 48[s] | 손실 0.83\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 52[s] | 손실 0.82\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 56[s] | 손실 0.81\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 59[s] | 손실 0.81\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 63[s] | 손실 0.81\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 67[s] | 손실 0.80\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1990-10-13\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1998-01-17\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1990-03-13\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1997-01-17\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1998-05-17\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1990-10-13\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1971-08-13\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1971-08-13\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1990-10-13\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1998-05-17\n",
      "---\n",
      "Seq2seq Accuracy 0.080%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 3[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 6[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 9[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 12[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 15[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 18[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 21[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 24[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 27[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 30[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 33[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 36[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 39[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 42[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 45[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 48[s] | 손실 0.80\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 52[s] | 손실 0.80\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1976-10-22\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1984-01-21\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1999-03-22\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1984-01-21\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1984-01-21\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1976-10-22\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1989-08-22\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1989-08-22\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1979-10-22\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1982-01-21\n",
      "---\n",
      "Seq2seq Accuracy 0.060%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 3[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 6[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 9[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 12[s] | 손실 0.81\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 15[s] | 손실 0.82\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 18[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 21[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 24[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 27[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 30[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 33[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 36[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 39[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 42[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 45[s] | 손실 0.79\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 47[s] | 손실 0.80\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 50[s] | 손실 0.80\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1989-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1986-07-15\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1989-03-15\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1986-07-15\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1986-07-15\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1983-10-14\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1999-08-15\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1999-08-15\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1989-10-15\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1986-07-15\n",
      "---\n",
      "Seq2seq Accuracy 0.080%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 리스트\n",
    "models = {\n",
    "    \"AttentionSeq2seq\": AttentionSeq2seq(vocab_size, wordvec_size, hidden_size),\n",
    "    \"Seq2seq\": Seq2seq(vocab_size, wordvec_size, hidden_size),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    optimizer = Adam()\n",
    "    trainer = Trainer(model, optimizer)\n",
    "\n",
    "    acc_list = []\n",
    "    for epoch in range(max_epoch):\n",
    "        trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                    batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "        correct_num = 0\n",
    "        for i in range(len(x_test)):\n",
    "            question, correct = x_test[[i]], t_test[[i]]\n",
    "            verbose = i < 10\n",
    "            correct_num += eval_seq2seq(model, question, correct,\n",
    "                                        id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "        acc = float(correct_num) / len(x_test)\n",
    "        acc_list.append(acc)\n",
    "        print(f'{model_name} Accuracy %.3f%%' % (acc * 100))\n",
    "\n",
    "    results[model_name] = acc_list\n",
    "    model.save_params()\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(12, 8))\n",
    "for model_name, acc_list in results.items():\n",
    "    x = np.arange(len(acc_list))\n",
    "    plt.plot(x, acc_list, marker='o', label=model_name)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
