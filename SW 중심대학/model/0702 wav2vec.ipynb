{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de313eb-7838-4799-9b0a-132b1e442083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (4.41.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: torch==2.3.1 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torchaudio) (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch==2.3.1->torchaudio) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch==2.3.1->torchaudio) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch==2.3.1->torchaudio) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch==2.3.1->torchaudio) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch==2.3.1->torchaudio) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch==2.3.1->torchaudio) (2023.10.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch==2.3.1->torchaudio) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torchaudio) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torchaudio) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from jinja2->torch==2.3.1->torchaudio) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from sympy->torch==2.3.1->torchaudio) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf736c0-286d-4123-aa80-22d07de841ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e34e69-199e-422a-be5f-af557031a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SR = 16000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = r\"C:\\Users\\KimDongyoung\\Downloads\\SW중심대학\"  # Update this to your dataset path\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 16\n",
    "    N_EPOCHS = 10\n",
    "    LR = 1e-4\n",
    "    HIDDEN_DIM = 256\n",
    "    DROPOUT_RATE = 0.3\n",
    "    SEED = 42\n",
    "\n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7963c168-b814-4373-9c09-ad22ce658bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# Load CSV files\n",
    "train_df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'test.csv'))\n",
    "sample_submission_df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'sample_submission.csv'))\n",
    "\n",
    "# Update paths in DataFrames\n",
    "train_df['path'] = train_df['path'].apply(lambda x: os.path.join(CONFIG.ROOT_FOLDER, x[2:]))\n",
    "test_df['path'] = test_df['path'].apply(lambda x: os.path.join(CONFIG.ROOT_FOLDER, x[2:]))\n",
    "\n",
    "# Split the training data\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=CONFIG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "205ef070-50d3-465b-adb2-e9cb408ebe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Wav2Vec2 model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base-960h\", num_labels=CONFIG.N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da7ba5ba-6437-44e4-9df3-9f9c83f0d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(waveform, target_length):\n",
    "    if len(waveform) > target_length:\n",
    "        return waveform[:target_length]\n",
    "    elif len(waveform) < target_length:\n",
    "        padding = target_length - len(waveform)\n",
    "        return np.pad(waveform, (0, padding), mode='constant')\n",
    "    else:\n",
    "        return waveform\n",
    "\n",
    "def preprocess_audio(file_path, target_sample_rate=16000, target_length=16000*5):  # Assume 5 seconds as the target length\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    if sample_rate != target_sample_rate:\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)(waveform)\n",
    "    waveform = waveform.squeeze().numpy()\n",
    "    waveform = pad_or_truncate(waveform, target_length)\n",
    "    return waveform\n",
    "\n",
    "def get_wav2vec_features(df, train_mode=True, target_length=16000*5):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        file_path = row['path']\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        audio = preprocess_audio(file_path, target_sample_rate=CONFIG.SR, target_length=target_length)\n",
    "        input_values = processor(audio, return_tensors=\"pt\", sampling_rate=CONFIG.SR).input_values\n",
    "        features.append(input_values.squeeze().numpy())\n",
    "        if train_mode:\n",
    "            label = row['label']\n",
    "            label_vector = 0 if label == 'fake' else 1\n",
    "            labels.append(label_vector)\n",
    "    if train_mode:\n",
    "        return features, labels\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c48d20-e35c-41e0-a85d-88b052ce43b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 44350/44350 [06:10<00:00, 119.59it/s]\n",
      " 93%|██████████████████████████████████████████████████████████████████████     | 10354/11088 [01:25<00:06, 107.79it/s]"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = get_wav2vec_features(train_data, True)\n",
    "val_features, val_labels = get_wav2vec_features(val_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13309c-f925-4741-909c-93bb008616d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.labels is not None:\n",
    "            return torch.tensor(self.features[index]), torch.tensor(self.labels[index])\n",
    "        return torch.tensor(self.features[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630420c-3047-472a-88cb-0e8566b9bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_features, train_labels)\n",
    "val_dataset = CustomDataset(val_features, val_labels)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253e1d6-0b0d-4ca9-b134-4592893ab2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, device, patience=5):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    early_stop_count = 0\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(features).logits\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score, accuracy, precision, recall, f1 = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "        print(f'Val Accuracy : [{accuracy:.5f}], Precision : [{precision:.5f}], Recall : [{recall:.5f}], F1 : [{f1:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "        \n",
    "        if early_stop_count >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11052f-91d8-4ba9-9ebd-52eb14ae52b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            probs = model(features).logits\n",
    "            loss = criterion(probs, labels)\n",
    "            val_loss.append(loss.item())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "        \n",
    "        all_preds = np.argmax(all_probs, axis=1)\n",
    "        all_true = all_labels\n",
    "        \n",
    "        accuracy = accuracy_score(all_true, all_preds)\n",
    "        precision = precision_score(all_true, all_preds)\n",
    "        recall = recall_score(all_true, all_preds)\n",
    "        f1 = f1_score(all_true, all_preds)\n",
    "    \n",
    "    return _val_loss, auc_score, accuracy, precision, recall, f1\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b02221-aea3-4737-b0b8-21789a4d92a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41fe56-57a1-443f-b83e-cf85a1f6763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=CONFIG.LR)\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e53ad-5c66-44fa-a995-b67b5fe408c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = get_wav2vec_features(test_df, False)\n",
    "test_dataset = CustomDataset(test_features)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ac731-de70-4252-b7c1-9dc854072fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features in tqdm(iter(test_loader)):\n",
    "            features = features.to(device)\n",
    "            probs = model(features).logits\n",
    "            probs = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "# Assuming you have true labels for the test set to compute the metrics\n",
    "test_labels = test_df['label'].apply(lambda x: 0 if x == 'fake' else 1).values\n",
    "test_preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "precision = precision_score(test_labels, test_preds)\n",
    "recall = recall_score(test_labels, test_preds)\n",
    "f1 = f1_score(test_labels, test_preds)\n",
    "auc = roc_auc_score(test_labels, [p[1] for p in preds])\n",
    "\n",
    "print(f'Test Accuracy : {accuracy:.5f}')\n",
    "print(f'Test Precision : {precision:.5f}')\n",
    "print(f'Test Recall : {recall:.5f}')\n",
    "print(f'Test F1 Score : {f1:.5f}')\n",
    "print(f'Test AUC : {auc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ee0dc-0ed7-4a49-bc7f-8196936532d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission file\n",
    "submit = sample_submission_df.copy()\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.to_csv(os.path.join(CONFIG.ROOT_FOLDER, '0702wav2vec_submit.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28cba32-cae5-4d87-ad6a-2d687d4d24f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
