{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37407188-fc87-4c32-8932-e88d77d98ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (1.5.0)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (0.59.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from pooch>=1.1->librosa) (23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.6.2)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (1.4.0.post0)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torchmetrics) (1.24.3)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torchmetrics) (23.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torchmetrics) (2.3.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torchmetrics) (0.11.3.post0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (2023.10.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->torchmetrics) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->torchmetrics) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kimdongyoung\\anaconda\\lib\\site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "305a887f-74f9-4baf-9adc-7068de53d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f028ca7a-4237-46a8-9136-f746ee550ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    # Dataset\n",
    "    ROOT_FOLDER = r\"C:\\Users\\KimDongyoung\\Downloads\\SW중심대학\"\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 128\n",
    "    N_EPOCHS = 3\n",
    "    LR = 1e-4\n",
    "    DROPOUT_RATE = 0.3\n",
    "    # Others\n",
    "    SEED = 42\n",
    "    \n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED) # Seed 고정\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'train.csv'))\n",
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# Update the 'path' column to have the full path\n",
    "train['path'] = train['path'].apply(lambda x: os.path.join(CONFIG.ROOT_FOLDER, x))\n",
    "val['path'] = val['path'].apply(lambda x: os.path.join(CONFIG.ROOT_FOLDER, x))\n",
    "\n",
    "# Ensure test paths are also updated\n",
    "test = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'test.csv'))\n",
    "test['path'] = test['path'].apply(lambda x: os.path.join(CONFIG.ROOT_FOLDER, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0313adb4-9e68-4dbb-a3c1-dc12ba8a9d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        file_path = row['path']\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        # Load audio file using librosa\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        \n",
    "        # Extract MFCC features using librosa\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['label']\n",
    "            label_vector = np.zeros(CONFIG.N_CLASSES, dtype=float)\n",
    "            label_vector[0 if label == 'fake' else 1] = 1\n",
    "            labels.append(label_vector)\n",
    "\n",
    "    if train_mode:\n",
    "        return features, labels\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9121b268-3609-48c3-8231-2804bc856fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4527it [00:59, 75.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call the function with the updated paths\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_mfcc, train_labels \u001b[38;5;241m=\u001b[39m get_mfcc_feature(train, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m val_mfcc, val_labels \u001b[38;5;241m=\u001b[39m get_mfcc_feature(val, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[24], line 10\u001b[0m, in \u001b[0;36mget_mfcc_feature\u001b[1;34m(df, train_mode)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load audio file using librosa\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m y, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(file_path, sr\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mSR)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Extract MFCC features using librosa\u001b[39;00m\n\u001b[0;32m     13\u001b[0m mfcc \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmfcc(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr, n_mfcc\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mN_MFCC)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# Otherwise try soundfile first, and then fall back if necessary\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m         y, sr_native \u001b[38;5;241m=\u001b[39m __soundfile_load(path, offset, duration, dtype)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\librosa\\core\\audio.py:222\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    219\u001b[0m         frame_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# Load the target number of frames, and transpose to match librosa form\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     y \u001b[38;5;241m=\u001b[39m sf_desc\u001b[38;5;241m.\u001b[39mread(frames\u001b[38;5;241m=\u001b[39mframe_duration, dtype\u001b[38;5;241m=\u001b[39mdtype, always_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, sr_native\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\soundfile.py:895\u001b[0m, in \u001b[0;36mSoundFile.read\u001b[1;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frames \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m frames \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(out):\n\u001b[0;32m    894\u001b[0m         frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[1;32m--> 895\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_array_io(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m, out, frames)\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m>\u001b[39m frames:\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\soundfile.py:1344\u001b[0m, in \u001b[0;36mSoundFile._array_io\u001b[1;34m(self, action, array, frames)\u001b[0m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39msizeof(ctype)\n\u001b[0;32m   1343\u001b[0m cdata \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mcast(ctype \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, array\u001b[38;5;241m.\u001b[39m__array_interface__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 1344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cdata_io(action, cdata, ctype, frames)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\soundfile.py:1353\u001b[0m, in \u001b[0;36mSoundFile._cdata_io\u001b[1;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[0;32m   1351\u001b[0m     curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m   1352\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_snd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m action \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m ctype)\n\u001b[1;32m-> 1353\u001b[0m frames \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file, data, frames)\n\u001b[0;32m   1354\u001b[0m _error_check(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errorcode)\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Call the function with the updated paths\n",
    "train_mfcc, train_labels = get_mfcc_feature(train, True)\n",
    "val_mfcc, val_labels = get_mfcc_feature(val, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85273c-ebe9-4bf1-811f-8a8e99e1a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.label is not None:\n",
    "            return self.mfcc[index], self.label[index]\n",
    "        return self.mfcc[index]\n",
    "\n",
    "train_dataset = CustomDataset(train_mfcc, train_labels)\n",
    "val_dataset = CustomDataset(val_mfcc, val_labels)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb5a80-14f7-4894-b626-170a889f940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d12124-a5c3-4b38-808e-fd7ecedb3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dim=128, output_dim=CONFIG.N_CLASSES, dropout_rate=CONFIG.DROPOUT_RATE):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device, patience=5):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    early_stop_count = 0\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(train_loader):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        val_loss, val_score = validate(model, criterion, val_loader, device)\n",
    "        train_loss_mean = np.mean(train_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch}], Train Loss: [{train_loss_mean:.5f}], Val Loss: [{val_loss:.5f}], Val AUC: [{val_score:.5f}]')\n",
    "        \n",
    "        if val_score > best_val_score:\n",
    "            best_val_score = val_score\n",
    "            best_model = model\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "        \n",
    "        if early_stop_count >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b702174-1cb9-4f39-bf9e-842a84928ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            loss = criterion(probs, labels)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    \n",
    "    val_loss_mean = np.mean(val_loss)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    \n",
    "    auc_score = roc_auc_score(all_labels, all_probs, average='macro')\n",
    "    \n",
    "    return val_loss_mean, auc_score\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features in test_loader:\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            predictions.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2b3e3a2-8d8a-46ee-86b9-e138ece89aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/347 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m RNNModel()\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mLR)\n\u001b[1;32m----> 5\u001b[0m best_model \u001b[38;5;241m=\u001b[39m train(model, optimizer, train_loader, val_loader, device)\n",
      "Cell \u001b[1;32mIn[18], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, val_loader, device, patience)\u001b[0m\n\u001b[0;32m     29\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 33\u001b[0m output \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m, in \u001b[0;36mRNNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[0;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m---> 13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "model = RNNModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG.LR)\n",
    "\n",
    "best_model = train(model, optimizer, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bed831-37d2-4731-9332-54d308bbf394",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mfcc = get_mfcc_feature(test, False)\n",
    "test_dataset = CustomDataset(test_mfcc, None)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f34b1-71b5-419e-9ad2-70bd155e5037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6cec1b-d86d-4b3d-8ee2-b7d86b81a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b16af9-dbfc-4e43-b4de-254aba065d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fbb63-587e-49c6-bb3c-60722e453bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = inference(infer_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb7e72-7b6b-429d-b1f6-c776318a09b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab4053a-c4f4-4a6c-aadb-350c51dbc7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb40f0-c78d-46d8-829c-a43244f50f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e0187-7647-4950-8e77-0d810e9e9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install librosa tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a8160-13ab-4583-8ec4-aa1ee00809c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5555d121-8890-409d-8938-fc50c54223c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f704eb-01a2-4239-ad31-69dd3cf48876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Config class\n",
    "class Config:\n",
    "    SR = 16000  # YAMNet expects 16kHz sample rate\n",
    "    N_MELS = 64  # YAMNet expects 64 Mel bands\n",
    "    ROOT_FOLDER = 'C:/Users/KimDongyoung/Downloads/SW중심대학'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 128\n",
    "    N_EPOCHS = 10\n",
    "    LR = 1e-4\n",
    "    DROPOUT_RATE = 0.3\n",
    "    SEED = 42\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "# Function to set the seed for reproducibility\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(CONFIG.SEED) # Fix the seed\n",
    "\n",
    "# Load your DataFrame\n",
    "train_df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'test.csv'))\n",
    "submission_df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'sample_submission.csv'))\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "train, val = train_test_split(train_df, test_size=0.2, random_state=CONFIG.SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe780ed-df2d-4ba7-a5a0-30d29dbf7580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure paths are updated\n",
    "def update_path(df, root_folder):\n",
    "    df['path'] = df['path'].apply(lambda x: os.path.join(root_folder, x))\n",
    "    return df\n",
    "\n",
    "train = update_path(train, CONFIG.ROOT_FOLDER)\n",
    "val = update_path(val, CONFIG.ROOT_FOLDER)\n",
    "test_df = update_path(test_df, CONFIG.ROOT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6f34e-7be6-4204-aaf7-44ce6583f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path, sr=CONFIG.SR):\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    return y, sr\n",
    "\n",
    "def extract_mel_spectrogram(y, sr, n_mels=CONFIG.N_MELS):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "def get_mel_spectrogram_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        file_path = row['path']\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        y, sr = load_audio(file_path)\n",
    "        mel_spec = extract_mel_spectrogram(y, sr)\n",
    "        features.append(mel_spec)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['label']\n",
    "            label_vector = np.zeros(CONFIG.N_CLASSES, dtype=float)\n",
    "            label_vector[0 if label == 'fake' else 1] = 1\n",
    "            labels.append(label_vector)\n",
    "\n",
    "    if train_mode:\n",
    "        return features, labels\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de54c25-cb8c-4265-9246-60e8bb1d5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Mel spectrogram features and labels\n",
    "train_mel, train_labels = get_mel_spectrogram_feature(train, True)\n",
    "val_mel, val_labels = get_mel_spectrogram_feature(val, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70420845-3cc3-44dc-9ed3-4a9ef5ca1002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAMNet model\n",
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)\n",
    "\n",
    "def extract_embedding(mel_spectrogram):\n",
    "    # Yamnet expects shape (1, 64, 96)\n",
    "    # Resize the mel spectrogram to fit YAMNet's expected input\n",
    "    mel_spectrogram_resized = tf.image.resize(mel_spectrogram, [64, 96])\n",
    "    scores, embeddings, spectrogram = yamnet_model(mel_spectrogram_resized)\n",
    "    return embeddings\n",
    "\n",
    "# Extract embeddings for training and validation data\n",
    "train_embeddings = np.array([extract_embedding(tf.convert_to_tensor(mel[np.newaxis, ...])).numpy() for mel in train_mel])\n",
    "val_embeddings = np.array([extract_embedding(tf.convert_to_tensor(mel[np.newaxis, ...])).numpy() for mel in val_mel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1591b-c380-4a55-bcf3-9b3f155abfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple classifier model\n",
    "def build_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=input_shape),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(lr=CONFIG.LR), loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e9324-a0bf-4d44-803d-781c75f2c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_embeddings.shape[1:]\n",
    "model = build_model(input_shape)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c186c-c87d-454f-88eb-032821355102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_embeddings, train_labels, epochs=CONFIG.N_EPOCHS, batch_size=CONFIG.BATCH_SIZE,\n",
    "          validation_data=(val_embeddings, val_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
